\documentclass[math, info]{cours}
\title{Optimisation Combinatoire}
\author{Chien-Chen Huang}

\begin{document}
\bettertitle
\section{Max-Flow (min-cut)}
\subsection{Ford-Fulkerson}
\begin{definition}
	Soit $G = \left(V, E\right)$ un graphe orienté, $c: E \to \R^{+}$ une fonction de capacité et $s, t \in V$ deux sommets terminaux,
	$f: E\to \R^{+}$ est un flot si:
	\begin{enumerate}
		\item $0 \leq f(e) \leq c(e), \forall e \in E$
		\item $\sum_{e \in \delta^{-}(v)} f(e) = \sum_{e \in \delta^{+}(v)} f(e), \forall v \in V \setminus \{s, t\}$ (Conservation du Flot).
	\end{enumerate}
	\label{def:flot}
\end{definition}

On va s'intéresser au problème suivant:
\problemStatement{Max-Flow}{Entrée={$G = \left(V, E\right)$ un graphe orienté, $c: E \to \R^{+}$ une fonction de capacité et $s, t \in V$ deux sommets terminaux}, Sortie={Une fonction de flot $f$ maximale, c'est à dire avec un volume maximal: $\sum_{e \in \delta^{+}(s)} f(e) - \sum_{e \in \delta^{-}(s)} f(e)$}}

\begin{thm}
	On rappelle qu'obtenir un flot maximal est équivalent à obtenir une coupe de poids minimal.
\end{thm}

On définit pour cela le réseau résiduel d'une fonction de flot $f$:
\begin{definition}
	Étant donné un flot $f$, pour chaque arrête $e = (u, v) \in E$, on définit:
	\begin{align*}
		(u, v) \in F \text{ si } c(e) - f(e) > 0 \\
		(v, u) \in F \text{ si } f(e) > 0
	\end{align*}
	Dans le premier cas, on définit $u(e) = c(e) - f(e)$. Dans le deuxième cas on définit $u(e) = f(e)$.
	\label{def:residualnetwork}
\end{definition}

\begin{proposition}
	Si on pousse de $u$ à $v$, i.e. si $f(((u, v)) > 0$, alors $(v, u)$ doit apparaître dans le réseau résiduel.
	\label{prop:pakompri}
\end{proposition}

On propose alors l'algorithme de Ford-Fulkerson, se basant sur des chemins augmentants pour résoudre le problème:
\begin{algorithm}
	\caption{Ford-Fulkerson}
	\label{alg:fordfulkerson}
	Tant que $G(f)$ a un chemin de $s$ à $t$ noté $p$, on pousse le plus possible le long du chemin $p$.
\end{algorithm}

\begin{thm}
	Si l'algorithme de Ford-Fulkerson termine, alors $f$ est un flot maximal.
\end{thm}

\begin{proof}
	Soit $U \subseteq V$ l'ensemble des sommets atteignables depuis $s$ dans $G(f)$. On a par \ref{prop:pakompri}
	\begin{equation*}
		\begin{aligned}
			\sum_{e \in \delta^{+}(s)} f(e) - \sum_{e\in \delta^{-}(s)}f(e) = & \sum_{e \in \delta^{+}(U)}f(e) - \sum_{e \in \delta^{-}(U)}f(e) \\
			=                                                                 & \sum_{e\in \delta^{+}(U)} c(e) = \text{cut size de U}
		\end{aligned}
	\end{equation*}
\end{proof}
Toutefois la complexité de cet algorithme dépend de la valeur du flot maximum, et celui-ci ne termine même pas pour des réels.

\subsection{Push-Relabel}
On va chercher un algorithme dont la complexité n'en dépend pas (dit fortement polynomial), ce qui nous amène à la notion de pré-flot:
\begin{definition}
	Soit $G = \left(V, E\right)$ un graphe orienté, $c: E \to \R^{+}$ une fonction de capacité et $s, t \in V$ deux sommets terminaux,
	$f: E\to \R^{+}$ est un pré-flot si:
	\begin{enumerate}
		\item $0 \leq f(e) \leq c(e), \forall e \in E$
		\item $\mathrm{exces}(v) = \sum_{e \in \delta^{-}(v)} f(e) \geq \sum_{e \in \delta^{+}(v)} f(e), \forall v \in V \setminus \{s, t\}$.
	\end{enumerate}
	\label{def:preflot}
\end{definition}

On va essayer de construire un algorithme dont le principe est cette fois ci d'avancer
\begin{definition}
	\begin{itemize}
		\item Un sommet $v \in V\setminus \{s, t\}$ est dite \emph{actif} si $\mathrm{exces}(v) > 0$.
		\item Un étiquetage des sommets $d: V \to \N$ est valide si $\forall (u, v) \in G(f)$ (pour $f$ un pré-flot), on a: $d(u) \leq d(v) + 1$.
		\item Une arête $(u, v) \in G(f)$ est admissible si $d(u) = d(v) + 1$.
	\end{itemize}
	\label{def:activenode}
\end{definition}

On obtient alors l'algorithme suivant, proposé originellement par Andrew Goldberg en 1989:
\begin{algorithm}
	\caption{Push-Relabel}
	\label{alg:pushrelabel}
	\begin{description}
		\item[Initialisation]: On pose $\forall e \in \delta^{+}(s), f(e) = c(e)$, sinon $f(e) = 0$.
		      On pose $d(s) = n, d(v) = 0 \forall v \neq s$.
		\item[Boucle] Tant qu'il existe un sommet actif $v$, on effectue deux actions:
		      \begin{description}
			      \item[\tt Push] S'il existe $(u,v) \in G(f)$ admissible, on pousse $\min \left(\mathrm{exces}(u), u(e)\right)$ selon l'arête $(u, v)$.
			      \item[\tt Relabel] On pose $d(u) = \min_{v\mid (u, v) \in G(f)}d(v) + 1$
		      \end{description}
	\end{description}
\end{algorithm}

On va donc prouver la correction de cet algorithme.
Pour cela, on se base sur les deux lemmes suivants:
\begin{lemme}
	Si $v$ est actif, alors $v$ a un chemin orienté vers $s$ dans $G(f)$.
	\label{lem:preflow1}
\end{lemme}
\begin{proof}
	Soit $X \subseteq V$ l'ensemble des sommets ayant un chemin vers $s$ dans $G(f)$.
	Par l'absurde, il existe $w \in V\setminus X$ actif. On a alors:
	\begin{equation*}
		0 < \sum_{v \in V \setminus X} \left(\sum_{e \in \delta^{-}(v)} f(e) - \sum_{e\in \delta^{+}(v)} f(e)\right) = \sum_{e \in \delta^{-}(v\setminus x)} f(e) - \sum_{e\in \delta^{+}(V\setminus X)} f(e)
	\end{equation*}
	Or, $\sum_{e \in \delta^{-}(v\setminus x)} f(e) = 0$, d'où le résultat.
\end{proof}

\begin{lemme}
	Étant donné un chemin $P$ de $u$ à $v$, alors, $\abs{P} \geq d(u) - d(v)$, si $d$ est valide.
	\label{lem:preflow2}
\end{lemme}
\begin{proof}
	Si $P = v_{0}v_{1}\cdots v_{x}$, puisque $d$ est valide: $d(v_{i}) \leq d(v_{i + 1}) + 1$ pour tout $i$.
	D'où, $d(v_{0}) \leq d(v_{x}) + x$.
	D'où le résultat.
\end{proof}

On obtient un corollaire très utile:
\begin{corollaire}
	Pour tout $n$, $d(u) \leq 2n - 1$.
	\label{cor:preflow1}
\end{corollaire}
\begin{proof}
	Si $u$ est actif et $d(u) = 2n$, tout chemin de $u$ à $s$ est de longueur au moins $n$, ce qui est impossible puisque $\abs{V} = n$.
\end{proof}

\begin{thm}[Correction de Push-Relabel]
	Quand l'algorithme \ref{alg:pushrelabel} s'arrête, on obtient un max-flow.
	\label{thm:pushrelabel}
\end{thm}
\begin{proof}
	Il n'y a jamais de chemin de $s$ à $t$ dans $G(f)$ par \ref{lem:preflow2}.
	Par ailleurs, il n'y a pas de sommet actif à l'arrêt de l'algorithme, ce qui signifie qu'on a bien un véritable flot.
	La preuve de correction de \ref{alg:fordfulkerson} s'applique donc.
\end{proof}

\begin{thm}[Complexité de Push-Relabel]
	L'algorithme \ref{alg:pushrelabel} s'arrête en temps $\O(V^{2}E)$.
\end{thm}

\begin{proof}
	On a toujours $3$ opérations:
	\begin{itemize}
		\item Le \texttt{Relabel} qui prend un temps $\O(V^{2})$ (au plus $(n-2) \times (2n-1)$ opérations).
		\item Le \texttt{Push Saturant} (push qui permet à $f((u, v))$ d'atteindre $c((u, v))$). Celui-ci va supprimer l'arête $(u, v)$ de $G(f)$.
		      Pour que l'arc soit réinséré dans $G(f)$ pour un autre push saturant,
		      $v$ doit d'abord être réétiqueté.
		      Ensuite, après un push sur $(v, u)$, $u$ doit être réétiqueté.
		      Au cours du processus, $d(u)$ augmente d'au moins $2$
		      Il y a donc $\O(V)$ push saturants sur $(u, v)$ et donc $\O(VE)$ push saturants au total.
		\item Le \texttt{Push Non-Saturant} qu'on effectue un nombre $\O(V^{2}E)$ de fois.
		      En effet, borner le nombre de push non-saturants peut se faire à partir d'un argument de potentiel.
		      On utilise la fonction de potentiel $\Phi =\sum_{v \text{ actif}} d(v)$.
		      Il est clair que $\Phi = 0$ à l'initialisation et reste toujours positive durant l'exécution.
		      Par ailleurs, un push non-saturant diminue $\Phi$ d'au moins $1$.
		      De plus, le relabel et le push augmentent $\Phi$ d'au plus $1$ et d'au plus $(2V - 1)$ respectivement.
		      On a donc: $\Phi \leq \left(2V-1\right)\left(V - 2\right) + \left(2V - 1\right)\left(2VE\right)$.
		      On a donc: $\Phi \leq \O\left(V^{2}E\right)$.
	\end{itemize}
	L'algorithme prend donc un temps $\O\left(V^{2}E\right)$.
\end{proof}

Pour essayer d'améliorer l'algorithme on propose la version suivante:
\begin{algorithm}
	\caption{Push-Relabel +}
	\label{alg:pushrelabel+}
	Boucle: On choisit le sommet actif $v$ avec la plus haute étiquette, on effectue deux actions:
	\begin{description}
		\item[\tt Push] S'il existe $(u,v) \in G(f)$ admissible, on pousse $\min \left(\mathrm{exces}(u), u(e)\right)$ selon l'arête $(u, v)$.
		\item[\tt Relabel] On pose $d(u) = \min_{v\mid (u, v) \in G(f)}d(v) + 1$
	\end{description}
\end{algorithm}
Il est clair que l'algorithme reste correcte, toutefois, on change la complexité pour de $V^{2}E$ à $V^{3}$.
On peut même encore améliorer la complexité pour obtenir $\O(V^{2}\sqrt{E})$, et même $\O(V^{1 + o(1)}\log(E))$

\subsection{Edmonds-Karp}
\begin{algorithm}
	\caption{Edmonds-Karp}
	\label{alg:edmondskarp}
	Pour cet algorithme, on applique Ford-Fulkerson en choisissant le plus court des chemins de $s$ à $t$.
\end{algorithm}
\begin{thm}[Complexité d'Edmonds-Karp]
	L'algorithme de Edmonds-Karp prend un temps $\O(VE^{2})$.
\end{thm}

Dans la suite, on note $f_{0}, f_{1},\cdots$ les flots obtenus de sorte que $f_{i + 1}$ est obtenu du plus court chemin $P_{i}$ dans $G(f_{i})$.

\begin{lemme}
	$\forall i$:
	\begin{itemize}
		\item $\abs{P_{i}} \leq \abs{P_{i + 1}}$
		\item Si $P_{i}$ et $P_{i + 1}$ utilisent deux arcs opposés (i.e. $(u, v)$ et $(v, u)$), alors $\abs{P_{i}} + 2 \leq \abs{P_{i + 1}}$.
	\end{itemize}
	\label{lem:edmondskarp1}
\end{lemme}
\begin{proof}
	On pose $H = P_{i} \cup P_{i + 1}$ où les arcs opposés sont supprimés.
	On ajoute alors $2$ arcs supplémentaires de $t$ à $s$.
	Comme alors $H$ est eulérien, il existe deux chemins disjoints $q_{1}, q_{2}$ de $s$ à $t$ dans $H$.
	Notons que toutes les arêtes de $H$ (sauf les arêtes de $t$ à $s$) sont dans $G(f_{i})$.
	On a de plus $\abs{P_{i}} \leq q_{1}, q_{2}$ d'où
	\begin{equation*}
		2\abs{P_{i}} \leq \abs{q_{1}} + \abs{q_{2}} \leq \abs{H} \leq \abs{P_{i}} + \abs{P_{i + 1}} - 2
	\end{equation*}
\end{proof}

\begin{lemme}
	Soit $l < k$ tel que $P_{l}$ et $P_{k}$ utilisent des arcs opposés.
	Alors, $\abs{P_{l}} + 2 \leq \abs{P_{k}}$.
	\label{lem:edmondskarp2}
\end{lemme}
\begin{proof}
	La preuve précédente peut être aisément adaptée:
	On peut supposer que pour $l < i < k$, $P_{i}$ n'a pas d'arcs opposés avec $P_{k}$, sinon le résultat se déduit par récurrence par le lemme précédent.
	On pose $H = P_{l} \cup P_{k}$ où les arcs opposés sont supprimés.
	On ajoute alors $2$ arcs supplémentaires de $t$ à $s$.
	Comme alors $H$ est eulérien, il existe deux chemins disjoints $q_{1}, q_{2}$ de $s$ à $t$ dans $H$.
	Notons que toutes les arêtes de $H$ (sauf les arêtes de $t$ à $s$) sont dans $G(f_{l})$.
	On a de plus $\abs{P_{l}} \leq q_{1}, q_{2}$ d'où
	\begin{equation*}
		2\abs{P_{l}} \leq \abs{q_{1}} + \abs{q_{2}} \leq \abs{H} \leq \abs{P_{l}} + \abs{P_{k}} - 2
	\end{equation*}
\end{proof}

\begin{lemme}
	Un arc dans $G(f)$ peut être une arête bottleneck (c'est à dire une arête avec le moins de capacité) au plus $\O(n)$ fois.
\end{lemme}
\begin{proof}
	Pour qu'une arête $e$ soit bottleneck une nouvelle fois, il faut que la longueur du nouveau plus court chemin ait augmenté au moins de $2$.
\end{proof}


\begin{proof}[Démonstration de la Complexité d'Edmonds-Karp]
	Chaque arête peut être utilisée au plus $\O(n)$, il y a donc au plus $nm$ itérations, et les itérations se font en temps $\O(m)$, ce qui est le résultat.
\end{proof}

\section{Couplage Maximal}
\begin{definition}
	Etant donné un graphe $G = \left(V, E \right)$, $M \subseteq E$ est un couplage si et seulement si $\delta_{M}(v) \leq 1$ pour tout noeud $v$.\\
	Un chemin $P$ est dit $M$-alternant s'il alterne entre une arête de $M$ et une arêtre de $E\setminus M$. \\
	Un chemin $M$-alternant $P$ est dit $M$-augmentant s'il commence et termine par un noeud non couvert par $M$.
	\label{def:matching}
\end{definition}

\begin{thm}
	Un couplage $M$ est maximal si et seulement si il n'y a pas de chemin $M$-augmentant.
\end{thm}
\begin{proof}
	\begin{itemize}
		\item[$\Leftarrow$] S'il y a un chemin $M$-augmentant
		\item[$\Rightarrow$] Supposons qu'il existe un $M^{*}$ avec $\abs{M^{*}} > \abs{M}$ est $M$ n'a pas de chemin augmentant.
			$M^{*} \otimes M$ est un sous-graphe de $G$ avec degré dans $\{0, 1, 2\}$.
			On a plusieurs possiblités pour ce graphe ($M^{*}$ est en rouge, $M$ en bleu).
			\begin{category}
				\cdot & & \cdot\arrow[color=red, r, no head] & \cdot\arrow[color=blue, d, no head]
				& & \cdot\arrow[color=red, dr, no head] & & &\cdot\arrow[dr, color=blue, no head]
				\\
				\cdot & & \cdot\arrow[color=blue, u, no head] & \cdot\arrow[color=red, l, no head]
				& & & \cdot\arrow[dl, color=blue, no head] & & & \cdot\arrow[dl, color=red, no head]
				\\
				& & \cdot\arrow[dr, color=red, no head] & & & \cdot\arrow[dr, color=red, no head] &  & &\cdot\arrow[dr, color=blue, no head]& \\
				& & & \cdot & & & \cdot & & &\cdot\\
			\end{category}
			Toutefois, le quatrième type de graphe n'est pas possible puisque s'il y a plus de rouges que de bleus, on a un chemin augmentant pour $M$.
			Alors en suivant le
	\end{itemize}
\end{proof}

\begin{thm}
	On peut trouver un couplage maximal en $\O(mn)$.
\end{thm}

\subsection{Théorème de K\H{o}nig}
\begin{thm}[de K\H{o}nig]
	Dans un graphe biparti $G = \left(A \sqcup B, E\right)$:
	\begin{equation*}
		\max_{couplage}\abs{M} = \min_{\text{\sc Vertex Cover}}\abs{C}
	\end{equation*}
\end{thm}

Avant de prouver le théorème, une observation:
\begin{proposition}[Dualité Faible]
	Dans un graphe $G$ (non nécessairement biparti), on a:
	\begin{equation*}
		\abs{M} \leq \abs{C}
	\end{equation*}
	\label{prop:weakduality}
\end{proposition}
\begin{proof}
	Puisque les arêtes de $M$ ne couvrent qu'au plus une fois chaque sommet, en particulier:
	\begin{equation*}
		\sum_{v \in C} 1 \geq \sum_{(u, v) \in M} \left(1 + 1\right) \geq \sum_{e \in M} 1
	\end{equation*}
\end{proof}

\begin{proof}[Démonstration du Théorème de K\H{o}nig]
	Soit $M$ un couplage maximal de $G$.
	On pose $U$ l'ensemble des sommets non atteints par $M$ dans $A$ et $Z$ l'ensemble des sommets connecté par un chemin $M$ alternant aux sommets de $U$.
	On pose $S = Z \cap X$ et $T = Z \cap Y$.
	Alors, chaque sommet de $T$ est atteint par $M$ et $T$ est l'ensemble des voisins de $S$.
	Posons $K  = \left( X \setminus S \right)\cup T$. Chaque arête de $G$ a une de ses extrémités dans $K$.
	Donc $K$ est une couverture des sommets de $G$ et $\abs{M} = \abs{K}$ d'où le résultat.
\end{proof}

\subsection{Dualité et Programmation Linéaire}
\begin{definition}
	La programmation linéaire s'intéresse à la maximisiation de $\transpose{w}X$ en vérifiant $X\leq 0$ et $AX\leq b$ (problème de packing) ou à la minimisation de $b^{T}Y$ en vérifiant $\transpose{A}y \geq w$ avec $y \geq 0$ (problème de covering).
	\label{def:linearprog}
\end{definition}

Quelques exemples:
\begin{exemple}[Couplage Maximal comme Programmation Linéaire]
	On cherche à maximiser $\sum_{e} X_{e}$ en ayant $\forall v \in A \cup B \sum_{e \in \delta(v)} X_{e} \leq 1$ et $\forall e \in E, X_{e} \geq 0$.
	On a donc ici $A$ la matrice $(A_{v\in V, e\in E})$ d'incidence du graphe et $b$ et $w$ le vecteur Attila.
	On essaye de mettre autant d'arêtes que possible (packing).

	Ce problème peut donc aussi s'écrire comme la minimisation de $\sum_{v \in A\cup B} Y_{v}$ en ayant $Y_{u} + Y_{v} \geq 1 \forall e = (u, v) \in E$, $Y_{u} \geq 0 \forall u \in A\cup B$.
	On essaye de mettre aussi peu d'arêtes que possible (covering).
\end{exemple}

\begin{proposition}
	La dualité faible de la programmation linéaire est: $\forall x, y$ qui vérifie les contraintes, on a:
	\begin{equation*}
		\transpose{w}X \leq \transpose{b}Y
	\end{equation*}
	\label{prop:dualitylinearprog}
\end{proposition}
\begin{proof}
	\begin{equation*}
		\transpose{w}x \leq \left(\transpose{Y}A\right)X = \transpose{Y}\left(AX\right) \leq \transpose{Y}b= \transpose{b}Y
	\end{equation*}
\end{proof}

Ceci redémontre la dualité faible pour les graphes.

\begin{thm}[Dualité Forte]
	Il existe $X^{*}, Y^{*}$ tels que $\transpose{w}X^{*} = \transpose{b}Y^{*}$.
\end{thm}
La dualité permet de faire les liens entre Max-Flow et Min-Cut ainsi qu'entre Max-Matching et Min-Vertex-Cover.

\subsection{Couplage Pondéré dans un graphe biparti}
\begin{definition}
	Soit un graphe $G = \left(A\cup B, E = A \times B\right)$ avec $\abs{A} = \abs{B}$ et une fonction de poids entière sur les arêtes, on dit que $\pi: A\cup B \to \N$ est une couverture si $\forall e \in E, \pi(a) + \pi(b) \geq w(e)$.
	\label{def:cover}
\end{definition}

\begin{thm}[Egrevary]
	On a:
	\begin{equation*}
		\max_{M \text{ couplage parfait}} w(M) = \min_{\pi \text{ couverture entière}} \sum_{v \in A\cup B} \pi(v)
	\end{equation*}
\end{thm}

\begin{proposition}[Dualité Faible]
	On a: $w(M) \leq \sum \pi(v)$.
	\label{prop:weakduality2}
\end{proposition}
\begin{proof}
	De même que précédemment en remplaçant par l'inégalité d'une couverture.
\end{proof}

\begin{proof}[Démonstration du Théorème d'Egevary]
	Soit $\pi$ une couverture minimale.
	Posons $G_{\pi} \subseteq G$ où une arête $e = (a, b)$ est dans $G_{\pi}$ si et seulement si $\pi(a) + \pi(b) = w(e)$.
	C'est le sous-graphe des arêtes qu'on ne peut pas modifier.
	Soit $M$ le couplage maximal dans $G_{\pi}$.
	Si $\abs{M} = \abs{A} = \abs{B}$ alors on a fini puisqu'on a toujours égalité dans la preuve de la dualité faible.
	Sinon, si $M$ n'est pas parfait, on définit $\pi'$ comme suit:
	\begin{equation*}
		\begin{cases}
			\forall a \in A_{1} & \pi'(a) = \pi(a) - 1\\
			\forall b \in B_{1} & \pi'(b) = \pi(b) + 1\\
			\forall v \notin A_{1} \cup B_{1} & \pi'(v) = \pi(v)
		\end{cases}
	\end{equation*}
	où $A_{1}$ est l'ensemble des sommets atteignables depuis un sommet non atteint pas $M$ passant par un chemin augmentant et $B_{1}$ est l'ensemble des sommets couplés à des sommets de $A_{1}$.
	On définit également $B_{2}$ l'ensemble des sommets de $B$ atteignables depuis un sommet non atteint par $M$ en passant par un chemin $M$-augmentant, $A_{2}$ l'ensemble des sommets couplés à des sommets de $B_{2}$ et $A_{3}, B_{3}$ le reste de $A$ et $B$.
	Puisque les arêtes qui ne sont pas dans $G_{\epsilon}$ vérifient $\pi(a) + \pi(b) \geq w\left( (a,b) \right) + 1$, on a toujours $\pi'(a) + \pi'(b) \geq w(e)$.
	De plus, on a bien $\sum \pi' < \sum \pi$ et $\pi'$ couvre toutes les arêtes, mais $\pi'$ peut être négative.
	Si $\pi'(a_{0}) = -1$, alors on définit $\pi''$ comme:
	\begin{equation*}
		\begin{cases}
			\pi''(a) = \pi'(a) + 1 & \forall a \in A\\
			\pi''(b) = \pi'(b) - 1 & \forall b \in B
		\end{cases}
	\end{equation*}
	Ceci garantit que tous les $\pi''(a)$ sont non négatifs et que $\sum\pi'' = \sum\pi'$.
	Par ailleurs les $\pi''(b)$ sont aussi non négatifs. En effet, si on arrive à celà, on a une arête d'un poids $0 \leq w(e) \leq -1 + 0$.
	Donc $\pi''$ est une couverture plus petite que $\pi$.
\end{proof}

Ceci nous donne par ailleurs un algorithme pour calculer une couverture minimale/un couplage maximal.
Toutefois, celui-ci effectue au plus $\abs{A}\max_{e} w(e) = \frac{nw}{2}$ itérations et est donc pseudo polynomial.
Par ailleurs, cet algorithme ne termine pas nécessairement sur les réels.
\medskip

La version ci-dessous, appelée méthode hongroise, termine quant à elle sur les réels et est plus efficace:
\begin{algorithm}
	\caption{Méthode Hongroise (Kuhn 1957)}
	\begin{algorithmic}
		\State $M \gets \emptyset$, $\displaystyle\pi(a) \gets \max_{e \in \delta(a)} w(e)$ et $\pi(b) \gets 0$
		\While {$M$ n'est pas parfait}
			\State {Construire $G_{\pi}$.}
			\State {Trouver le couplage maximal $M$ dans $G_{\pi}$ (par augmentation) et définir $A_{1}, A_{2}, A_{3}, B_{1}, B_{2}, B_{3}$}
			\State{Poser $\displaystyle\Delta \gets \min_{a\in A_{1}, b\in B_{2} \cup B_{3}} \pi(a) + \pi(b) - w((a, b))$}
			\State{$\pi(a) \gets \pi(a) - \Delta$ et $\pi(b) \gets \pi(b) + \Delta$}
			\State{$\delta \gets \min_{a \in A}\pi(a)$.}
			\If {$\delta < 0$}
				\State {$\pi(a) \gets \pi(a) - \delta$ et $\pi(b) \gets \pi(b) + \delta$}
			\EndIf
		 \EndWhile
	\end{algorithmic}
\end{algorithm}

\begin{thm}
	Cette méthode termine et est demande un temps $\O(n^{2}nn)$.
\end{thm}
\begin{proof}
	\begin{itemize}
		\item La boucle prend un temps $\O(m)$ pour s'effectuer.
		\item On peut l'effectuer au plus $n$ fois (lorsqu'$\abs{A_{1}}$ augmente) et $n$ fois lorsque $\abs{M}$ augmente.
			Puisque ces deux effets peuvent se combiner, on a bien le résultat.
	\end{itemize}
\end{proof}

\subsection{Relaxation et Optimalité}
\begin{proposition}[Relaxation]
	Étant donné un problème de programmation linéaire dual, $X^{*}$ et $Y^{*}$ sont tous les deux optimaux si et seulement si:
	\begin{align*}
		X_{i}^{*} \Longrightarrow \left(\transpose{A}Y^{*}\right)_{i} = W_{i}\\
		Y_{j}^{*} \Longrightarrow \left(AX^{*}\right)_{j} = b_{j}
	\end{align*}
	\label{prop:relaxation}
\end{proposition}
\begin{proof}
	On a:
	\begin{equation*}
		\transpose{W}X \leq \transpose{Y}AX \leq \transpose{Y}b
	\end{equation*}
	Il y a égalité si et seulement si les contraintes sont serrées.
	D'où le résultat.
\end{proof}

On peut appliquer ceci aux problèmes de couplage maximal:
Si une arête est utilisée ($X_{e} > 0$), alors $Y_{u} + Y_{v} = 1$ et donc elle est la seule à atteindre l'une de ses extrémités.

On peut appliquer de même cela à Max-Flow Min-Cut


\subsection{Forêts Hongroises et Floraisons}
\begin{definition}[Forêt Hongroise]
	Dans un graphe biparti, on construit une forêt dite hongroise en partant des sommets non couplésde $B$, en considérant ses voisins.
	Si ses voisins ne sont pas dans le couplage, on a un chemin augmentant.
	Sinon on ajoute les deux arêtes à la forêt et le nouveau sommet de $B$ à la liste des sommets à considérer et on itère en évitant les sommets déjà atteints (puisque ça ne sert à rien).
	On appelle l'étape d'agrandissement de la forêt le \emph{probing}.
	\label{def:hungarianforest}
\end{definition}

On regarde toutes les arêtes au plus une fois, ce qui donne une complexité $\O(\abs{E})$.

\begin{proposition}
	Si on ne trouve pas de chemin augmentant, on a un couplage maximal.
	\label{prop:maxcouple}
\end{proposition}

On va s'intéresser aux floraisons (cycles de longueur impaire) qui peuvent apparaître en créant l'équivalent d'une forêt hongroise dans un graphe non biparti.
Ceci nous mène à l'algorithme suivant.
\begin{algorithm}
	\caption{Edmonds Blossom Algorithm}
	\begin{description}
		\item[Initialization] (étant donné un couplage initial $M$)\\
			$A = \emptyset$\\
			$B = \left\{v \in V\mid \not\exists u, (u, v)\in M \right\}$ (ce seront les racines de la forêt hongroise)\\
		\item[Boucle] Tant qu'il existe $u \in B$ tel que $(u, v)$ n'a pas été probée:
		\begin{enumerate}
			\item Ou bien $v \in A$ et on ne fait rien
			\item Ou bien $v \notin A \cup B$, auquel cas $v$ est couplé à $w$ et on ajoute $(u, v)$ et $(v, w)$ à la forêt, en posant $A = A \cup \{v\}$ et $B = B \cup \{w\}$.
			\item Ou bien $v \in B$ et:
				\begin{enumerate}
					\item ou bien $u$ et $v$ sont dans des arbres différents: on augmente (en utilisant le chemin de la racine de $u$ à la racine de $v$) et on recommence l'algorithme en rouvrant toutes les floraisons
					\item ou bien on contracte la floraison constituées des chemins de $u$ et $v$ à leur plus petit ancêtre commun (LCA) et $(u, v)$.
						On traite ensuite cette floraison comme un sommet dans $B$.
				\end{enumerate}
		\end{enumerate}
	\end{description}
\end{algorithm}

\begin{thm}
	L'algorithme d'Edmonds trouve le couplage maximal.
	\label{thm:edmondsblossom}
\end{thm}

\begin{proposition}
	Le graphe après contraction a un chemin augmentant si et seulement si le graphe pré contraction a un chemin augmentant.
	\label{prop:blossomcontraction}
\end{proposition}
\begin{proof}
	On note $G'$ le graphe formé après la contraction d'une floraison dans $G$.
	\begin{itemize}
		\item Considérons un chemin augmentant dans $G'$ qui passe par la contraction. On a deux cas:
			\begin{enumerate}
				\item Si la floraison est une des extrémités du chemin, on sait que dans la floraison (qui a $2k + 1$ sommets) il n'y aura qu'un noeud d'attache.
					Ce sommet n'est pas dans $M$ car il s'agit d'une arête finale d'un chemin augmentant dans $G'$.
					Ainsi, si le sommet est la base de la floraison, le chemin s'arrête et on renvoit le chemin original.
					Sinon, on peut étendre notre chemin selon des arêtes alternants jusqu'à avoir atteint le sommet de base.
				\item Sinon, on définit $v_{L}$ et $v_{R}$ les sommets adjacents à la floraison.
					On peut toujours prendre un chemin de $v_{L}$ à $v_{R}$ de longueur paire.
					Ainsi, on trouve un chemin augmentant dans $G$, puisque seulement l'une des deux $v_{L}$ et $v_{R}$ est couplée à la floraison et donc à la base de la floraison.
			\end{enumerate}
		\item Si on a chemin augmentant dans $G$ qui passe par la floraison, on enlève un nombre pair d'arêtes du chemin donc le chemin dans $G'$ reste un chemin augmentant.
	\end{itemize}
\end{proof}

On définit quelques invariants pour l'analyse de l'algorithme:
\begin{itemize}
	\item Tous les sommets de $A$ sont reliés à des sommets de $B$
	\item Une floraison est toujours dans $B$
	\item Si $G'$ est le graphe dérivé d'une suite de contraction de floraisons, alors $G'$ a un chemin augmentant si et seulement si $G$ a un chemin augmentant.
		Ceci découle trivialement de \ref{prop:blossomcontraction}.
	\item Si on trouve un chemin augmentant dans $G'$ alors on peut rouvrir toutes les floraisons pour augmenter la taille de $M$ dans $G$.
	\item Chaque noeud dans $B$ (resp. $A$) peut revenir à sa racine par un chemin alternant de longueur paire (resp. impaire).
\end{itemize}

Soit $U$ l'ensemble des sommets qui ne sont pas dans la forêt hongroise.

\begin{proof}[Démonstration de \ref{thm:edmondsblossom}]
	\begin{description}
		\item[Preuve 1] On montre qu'à la fin de cet algorithme, dans $G'$ (possiblement après beaucoup de contractions), il n'y a pas de chemin augmentant.\\
				À la fin de l'algorithme d'Edmonds, il n'y a pas d'arêtes dans $B \times \left(B \cup U \right)$ (il peut y avoir des arêtes dans $A \times \left( A\cup U \right)$).
				Sinon, on aurait un moyen d'ajouter une arête à la forêt hongroise.
				Il n'y a donc pas de chemin augmentant dans $G'$.
		 \item[Preuve 2] Par le théorème de Tutte-Berge.
			 \begin{thm}[Tutte-Berge Minimax]
				 \begin{equation*}
				 	\max_{M \ couplage} \abs{M} = \min_{A \subseteq V}\frac{\abs{V} - \left(oc\left(V \setminus A \right) - \abs{A} \right)}{2}
				\end{equation*}
				où $oc(V \setminus A)$ est le nombre de composantes connexes de taille impaire de $V \setminus A$
			 \end{thm}
			 L'algorithme d'Edmonds trouve un certain $A$ tel que:
			 \begin{equation*}
				 \abs{U} = oc\left(V\setminus A \right) - \abs{A}
			 \end{equation*}
			 En effet, tout élément de $A$ est matché à un élément de $B$ non racine.
			 Les éléments non matchés étant les racines d'arbres de la forêt, ils sont dans $B$.
			 En enlevant les éléments de $A$, on crée une composante connexe de taille impaire par élément de $B$.
			 Comme par ailleurs, $\abs{B} - \abs{A}$ est exactement l'ensemble des sommets non couplés, on a le résultat.
			 En a donc bien le résultat.
	\end{description}
\end{proof}

\begin{thm}
	Cet algorithme demande un temps en $\O(mn^{2})$.
	\label{prop:edmondsblossomtimecomplexity}
\end{thm}
\begin{proof}
	La construction d'une forêt hongroise se fait en $\O(m)$.\\
	La construction d'une floraison se fait en au plus $\O(m)$, et peut se faire au plus $\O(n)$.
	Ceci signifie qu'en temps au plus $\O(nm)$ il y a une augmentation.
	Puisqu'on ne peut pas augmenter plus de $n$ fois, on a bien le résultat.
\end{proof}

On peut améliorer la complexité de cet algorithme.
Batinski a donné un algorithme pour une complexité en $\O(n^{3})$ et Tarjan a donné un algorithme en $\O(nm\log n)$.
Il existe un algorithme en $\O(\sqrt{n} m)$ datant de 1980.
On a désormais des algorithmes en $\O(\sqrt{n}m\log_{n}\frac{n^{2}}{m})$ et $\O(n^{w})$ la complexité optimale de multiplication de matrices.

\subsection{Algorithmes de Streaming}
\begin{definition}
	Un algorithme de streaming est un algorithme qui ne peut pas stocker en mémoire toute son entrée en même temps.
	\label{def:streamingalgorithm}
\end{definition}

On se limite ici à un algorithme qui ne peut stocker que $\O(n\mathrm{polylog}(n))$ arêtes pour trouver un couplage maximal.

\begin{thm}
	Il y a un algorithme de streaming qui nécessite $\O\left(\frac{n}{\varepsilon}\log W \right)$ d'espace et renvoie une $\left(\frac{1}{2}-\varepsilon \right)$-approximation.
\end{thm}
\begin{proof}
\begin{algorithm}
	\caption{Algorithme de Streaming pour les Couplages Maximaux}
	\begin{description}
		\item[Initialisation] On pose $\pi(u) = 0$ pour tout $u \in V$
			On crée une pile $S$ vide.
		\item[Phase de Streaming]
			Tant qu'une nouvelle arête $e =(a, b)$ arrive:
			Si $w(e) > (1 + \epsilon)(\pi(a) + \pi(b))$:
			\begin{itemize}
				\item $\Delta = w(e) - (1 + \varepsilon)(\pi(a) + \pi(b))$
				\item $\pi(a) += \Delta$
				\item $\pi(b) += \Delta$
			\end{itemize}
		\item[Phase Finale] On effectue un algorithme glouton inverse en enlevant les arêtes de $S$ une par une et en les ajoutant dans $M$ si possible.
	\end{description}
	\label{alg:streamingmatching}
\end{algorithm}

\begin{lemme}
	On a:
	\begin{equation*}
		\left(\sum_{v \in V} \pi(v) \right)\left(1 + \varepsilon \right) \geq \max_{M \ couplage} w(M)
	\end{equation*}
	\label{lemma:streammatching1}
\end{lemme}
\begin{proof}
	On a le problème de programmation linéaire suivant:
	\begin{align*}
		\max \sum_{e \in E} w(e)\chi_{e}\\
		\forall v \in V \sum_{e \in \delta(v)} \chi_{e} \geq 1\\
		\forall e \in E \chi_{e} \geq 0
	\end{align*}
	dont le dual est:
	\begin{align*}
		\min \sum_{v \in V}\pi(v)\\
		\pi(a) + \pi(b) \geq w(e) \forall e=(a, b) \in E\\
		\pi(v) \geq 0 \forall v \in V
	\end{align*}
	On a le résultat par dualité faible en vérifiant que $(1 + \epsilon)\pi$ convient.
\end{proof}

\begin{lemme}
	Le couplage résultat $M$ vérifie l'inégalité:
	\begin{equation*}
		w(M) \geq \frac{1}{2}\left(\sum_{v\in V}\pi(v) \right)
	\end{equation*}
\end{lemme}
\begin{proof}
	L'algorithme va créer un pile de poids pour chaque noeuds (les $\Delta$ consécutifs des arêtes mettant en jeu le sommet).
	En particulier
\end{proof}

On a alors:
\begin{align*}
	w(M) \leq \frac{1}{2(1+\varepsilon)}opt\\
	\geq \left(\frac{1}{2} - 3\varepsilon \right)opt
\end{align*}

\medskip

Pour ce qui est l'espace, il y a toujours au plus $\log_{1 + \epsilon}W$ $\Delta$ dans la fonction de poids, et on a $n$ sommets.
En effet, à chaque fois qu'on ajoute un $\Delta$, la valeur augmente de $1 + \epsilon$ au moins.
On a le résultat en prenant un développement limité.
\end{proof}


\section{Global Min-Cut}
On s'intéresse au problème suivant:
\problemStatement{Global Min-Cut}{Input={$G=(V, E)$ avec une fonction de capacité $E\to \R^{+}$}, Output={$\emptyset\subsetneq U\subsetneq V$ tel que $c\left(\delta(U)\right)$ est minimal.}}
Dans le cas d'un graphe orienté, on cherche à minimiser $c\left(\delta^{+}(U)\right)$.

Il est clair que résoudre le problème dans le cadre d'un graphe orienté suffit.
On va simplifier le problème pour résoudre le problème suivant:
\problemStatement{Minimal $s$-Cut}{Input={$G=(V, E)$ avec une fonction de capacité $E\to \R^{+}$, $s\in V$}, Output={$\emptyset\subsetneq U\subsetneq V$ tel que $s \in U$ et $c\left(\delta(U)\right)$ est minimal.}}

\begin{proposition}
	On peut résoudre \textsc{Global Min-Cut} en résolvant deux fois \textsc{Minimal $s$-cut}, pour tout $s$.
	\label{prop:minscut}
\end{proposition}
\begin{proof}
	On résout le problème pour un certain $s$, puis on le résout sur le graphe renversé.
\end{proof}

Similairement, on va trouver un autre problème plus simple encore pour résoudre \textsc{Minimal $s$-cut}.
\problemStatement{Minimal $X$-$t$-cut}{Input={$G=(V, E)$ avec une fonction de capacité $E\to \R^{+}$, $t\in V$ et $X \subsetneq V\setminus \{t\}$}, Output={$\emptyset \subsetneq U \subseteq V$ tel que $X \subseteq U \subseteq V\setminus \{t\}$ et $c\left(\delta(U)\right)$ est minimal.}}

\begin{proposition}
	Si on résout \textsc{Minimal $X$-$t$-cut} $n$ fois, on peut résoudre \textsc{Minimal $s$-cut}.
	\label{prop:minxtcut}
\end{proposition}

On propose pour cela l'algorithme suivant:
\begin{algorithm}
	\caption{Minimal $s$-cut par Minimal $X$-$t$-cut}
	\begin{description}
		\item[Initialisation] Soit $X = \{s\}$.
		\item[Boucle] Tant que $X \neq V$, on choisit $t \in V\setminus X$ arbitrairement.
			Soit $S_{\abs{X}}$ le résultat de \textsc{Minimal $X$-$t$-cut}.
			On augmente $X = X \cup \{t\}$
		\item[Renvoi] On renvoie $\displaystyle\argmin_{1 \leq t \leq \abs{X} - 1} S_{t}$.
	\end{description}
\end{algorithm}

\begin{proof}
	Considérons la première itération (notée $\gamma$) où un sommet de l'autre côté de la $s$-cut optimale est choisi pour $t$.
	Alors $c\left(\delta^{+}(S_{\gamma}\right) \leq c\left(\delta^{+}(\mathrm{opt s-cut}\right)$ et $c\left(\delta^{+}(\mathrm{opt s-cut}\right) \leq c(\delta^{+}(S_{\gamma}))$ car les deux coupes vérifient les deux propriétés.
\end{proof}

On va pouvoir résoudre toutes les \textsc{Minimal $X$-$t$-cut} en résolvant un seul problème de flot.
On rappelle que $exces(v) = \sum_{e \in \delta^{-}(v)} f(e) - \sum_{e \in \delta^{+}(v)} f(e)$.
On va construire un algorithme se basant sur l'algorithme Preflow Push \ref{alg:pushrelabel} qui va vérifier les deux invariants suivants:
\begin{enumerate}
	\item Sur $V \setminus X$, $exces(v) \geq 0$.
	\item Pour tout $v \in X$, $d(v) = n$, $\forall u, v \in G(f), d(u) \leq d(v) + 1$, $d(t) \leq \abs{X} - 1$ et $d(t) \leq d(u)$.
\end{enumerate}

\begin{definition}
	Le seau associé à $k$ est l'ensemble $B(k) = \left\{v \mid d(v) = k\right\}$.
	$k$ est un niveau de coupe si pour tout $u \in B(k), (u, v) \in G(f), d(v) \geq d(u)$.
	\label{def:bucketlevel}
\end{definition}

\begin{proposition}
	Si $k$ est un niveau de coupe, l'ensemble des $u$ tels que $d(u) \geq k$ est une coupe dans le graphe résiduel $G(f)$.
	\label{prop:cutleveliscut}
\end{proposition}

\begin{proposition}
	Soit $k < n$ un niveau de coupe. Si pour tout $v \neq t$ tel que $d(v) < k$ on a $\mathrm{exces}(v) = 0$, alors $\{u \mid d(u) \geq k\}$.
	\label{prop:cutlevelexcess}
\end{proposition}
\begin{proof}
	Débrouille-toi, normalien~!
\end{proof}

\begin{algorithm}
	\caption{Hao-Ortin --- $X$-preflow-push}
	\begin{description}
		\item[Initialisation] On pose $X = \{s\}$, $d(s) = n$, $d(v) = 0, \forall v \neq t$ où $t \in V \setminus\{s\}$,
			$l = n - 1$, $f(e) = c(e), \forall e \in \delta^{+}(s)$.
		\item[Boucle] Tant que $X \neq V$, on applique l'algorithme preflow-push (ou push-relabel) avec 3 modifications:
			\begin{enumerate}
				\item Un sommet $u \neq t$ est actif si et seulement si $d(u) < l$ et $\mathrm{exces}(u) > 0$.
				\item Si $u$ va être ré-étiqueté et que $\abs{B(d(u))} = 1$, on ne ré-étiquette pas $u$ et on pose $l = d(u)$.
				\item Si $u$ est ré-étiqueté pour avoir $d(u) > l$, on pose $l = n - 1$.
			\end{enumerate}
			On pose $S_{\abs{X}} = \left\{u \mid d(u) \geq l\right\}$, $X = X \cup \{t\}$, $d(t) = n$ et $\forall e \in \delta^{+}(t), f(e) = c(e)$.
			On choisit pour le nouveau $t$ celui celui avec le minimum de $d(t)$ parmi les sommets de $V \setminus X$.
			Si $d(t) \geq l$ on pose en plus $l = n - 1$.
		\item[Renvoi] On renvoie $\argmin_{1 \leq \alpha \leq \abs{X} - 1} c\left(\delta^{+}S_{\alpha} \right)$.
	\end{description}
	\label{alg:haoortin}
\end{algorithm}

\begin{lemme}
	Les seaux non vides (de niveau $< n$) sont toujours consécutifs.
	\label{lemme:haoortin1}
\end{lemme}
\begin{proof}
	Il est clair que le lemme est vrai initialement.
	Par induction, il y a trois dangers possibles:
	\begin{enumerate}
		\item Un sommet est retiré par ré-étiquetage du niveau $k$. Ceci ne peut pas arriver puisqu'on ne ré-étiquette pas le dernier sommet d'un seau.
		\item Un sommet est ajouté par ré-étiquetage au niveau $k$. Par définition, on ne peut pas aller plus haut que le minimum plus un.
		\item Un $t$ est ajouté à $X$. Puisque l'hypothèse d'induction est valide, et puisqu'on a toujours $d(v) >= d(t)$, le seau en $d(t) - 1$ est vide et: ou bien tous les seaux autres sont vides, ou bien le seau en $d(t) + 1$ est non vide et l'hypothèse reste vraie.
	\end{enumerate}
\end{proof}

\begin{lemme}
	On a toujours $d(t) \leq \abs{X} - 1$.
	\label{lemme:haoortin2}
\end{lemme}
\begin{proof}
	Soit $t'$ le nouveau $t$ choisi.
	On a $d(t') \leq d(t) + 1$ par le lemme \ref{lemme:haoortin1}.
	Par induction, $d(t') \leq d(t) - 1 \leq \abs{X} = \abs{X \cup \{t\}} - 1$
\end{proof}

Le lemme précédent montre que l'algorithme \ref{alg:haoortin} vérifie les invariants précédents.

\begin{lemme}
	Pour $u \notin X$, on a $d(u) \leq n - 2$
	\label{lemme:haoortin3}
\end{lemme}
\begin{proof}
	Soit $v$ le sommet de $V \setminus X$ avec la plus grande étiquette.
	Par le lemme \ref{lemme:haoortin1}, il y a au plus $n - \abs{X} - 1$ seaux entre $d(t)$ et $d(v$ d'où, par le lemme \ref{lemme:haoortin2}, $d(u) \leq d(t) + n - \abs{X} - 1 \leq n - 1 - 1 = n - 2$.
\end{proof}

\begin{corollaire}
	$n - 1$ est un niveau de coupe.
	\label{corollaire:haoortin1}
\end{corollaire}

\begin{lemme}
	On a toujours $d(t) < l \leq n - 1$ et $l$ est toujours un niveau de coupe.
	\label{lemme:haoortin4}
\end{lemme}
\begin{proof}
	On va procéder par induction:
	\begin{enumerate}
		\item Dans le cas on prend $l = n - 1$, par \ref{corollaire:haoortin1}, on a le résultat.
		\item Dans le cas où on pose $l = d(u)$, puisque $u$ est le dernier de son seau, tous ses voisins vérifient $d(v) \geq d(u) > d(t)$ d'où le résultat.
	\end{enumerate}
\end{proof}

\begin{thm}
	La cut de renvoi est une coupe $s$-minimale.
\end{thm}
\begin{proof}
	Lorsque l'algorithme s'arrête, tous les sommets d'étiquette $< l$ sont inactifs et ont donc $\mathrm{exces} = 0$, d'où le résultat par la proposition \ref{prop:cutlevelexcess}
\end{proof}

\begin{thm}
	L'algorithme demande un temps $\O(n^{2}m)$.
\end{thm}
\begin{proof}
	\begin{itemize}
		\item Les \emph{vrais} ré-étiquetages (ceux où $d(u)$ croît) demandent un temps $\O(n^{2})$
		\item Les \emph{faux} ré-étiquetages (ceux où on change la valeur de $l$) demandent un temps $\O(n^{2})$: en effet chaque fois qu'on ré-étiquette un sommet autre que $t$, son étiquette augmente d'au moins 1 et l'augmentation est inférieure à $n - 2$.
			L'étiquette de $t$ est $n$ à la fin de l'itération et n'augmente pas plus.
		\item Les pushs saturants et non-saturants ne voient pas leur complexité changer.
	\end{itemize}
\end{proof}

On peut trouver un algorithme un peu plus efficace en $\O(n^{2}m\log_{n}\frac{n^{2}}{m})$.

\subsection{Karger}
On présente ici un algorithme qui renvoie une cut correcte avec une bonne probabilité.
\begin{algorithm}
	\caption{Karger}
	Tant que $\abs{V} > 2$, on choisit $e = (u, v)$ uniformément au hasard. On contracte $e$ en un sommet et on renvoie la cut définie par les $2$ sommets restants.
\end{algorithm}

\begin{lemme}
	Étant donné une coupe optimale $\left(S, \bar{S} \right)$, la probabilité de prendre une arête dans $E\cap\left( S\times \bar{S} \right)$ est au plus
	\label{lemme:karger1}
\end{lemme}
\begin{proof}
	On a: $opt \leq \min_{v \in V} d(v) = \delta$.
	Ainsi, $\P(échec) \leq \frac{\delta}{\abs{E}} \leq \frac{\delta}{\delta \abs{V}/2}\leq \frac{2}{\abs{V}}$.
\end{proof}

\begin{lemme}
	La probabilité qu'on ne prenne jamais d'arête dans $E\cap \left(S \times \bar{S} \right)$ est $\geq \frac{1}{\binom{n}{2}} \geq \frac{2}{n^{2}}$.
	\label{lemme:karger2}
\end{lemme}
\begin{proof}
	Par le lemme \ref{lemme:karger1}, la probabilité $p$ vérifie:
	\begin{equation*}
		\begin{aligned}
			p \geq & \left(1 - \frac{2}{n} \right)\left(1 - \frac{2}{n - 1} \right)\cdots \left( 1 - \frac{2}{3} \right)\\
			=& \frac{n - 2}{n}\frac{n - 3}{n - 1}\frac{n - 4}{n - 2}\cdots\frac{2}{4}\frac{1}{3}\\
			=& \frac{2}{n(n-1)} = \frac{1}{\binom{n}{2}}
		\end{aligned}
	\end{equation*}
\end{proof}

\begin{thm}
	Si on répète l'algorithme de Karger $10\times n^{2}\log n$ fois, on réussit au moins une fois avec probabilité $1 - \frac{1}{n^{\O(1)}}$
\end{thm}
\begin{proof}
	La probabilité $p$ d'échec vérifie:
	\begin{equation*}
		p \leq \left(1 - \frac{2}{n^{2}} \right)^{10n^{2}\log(n)} \leq \left(e^{-1}\right)^{5\log n} = \frac{1}{n^{5}}
	\end{equation*}
\end{proof}

\begin{thm}
	Le nombre de coupes minimales est inférieur à $\binom{n}{2}$ et cette borne est optimale.
\end{thm}
\begin{proof}
	Toutes les preuves précédentes étant vraies pour une coupe minimale quelconque, en sommant les probabilités on a le résultat.
	En prenant un cycle on a le résultat, puisqu'une coupe est équivalente à choisir deux sommets
\end{proof}

\section{Matroïdes}
On notera ici $I + x = I\cup \{x\}$ et $I - x = I \setminus \{x\}$.

\begin{definition}
	$M = \left(U, \mathbb{I} \right), \mathbb{I} \subseteq 2^{U}$ est un système indépendant si:
	\begin{itemize}
		\item[$A_{0}$] $\emptyset \in \I$
		\item[$A_{1}$] $u \subseteq V \in \I \Rightarrow u \in \I$.
	\end{itemize}
	Les éléments de $\I$ sont appelés ensembles indépendants.
	Si de plus, $M$ vérifie l'une des conditions équivalentes suivantes, $M$ est appelé un matroïde:
	\begin{itemize}
		\item[$A_{2}$] Pour $\bar{U} \subseteq U$ tous les ensembles indépendants maximaux dans $U$ ont la même taille.
		\item[$A_{2}'$] Étant donné $I, J \in \I$, si $\abs{I} > \abs{J}$ il existe $x \in J \setminus I$ tel que $I + x \in \I$.
		\item[$A_{2}'$] Pour $I, J \in I$ tels que $\abs{J\setminus I} = 2$ et $\abs{I\setminus J} = 1$ alors il existe $x \in J \setminus I$ tel que $I + x\in \I$.
	\end{itemize}
\end{definition}
Les matroïdes ont été introduits dans différents domaines de manière indépendante dans les années 30.

\begin{exemple}[Matroïde de Graphe]
	Étant donné un graphe $(V, E)$, en considérant $U = E$ et $\I$ l'ensemble des forêts, $\left(U, \I\right)$ est un matroïde.
	En effet, toutes les forêts maximales sont de taille $\abs{U} - \mathrm{cc}(G)$ où $\mathrm{cc}(G)$ est le nombre de composantes connexes de $G$.
\end{exemple}

\begin{exemple}[Matroïde Linéaire]
	Étant donné une matrice $A = \left(C_{1}, \ldots, C_{n}\right)$, $U$ l'ensemble de ses colonnes et $\I$ l'ensemble des sous familles des colonnes linéairement indépendantes forment un matroïde.
	En effet, cela à revient à dire que toutes les bases d'un même espace ont même cardinal.
\end{exemple}

\begin{exemple}[Matroïde de Partitions]
	Étant donné un ensemble $U = \sqcup U_{i}$, on prend $\I$ l'ensemble des ensembles dont l'intersection avec chacun des $U_{i}$ est de cardinal au plus $1$.
\end{exemple}

\begin{definition}
	\begin{itemize}
		\item Le rang de $V \subseteq U$, noté $\gamma_{M}(V)$ l'entier $\max_{I \in \I \cap P(V)} \abs{I}$.
		\item Si $\abs{I} = \gamma_{M}(V)$, $I$ est une base de $V$.
		\item Si $\abs{I} = \gamma_{M}(U)$, $I$ est une base.
		\item $C \subseteq U$ est un circuit si $C$ est un ensemble dépendant minimal.
		\item On appelle l'ensemble engendré par $V$ l'ensemble $\mathrm{span}_{M}(V) = \left\{e \mid \gamma_{M}(V) = \gamma_{M}(V + e)\right\}$.
	\end{itemize}
\end{definition}

\problemStatement{}{Entrée={Matroïde $M = \left(U, \I \right)$ avec une fonction de poids $w : U \to \R$}, Sortie={Ensemble indépendant de poids maximal.}}

\begin{algorithm}
	\caption{Algorithme Glouton -- Edmonds}
	\begin{algorithmic}
		\State $I \gets \emptyset$
		\While {$\exists x \in U \setminus I$ tel que $w(x) > 0$ et $I + x \in \I$}
			\State {$I = I + e$ où $e = \argmax w(e)$}
		\EndWhile
	\end{algorithmic}
\end{algorithm}

\begin{thm}[Rado - Edmonds]
	L'algorithme glouton renvoie une solution optimale.
\end{thm}
\begin{proof}
	Supposons que l'algorithme renvoie $I = \{e_{1}, \cdots, e_{h}\}$ (les éléments étant ordonnés par ordre décroissants de poids).
	Supposons qu'on ait un ensemble $OPT = \{f_{1}, \cdots, f_{s}\}$ de poids optimal.
	Supposons $w(OPT) > w(I)$.
	Alors, on choisit le plus petit $l$ tel que $w(f_{l}) > w(e_{l})$. S'il n'en existe pas, prenons $l = h$.
	Alors, $\{e_{1},\ldots, e_{l - 1}\}$ est un ensemble indépendent maximal dans $\{e_{1}, \ldots, e_{l - 1}, f_{1}, \ldots, f_{l}\}$.
	Or, $f_{1}, \ldots, f_{l}$ est indépendant: on a une contradiction par la propriété $A_{2}$ des matroïdes.
\end{proof}

La preuve de cette algorithme nous donne une nouvelle condition équivalente pour la définition d'un matroïde:
\begin{proposition}
	L'algorithme glouton renvoie un ensemble indépendant de poids maximal pour toute fonction de poids si et seulement si $(U, \I)$ est un matroïde.
	\label{prop:greedymatroid}
\end{proposition}
\begin{proof}
	Il nous reste à montrer que cette propriété implique l'une des propriétés précédentes.
	Supposons que la propriété ci-dessus soit vérifiée mais que $\left(U, \I\right)$ ne soit pas un matroïde.
	Il existe $I, J$ maximaux tels que $\abs{I} > \abs{J}$.
	Alors, on pose $w(e) = 1$ pour $e \in I \cup J$, $w(e) = 0$ sinon.
	L'algorithme glouton ne spécifiant pas comment résoudre les conflits d'égalité, on pourrait avoir $J$ en résultat.
\end{proof}

\problemStatement{Intersection de Matroïdes}{Entrée={$M_{1} = \left(U, \I_{1}\right), M_{2} = \left(U, \I_{2}\right)$}, Sortie={Le plus grand $I \in \I_{1} \cap \I_{2}$}}

On peut voir le problème de couplage dans les graphes bipartis ou le problème de couverture par un arbre coloré comme des problèmes d'intersection de matroïdes.

Dans le cas du problème de couverture d'un arbre coloré par exemple, on prend le matroïde du graphe et le matroïde de partition sur les arêtes en considérant la partition par couleurs.
Pour la couverture par un arbre dirigé, on prend le matroïde du graphe et le matroïde de la partition des arêtes selon leur destination.

On note que l'intersection de deux matroïdes n'est pas un matroïde.
On note par ailleurs que résoudre l'intersection de 3 matroïdes permet de résoudre des problèmes NP-Complets (réduction aux Chemins Hamiltonniens, ou 3D-Matching)

\begin{lemme}
	Si $I \in \I$, $I + x$ crée au plus un circuit $C$.
\end{lemme}
\begin{proof}
	Par l'absurde, soit $A$ le plus petit contre-example. $A + x$ contient deux circuits $C_{1}, C_{2}$.
	On peut supposer $A + x = C_{1} \cup C_{2}$.
	Il existe par ailleurs $a \in C_{1} \setminus C_{2}$ et $b \in C_{2} \setminus C_{1}$.
	Considérons $A' = A + x - a - b$.
	Alors $A' \in I$ et $A$ et $A'$ sont tous deux des ensembles indépendants maximaux dans $A + x$.
\end{proof}

\begin{corollaire}
	Si $I \in \I$, $I + x$ contient $C$, alors $I + x - y \in \I$, pour tout $y \in C$.
\end{corollaire}

\begin{definition}
	On définit le graphe auxiliaire $G(I) = \left(I \sqcup U \setminus I, E\right)$ d'un ensemble indépendant $I \in \I_{1} \cap \I_{2}$ où $E$ est un ensemble d'arêtes orientées défini par:
	\begin{itemize}
		\item Si $I + x$ contient un circuit $C_{2}$ dans $M_{2}$, on met une arête de $y$ vers $x$ pour tout $y \in C_{2} - x$.
		\item Si $I +x $ contient un circuit $C_{1}$ dans $M_{1}$, on met une arête de $x$ vers $y$ pour tout $y \in C_{1} - x$.
	\end{itemize}
	On définit de plus $X_{2} = \{e \mid I + e \in \I_{2}\}$ et $X_{1} = \{e \mid I + e\in \I_{1}\}$ des parties de $U \setminus I$.
	\label{def:auxilarygraph}
\end{definition}

\begin{algorithm}
	\caption{Edmonds}
	\begin{algorithmic}
		\State {$I \gets \emptyset$}
		\While {il existe un chemin de $X_{2}$ vers $X_{1}$ dans $G(I)$}
		\State {Trouver le plus court chemin $p$}
		\State {$I = I \oplus P$}
		\EndWhile
	\end{algorithmic}
\end{algorithm}

\begin{thm}
	Quand l'algorithme s'arrête, $I$ est le plus grand ensemble indépendent commun.
\end{thm}

\begin{lemme}
	Soit $P = e_{0}f_{1}e_{1}f_{2}\cdots f_{t}e_{t}$ le plus court chemin dans $G(I)$ de $X_{2}$ vers $X_{1}$.
	Soit $I_{0} = I + e_{0}$, $I_{t} = I_{0} + \{e_{0}, \ldots, e_{t}\} - \{f_{1}, \ldots, f_{t} \}$
	Alors pour tout $i$, $I_{i}\in \I$.
\end{lemme}
\begin{proof}
	Par induction sur $i$, $I_{0} \in \I_{2}$ par définition de $X_{2}$.
	Pour $i > 0$, $I_{j - 1} + f_{j}$ pour $j \geq i$ a le même circuit que $I + f_{j}$ car aucun des circuits dans $I + e_{z}$ pour $z < j$ n'implique d'éléments de $\{f_{1}, \ldots, f_{z - 1}\}$ puisque $P$ est le plus court chemin.
	En effet, sinon, on aurait un raccourci et une arête d'un certain $f_{k}$ vers $e_{z}$, ce qui permettrait de raccourcir $P$.
\end{proof}

\begin{lemme}
	On a $I \oplus P \in \I_{1}$.
\end{lemme}
\begin{proof}
	En inversant les indices dans la preuve précédente.
\end{proof}

\begin{thm}[Minimax d'Edmonds]
	On a:
	\begin{equation*}
		\max_{I \in \I_{1} \cap \I_{2}} \abs{I} = \min_{A \subseteq U}\gamma_{1}(A) + \gamma_{2}\left(U\setminus A\right)
	\end{equation*}
\end{thm}
\begin{proof}
	Prouvons d'abord la dualité faible:
	Par définition du rang*, $I \cap A$ étant indépendant dans $M_{1}$ et $I\cap U\setminus A$ étant indépendant dans $M_{2}$, $\abs{I\cap A} \leq \gamma_{1}(A)$ et $\abs{I\cap U\setminus A} \leq \gamma_{2}(U\setminus A)$.
	La dualité forte découlera de la correction de l'algorithme d'Edmonds.
\end{proof}

\begin{proof}[Démonstration de la Correction de l'Algorithme d'Edmonds]
	Quand l'algorithme s'arrête, il n'y a pas de chemin.
	Soit $R$ l'ensemble des sommets atteignables depuis $X_{2}$.
	Alors $R \cap I$ est une base de $R$ dans $M_{1}$.
	\begin{proof}
	En effet, $R \setminus I \cap X_{1} = \emptyset$ donc, pour $e \in R\setminus I$, $I + e$ a un circuit $C$.
	Or, $C$ n'implique que $e$ et $R\cap I$, puisque $G(I)$ est biparti.
	Ceci prouve bien que $R\cap I$ est de cardinal maximal et donc une base.
	\end{proof}
	Par symétrie, $I\setminus R$ est une base de $R^{\complement}$ dans $M_{2}$.
	Ainsi, $\abs{I} = \abs{R \cap I} + \abs{I\setminus R} = \gamma_{1}\left(R\right) + \gamma_{2}\left(R^{\complement}\right)$.
	Donc $I$ fait égalité dans la formule d'Edmonds.
\end{proof}

\begin{thm}
	L'algorithme demande un temps polynomial.
\end{thm}

\begin{definition}
	Étant donné un matroïde $M = \left(U, \I \right)$, on peut définir un autre matroïde appelé \emph{matroïde dual} $M^{*} = \left(U, \I^{*}\right)$ où $\I^{*} = \left\{A \mid U\setminus A \text{ contient une base dans } M \right\}$.
	\label{def:dualmatroid}
\end{definition}

Pour le matroïde graphique, c'est l'ensemble des ensembles d'arêtes qu'on peut retirer en gardant chaque composante du graphe connexe.
Un circuit devient donc une coupe.

\begin{proof}
	Étant donné $A \subseteq U$, on pose $I$ l'ensemble indépendent maximal dans $A$ considéré dans $M^{*} = \left(U, \I^{*}\right)$.
	On choisit une base $X$ de $U \setminus A$ dans $M$.
	Notons $B = X + A \setminus I$.
	Alors, $B$ est une base de $U$ dans $M$.
	En effet: CÉPAFINI
\end{proof}<++>

\end{document}
