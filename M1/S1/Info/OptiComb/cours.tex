\documentclass[math, info]{cours}
\title{Optimisation Combinatoire}
\author{Chien-Chen Huang}

\begin{document}
\bettertitle
\section{Max-Flow (min-cut)}
\subsection{Ford-Fulkerson}
\begin{definition}
	Soit $G = \left(V, E\right)$ un graphe orienté, $c: E \to \R^{+}$ une fonction de capacité et $s, t \in V$ deux sommets terminaux,
	$f: E\to \R^{+}$ est un flot si:
	\begin{enumerate}
		\item $0 \leq f(e) \leq c(e), \forall e \in E$
		\item $\sum_{e \in \delta^{-}(v)} f(e) = \sum_{e \in \delta^{+}(v)} f(e), \forall v \in V \setminus \{s, t\}$ (Conservation du Flot).
	\end{enumerate}
	\label{def:flot}
\end{definition}

On va s'intéresser au problème suivant:
\problemStatement{Max-Flow}{Entrée={$G = \left(V, E\right)$ un graphe orienté, $c: E \to \R^{+}$ une fonction de capacité et $s, t \in V$ deux sommets terminaux}, Sortie={Une fonction de flot $f$ maximale, c'est à dire avec un volume maximal: $\sum_{e \in \delta^{+}(s)} f(e) - \sum_{e \in \delta^{-}(s)} f(e)$}}

\begin{thm}
	On rappelle qu'obtenir un flot maximal est équivalent à obtenir une coupe de poids minimal.
\end{thm}

On définit pour cela le réseau résiduel d'une fonction de flot $f$:
\begin{definition}
	Étant donné un flot $f$, pour chaque arrête $e = (u, v) \in E$, on définit:
	\begin{align*}
		(u, v) \in F \text{ si } c(e) - f(e) > 0 \\
		(v, u) \in F \text{ si } f(e) > 0
	\end{align*}
	Dans le premier cas, on définit $u(e) = c(e) - f(e)$. Dans le deuxième cas on définit $u(e) = f(e)$.
	\label{def:residualnetwork}
\end{definition}

\begin{proposition}
	Si on pousse de $u$ à $v$, i.e. si $f(((u, v)) > 0$, alors $(v, u)$ doit apparaître dans le réseau résiduel.
	\label{prop:pakompri}
\end{proposition}

On propose alors l'algorithme de Ford-Fulkerson, se basant sur des chemins augmentants pour résoudre le problème:
\begin{algorithm}
	\caption{Ford-Fulkerson}
	\label{alg:fordfulkerson}
	Tant que $G(f)$ a un chemin de $s$ à $t$ noté $p$, on pousse le plus possible le long du chemin $p$.
\end{algorithm}

\begin{thm}
	Si l'algorithme de Ford-Fulkerson termine, alors $f$ est un flot maximal.
\end{thm}

\begin{proof}
	Soit $U \subseteq V$ l'ensemble des sommets atteignables depuis $s$ dans $G(f)$. On a par \ref{prop:pakompri}
	\begin{equation*}
		\begin{aligned}
			\sum_{e \in \delta^{+}(s)} f(e) - \sum_{e\in \delta^{-}(s)}f(e) = & \sum_{e \in \delta^{+}(U)}f(e) - \sum_{e \in \delta^{-}(U)}f(e) \\
			=                                                                 & \sum_{e\in \delta^{+}(U)} c(e) = \text{cut size de U}
		\end{aligned}
	\end{equation*}
\end{proof}
Toutefois la complexité de cet algorithme dépend de la valeur du flot maximum, et celui-ci ne termine même pas pour des réels.

\subsection{Push-Relabel}
On va chercher un algorithme dont la complexité n'en dépend pas (dit fortement polynomial), ce qui nous amène à la notion de pré-flot:
\begin{definition}
	Soit $G = \left(V, E\right)$ un graphe orienté, $c: E \to \R^{+}$ une fonction de capacité et $s, t \in V$ deux sommets terminaux,
	$f: E\to \R^{+}$ est un pré-flot si:
	\begin{enumerate}
		\item $0 \leq f(e) \leq c(e), \forall e \in E$
		\item $\mathrm{exces}(v) = \sum_{e \in \delta^{-}(v)} f(e) \geq \sum_{e \in \delta^{+}(v)} f(e), \forall v \in V \setminus \{s, t\}$.
	\end{enumerate}
	\label{def:preflot}
\end{definition}

On va essayer de construire un algorithme dont le principe est cette fois ci d'avancer
\begin{definition}
	\begin{itemize}
		\item Un sommet $v \in V\setminus \{s, t\}$ est dite \emph{actif} si $\mathrm{exces}(v) > 0$.
		\item Un étiquetage des sommets $d: V \to \N$ est valide si $\forall (u, v) \in G(f)$ (pour $f$ un pré-flot), on a: $d(u) \leq d(v) + 1$.
		\item Une arête $(u, v) \in G(f)$ est admissible si $d(u) = d(v) + 1$.
	\end{itemize}
	\label{def:activenode}
\end{definition}

On obtient alors l'algorithme suivant, proposé originellement par Andrew Goldberg en 1989:
\begin{algorithm}
	\caption{Push-Relabel}
	\label{alg:pushrelabel}
	\begin{description}
		\item[Initialisation]: On pose $\forall e \in \delta^{+}(s), f(e) = c(e)$, sinon $f(e) = 0$.
		      On pose $d(s) = n, d(v) = 0 \forall v \neq s$.
		\item[Boucle] Tant qu'il existe un sommet actif $v$, on effectue deux actions:
		      \begin{description}
			      \item[\tt Push] S'il existe $(u,v) \in G(f)$ admissible, on pousse $\min \left(\mathrm{exces}(u), u(e)\right)$ selon l'arête $(u, v)$.
			      \item[\tt Relabel] On pose $d(u) = \min_{v\mid (u, v) \in G(f)}d(v) + 1$
		      \end{description}
	\end{description}
\end{algorithm}

On va donc prouver la correction de cet algorithme.
Pour cela, on se base sur les deux lemmes suivants:
\begin{lemme}
	Si $v$ est actif, alors $v$ a un chemin orienté vers $s$ dans $G(f)$.
	\label{lem:preflow1}
\end{lemme}
\begin{proof}
	Soit $X \subseteq V$ l'ensemble des sommets ayant un chemin vers $s$ dans $G(f)$.
	Par l'absurde, il existe $w \in V\setminus X$ actif. On a alors:
	\begin{equation*}
		0 < \sum_{v \in V \setminus X} \left(\sum_{e \in \delta^{-}(v)} f(e) - \sum_{e\in \delta^{+}(v)} f(e)\right) = \sum_{e \in \delta^{-}(v\setminus x)} f(e) - \sum_{e\in \delta^{+}(V\setminus X)} f(e)
	\end{equation*}
	Or, $\sum_{e \in \delta^{-}(v\setminus x)} f(e) = 0$, d'où le résultat.
\end{proof}

\begin{lemme}
	Étant donné un chemin $P$ de $u$ à $v$, alors, $\abs{P} \geq d(u) - d(v)$, si $d$ est valide.
	\label{lem:preflow2}
\end{lemme}
\begin{proof}
	Si $P = v_{0}v_{1}\cdots v_{x}$, puisque $d$ est valide: $d(v_{i}) \leq d(v_{i + 1}) + 1$ pour tout $i$.
	D'où, $d(v_{0}) \leq d(v_{x}) + x$.
	D'où le résultat.
\end{proof}

On obtient un corollaire très utile:
\begin{corollaire}
	Pour tout $n$, $d(u) \leq 2n - 1$.
	\label{cor:preflow1}
\end{corollaire}
\begin{proof}
	Si $u$ est actif et $d(u) = 2n$, tout chemin de $u$ à $s$ est de longueur au moins $n$, ce qui est impossible puisque $\abs{V} = n$.
\end{proof}

\begin{thm}[Correction de Push-Relabel]
	Quand l'algorithme \ref{alg:pushrelabel} s'arrête, on obtient un max-flow.
	\label{thm:pushrelabel}
\end{thm}
\begin{proof}
	Il n'y a jamais de chemin de $s$ à $t$ dans $G(f)$ par \ref{lem:preflow2}.
	Par ailleurs, il n'y a pas de sommet actif à l'arrêt de l'algorithme, ce qui signifie qu'on a bien un véritable flot.
	La preuve de correction de \ref{alg:fordfulkerson} s'applique donc.
\end{proof}

\begin{thm}[Complexité de Push-Relabel]
	L'algorithme \ref{alg:pushrelabel} s'arrête en temps $\O(V^{2}E)$.
\end{thm}

\begin{proof}
	On a toujours $3$ opérations:
	\begin{itemize}
		\item Le \texttt{Relabel} qui prend un temps $\O(V^{2})$ (au plus $(n-2) \times (2n-1)$ opérations).
		\item Le \texttt{Push Saturant} (push qui permet à $f((u, v))$ d'atteindre $c((u, v))$). Celui-ci va supprimer l'arête $(u, v)$ de $G(f)$.
		      Pour que l'arc soit réinséré dans $G(f)$ pour un autre push saturant,
		      $v$ doit d'abord être réétiqueté.
		      Ensuite, après un push sur $(v, u)$, $u$ doit être réétiqueté.
		      Au cours du processus, $d(u)$ augmente d'au moins $2$
		      Il y a donc $\O(V)$ push saturants sur $(u, v)$ et donc $\O(VE)$ push saturants au total.
		\item Le \texttt{Push Non-Saturant} qu'on effectue un nombre $\O(V^{2}E)$ de fois.
		      En effet, borner le nombre de push non-saturants peut se faire à partir d'un argument de potentiel.
		      On utilise la fonction de potentiel $\Phi =\sum_{v \text{ actif}} d(v)$.
		      Il est clair que $\Phi = 0$ à l'initialisation et reste toujours positive durant l'exécution.
		      Par ailleurs, un push non-saturant diminue $\Phi$ d'au moins $1$.
		      De plus, le relabel et le push augmentent $\Phi$ d'au plus $1$ et d'au plus $(2V - 1)$ respectivement.
		      On a donc: $\Phi \leq \left(2V-1\right)\left(V - 2\right) + \left(2V - 1\right)\left(2VE\right)$.
		      On a donc: $\Phi \leq \O\left(V^{2}E\right)$.
	\end{itemize}
	L'algorithme prend donc un temps $\O\left(V^{2}E\right)$.
\end{proof}

Pour essayer d'améliorer l'algorithme on propose la version suivante:
\begin{algorithm}
	\caption{Push-Relabel +}
	\label{alg:pushrelabel+}
	Boucle: On choisit le sommet actif $v$ avec la plus haute étiquette, on effectue deux actions:
	\begin{description}
		\item[\tt Push] S'il existe $(u,v) \in G(f)$ admissible, on pousse $\min \left(\mathrm{exces}(u), u(e)\right)$ selon l'arête $(u, v)$.
		\item[\tt Relabel] On pose $d(u) = \min_{v\mid (u, v) \in G(f)}d(v) + 1$
	\end{description}
\end{algorithm}
Il est clair que l'algorithme reste correcte, toutefois, on change la complexité pour de $V^{2}E$ à $V^{3}$.
On peut même encore améliorer la complexité pour obtenir $\O(V^{2}\sqrt{E})$, et même $\O(V^{1 + o(1)}\log(E))$

\subsection{Edmonds-Karp}
\begin{algorithm}
	\caption{Edmonds-Karp}
	\label{alg:edmondskarp}
	Pour cet algorithme, on applique Ford-Fulkerson en choisissant le plus court des chemins de $s$ à $t$.
\end{algorithm}
\begin{thm}[Complexité d'Edmonds-Karp]
	L'algorithme de Edmonds-Karp prend un temps $\O(VE^{2})$.
\end{thm}

Dans la suite, on note $f_{0}, f_{1},\cdots$ les flots obtenus de sorte que $f_{i + 1}$ est obtenu du plus court chemin $P_{i}$ dans $G(f_{i})$.

\begin{lemme}
	$\forall i$:
	\begin{itemize}
		\item $\abs{P_{i}} \leq \abs{P_{i + 1}}$
		\item Si $P_{i}$ et $P_{i + 1}$ utilisent deux arcs opposés (i.e. $(u, v)$ et $(v, u)$), alors $\abs{P_{i}} + 2 \leq \abs{P_{i + 1}}$.
	\end{itemize}
	\label{lem:edmondskarp1}
\end{lemme}
\begin{proof}
	On pose $H = P_{i} \cup P_{i + 1}$ où les arcs opposés sont supprimés.
	On ajoute alors $2$ arcs supplémentaires de $t$ à $s$.
	Comme alors $H$ est eulérien, il existe deux chemins disjoints $q_{1}, q_{2}$ de $s$ à $t$ dans $H$.
	Notons que toutes les arêtes de $H$ (sauf les arêtes de $t$ à $s$) sont dans $G(f_{i})$.
	On a de plus $\abs{P_{i}} \leq q_{1}, q_{2}$ d'où
	\begin{equation*}
		2\abs{P_{i}} \leq \abs{q_{1}} + \abs{q_{2}} \leq \abs{H} \leq \abs{P_{i}} + \abs{P_{i + 1}} - 2
	\end{equation*}
\end{proof}

\begin{lemme}
	Soit $l < k$ tel que $P_{l}$ et $P_{k}$ utilisent des arcs opposés.
	Alors, $\abs{P_{l}} + 2 \leq \abs{P_{k}}$.
	\label{lem:edmondskarp2}
\end{lemme}
\begin{proof}
	La preuve précédente peut être aisément adaptée:
	On peut supposer que pour $l < i < k$, $P_{i}$ n'a pas d'arcs opposés avec $P_{k}$, sinon le résultat se déduit par récurrence par le lemme précédent.
	On pose $H = P_{l} \cup P_{k}$ où les arcs opposés sont supprimés.
	On ajoute alors $2$ arcs supplémentaires de $t$ à $s$.
	Comme alors $H$ est eulérien, il existe deux chemins disjoints $q_{1}, q_{2}$ de $s$ à $t$ dans $H$.
	Notons que toutes les arêtes de $H$ (sauf les arêtes de $t$ à $s$) sont dans $G(f_{l})$.
	On a de plus $\abs{P_{l}} \leq q_{1}, q_{2}$ d'où
	\begin{equation*}
		2\abs{P_{l}} \leq \abs{q_{1}} + \abs{q_{2}} \leq \abs{H} \leq \abs{P_{l}} + \abs{P_{k}} - 2
	\end{equation*}
\end{proof}

\begin{lemme}
	Un arc dans $G(f)$ peut être une arête bottleneck (c'est à dire une arête avec le moins de capacité) au plus $\O(n)$ fois.
\end{lemme}
\begin{proof}
	Pour qu'une arête $e$ soit bottleneck une nouvelle fois, il faut que la longueur du nouveau plus court chemin ait augmenté au moins de $2$.
\end{proof}


\begin{proof}[Démonstration de la Complexité d'Edmonds-Karp]
	Chaque arête peut être utilisée au plus $\O(n)$, il y a donc au plus $nm$ itérations, et les itérations se font en temps $\O(m)$, ce qui est le résultat.
\end{proof}

\section{Couplage Maximal}
\begin{definition}
	Etant donné un graphe $G = \left(V, E \right)$, $M \subseteq E$ est un couplage si et seulement si $\delta_{M}(v) \leq 1$ pour tout noeud $v$.\\
	Un chemin $P$ est dit $M$-alternant s'il alterne entre une arête de $M$ et une arêtre de $E\setminus M$. \\
	Un chemin $M$-alternant $P$ est dit $M$-augmentant s'il commence et termine par un noeud non couvert par $M$.
	\label{def:matching}
\end{definition}

\begin{thm}
	Un couplage $M$ est maximal si et seulement si il n'y a pas de chemin $M$-augmentant.
\end{thm}
\begin{proof}
	\begin{itemize}
		\item[$\Leftarrow$] S'il y a un chemin $M$-augmentant
		\item[$\Rightarrow$] Supposons qu'il existe un $M^{*}$ avec $\abs{M^{*}} > \abs{M}$ est $M$ n'a pas de chemin augmentant.
			$M^{*} \otimes M$ est un sous-graphe de $G$ avec degré dans $\{0, 1, 2\}$.
			On a plusieurs possiblités pour ce graphe ($M^{*}$ est en rouge, $M$ en bleu).
			\begin{category}
				\cdot & & \cdot\arrow[color=red, r, no head] & \cdot\arrow[color=blue, d, no head]
				& & \cdot\arrow[color=red, dr, no head] & & &\cdot\arrow[dr, color=blue, no head]
				\\
				\cdot & & \cdot\arrow[color=blue, u, no head] & \cdot\arrow[color=red, l, no head]
				& & & \cdot\arrow[dl, color=blue, no head] & & & \cdot\arrow[dl, color=red, no head]
				\\
				& & \cdot\arrow[dr, color=red, no head] & & & \cdot\arrow[dr, color=red, no head] &  & &\cdot\arrow[dr, color=blue, no head]& \\
				& & & \cdot & & & \cdot & & &\cdot\\
			\end{category}
			Toutefois, le quatrième type de graphe n'est pas possible puisque s'il y a plus de rouges que de bleus, on a un chemin augmentant pour $M$.
			Alors en suivant le
	\end{itemize}
\end{proof}

\begin{thm}
	On peut trouver un couplage maximal en $\O(mn)$.
\end{thm}

\subsection{Théorème de K\H{o}nig}
\begin{thm}[de K\H{o}nig]
	Dans un graphe biparti $G = \left(A \sqcup B, E\right)$:
	\begin{equation*}
		\max_{couplage}\abs{M} = \min_{\text{\sc Vertex Cover}}\abs{C}
	\end{equation*}
\end{thm}

Avant de prouver le théorème, une observation:
\begin{proposition}[Dualité Faible]
	Dans un graphe $G$ (non nécessairement biparti), on a:
	\begin{equation*}
		\abs{M} \leq \abs{C}
	\end{equation*}
	\label{prop:weakduality}
\end{proposition}
\begin{proof}
	Puisque les arêtes de $M$ ne couvrent qu'au plus une fois chaque sommet, en particulier:
	\begin{equation*}
		\sum_{v \in C} 1 \geq \sum_{(u, v) \in M} \left(1 + 1\right) \geq \sum_{e \in M} 1
	\end{equation*}
\end{proof}

\begin{proof}[Démonstration du Théorème de K\H{o}nig]
	Soit $M$ un couplage maximal de $G$.
	On pose $U$ l'ensemble des sommets non atteints par $M$ dans $A$ et $Z$ l'ensemble des sommets connecté par un chemin $M$ alternant aux sommets de $U$.
	On pose $S = Z \cap X$ et $T = Z \cap Y$.
	Alors, chaque sommet de $T$ est atteint par $M$ et $T$ est l'ensemble des voisins de $S$.
	Posons $K  = \left( X \setminus S \right)\cup T$. Chaque arête de $G$ a une de ses extrémités dans $K$.
	Donc $K$ est une couverture des sommets de $G$ et $\abs{M} = \abs{K}$ d'où le résultat.
\end{proof}

\subsection{Dualité et Programmation Linéaire}
\begin{definition}
	La programmation linéaire s'intéresse à la maximisiation de $\transpose{w}X$ en vérifiant $X\leq 0$ et $AX\leq b$ (problème de packing) ou à la minimisation de $b^{T}Y$ en vérifiant $\transpose{A}y \geq w$ avec $y \geq 0$ (problème de covering).
	\label{def:linearprog}
\end{definition}

Quelques exemples:
\begin{exemple}[Couplage Maximal comme Programmation Linéaire]
	On cherche à maximiser $\sum_{e} X_{e}$ en ayant $\forall v \in A \cup B \sum_{e \in \delta(v)} X_{e} \leq 1$ et $\forall e \in E, X_{e} \geq 0$.
	On a donc ici $A$ la matrice $(A_{v\in V, e\in E})$ d'incidence du graphe et $b$ et $w$ le vecteur Attila.
	On essaye de mettre autant d'arêtes que possible (packing).

	Ce problème peut donc aussi s'écrire comme la minimisation de $\sum_{v \in A\cup B} Y_{v}$ en ayant $Y_{u} + Y_{v} \geq 1 \forall e = (u, v) \in E$, $Y_{u} \geq 0 \forall u \in A\cup B$.
	On essaye de mettre aussi peu d'arêtes que possible (covering).
\end{exemple}

\begin{proposition}
	La dualité faible de la programmation linéaire est: $\forall x, y$ qui vérifie les contraintes, on a:
	\begin{equation*}
		\transpose{w}X \leq \transpose{b}Y
	\end{equation*}
	\label{prop:dualitylinearprog}
\end{proposition}
\begin{proof}
	\begin{equation*}
		\transpose{w}x \leq \left(\transpose{Y}A\right)X = \transpose{Y}\left(AX\right) \leq \transpose{Y}b= \transpose{b}Y
	\end{equation*}
\end{proof}

Ceci redémontre la dualité faible pour les graphes.

\begin{thm}[Dualité Forte]
	Il existe $X^{*}, Y^{*}$ tels que $\transpose{w}X^{*} = \transpose{b}Y^{*}$.
\end{thm}
La dualité permet de faire les liens entre Max-Flow et Min-Cut ainsi qu'entre Max-Matching et Min-Vertex-Cover.

\subsection{Couplage Pondéré dans un graphe biparti}
\begin{definition}
	Soit un graphe $G = \left(A\cup B, E = A \times B\right)$ avec $\abs{A} = \abs{B}$ et une fonction de poids entière sur les arêtes, on dit que $\pi: A\cup B \to \N$ est une couverture si $\forall e \in E, \pi(a) + \pi(b) \geq w(e)$.
	\label{def:cover}
\end{definition}

\begin{thm}[Egrevary]
	On a:
	\begin{equation*}
		\max_{M \text{ couplage parfait}} w(M) = \min_{\pi \text{ couverture entière}} \sum_{v \in A\cup B} \pi(v)
	\end{equation*}
\end{thm}

\begin{proposition}[Dualité Faible]
	On a: $w(M) \leq \sum \pi(v)$.
	\label{prop:weakduality2}
\end{proposition}
\begin{proof}
	De même que précédemment en remplaçant par l'inégalité d'une couverture.
\end{proof}

\begin{proof}[Démonstration du Théorème d'Egevary]
	Soit $\pi$ une couverture minimale.
	Posons $G_{\pi} \subseteq G$ où une arête $e = (a, b)$ est dans $G_{\pi}$ si et seulement si $\pi(a) + \pi(b) = w(e)$.
	C'est le sous-graphe des arêtes qu'on ne peut pas modifier.
	Soit $M$ le couplage maximal dans $G_{\pi}$.
	Si $\abs{M} = \abs{A} = \abs{B}$ alors on a fini puisqu'on a toujours égalité dans la preuve de la dualité faible.
	Sinon, si $M$ n'est pas parfait, on définit $\pi'$ comme suit:
	\begin{equation*}
		\begin{cases}
			\forall a \in A_{1} & \pi'(a) = \pi(a) - 1\\
			\forall b \in B_{1} & \pi'(b) = \pi(b) + 1\\
			\forall v \notin A_{1} \cup B_{1} & \pi'(v) = \pi(v)
		\end{cases}
	\end{equation*}
	où $A_{1}$ est l'ensemble des sommets atteignables depuis un sommet non atteint pas $M$ passant par un chemin augmentant et $B_{1}$ est l'ensemble des sommets couplés à des sommets de $A_{1}$.
	On définit également $B_{2}$ l'ensemble des sommets de $B$ atteignables depuis un sommet non atteint par $M$ en passant par un chemin $M$-augmentant, $A_{2}$ l'ensemble des sommets couplés à des sommets de $B_{2}$ et $A_{3}, B_{3}$ le reste de $A$ et $B$.
	Puisque les arêtes qui ne sont pas dans $G_{\epsilon}$ vérifient $\pi(a) + \pi(b) \geq w\left( (a,b) \right) + 1$, on a toujours $\pi'(a) + \pi'(b) \geq w(e)$.
	De plus, on a bien $\sum \pi' < \sum \pi$ et $\pi'$ couvre toutes les arêtes, mais $\pi'$ peut être négative.
	Si $\pi'(a_{0}) = -1$, alors on définit $\pi''$ comme:
	\begin{equation*}
		\begin{cases}
			\pi''(a) = \pi'(a) + 1 & \forall a \in A\\
			\pi''(b) = \pi'(b) - 1 & \forall b \in B
		\end{cases}
	\end{equation*}
	Ceci garantit que tous les $\pi''(a)$ sont non négatifs et que $\sum\pi'' = \sum\pi'$.
	Par ailleurs les $\pi''(b)$ sont aussi non négatifs. En effet, si on arrive à celà, on a une arête d'un poids $0 \leq w(e) \leq -1 + 0$.
	Donc $\pi''$ est une couverture plus petite que $\pi$.
\end{proof}

Ceci nous donne par ailleurs un algorithme pour calculer une couverture minimale/un couplage maximal.
Toutefois, celui-ci effectue au plus $\abs{A}\max_{e} w(e) = \frac{nw}{2}$ itérations et est donc pseudo polynomial.
Par ailleurs, cet algorithme ne termine pas nécessairement sur les réels.
\medskip

La version ci-dessous, appelée méthode hongroise, termine quant à elle sur les réels et est plus efficace:
\begin{algorithm}
	\caption{Méthode Hongroise (Kuhn 1957)}
	\begin{algorithmic}
		\State $M \gets \emptyset$, $\displaystyle\pi(a) \gets \max_{e \in \delta(a)} w(e)$ et $\pi(b) \gets 0$
		\While {$M$ n'est pas parfait}
			\State {Construire $G_{\pi}$.}
			\State {Trouver le couplage maximal $M$ dans $G_{\pi}$ (par augmentation) et définir $A_{1}, A_{2}, A_{3}, B_{1}, B_{2}, B_{3}$}
			\State{Poser $\displaystyle\Delta \gets \min_{a\in A_{1}, b\in B_{2} \cup B_{3}} \pi(a) + \pi(b) - w((a, b))$}
			\State{$\pi(a) \gets \pi(a) - \Delta$ et $\pi(b) \gets \pi(b) + \Delta$}
			\State{$\delta \gets \min_{a \in A}\pi(a)$.}
			\If {$\delta < 0$}
				\State {$\pi(a) \gets \pi(a) - \delta$ et $\pi(b) \gets \pi(b) + \delta$}
			\EndIf
		 \EndWhile
	\end{algorithmic}
\end{algorithm}

\begin{thm}
	Cette méthode termine et est demande un temps $\O(n^{2}nn)$.
\end{thm}
\begin{proof}
	\begin{itemize}
		\item La boucle prend un temps $\O(m)$ pour s'effectuer.
		\item On peut l'effectuer au plus $n$ fois (lorsqu'$\abs{A_{1}}$ augmente) et $n$ fois lorsque $\abs{M}$ augmente.
			Puisque ces deux effets peuvent se combiner, on a bien le résultat.
	\end{itemize}
\end{proof}

\subsection{Relaxation et Optimalité}
\begin{proposition}[Relaxation]
	Étant donné un problème de programmation linéaire dual, $X^{*}$ et $Y^{*}$ sont tous les deux optimaux si et seulement si:
	\begin{align*}
		X_{i}^{*} \Longrightarrow \left(\transpose{A}Y^{*}\right)_{i} = W_{i}\\
		Y_{j}^{*} \Longrightarrow \left(AX^{*}\right)_{j} = b_{j}
	\end{align*}
	\label{prop:relaxation}
\end{proposition}
\begin{proof}
	On a:
	\begin{equation*}
		\transpose{W}X \leq \transpose{Y}AX \leq \transpose{Y}b
	\end{equation*}
	Il y a égalité si et seulement si les contraintes sont serrées.
	D'où le résultat.
\end{proof}

On peut appliquer ceci aux problèmes de couplage maximal:
Si une arête est utilisée ($X_{e} > 0$), alors $Y_{u} + Y_{v} = 1$ et donc elle est la seule à atteindre l'une de ses extrémités.

On peut appliquer de même cela à Max-Flow Min-Cut


\subsection{Forêts Hongroises et Floraisons}
\begin{definition}[Forêt Hongroise]
	Dans un graphe biparti, on construit une forêt dite hongroise en partant des sommets non couplésde $B$, en considérant ses voisins.
	Si ses voisins ne sont pas dans le couplage, on a un chemin augmentant.
	Sinon on ajoute les deux arêtes à la forêt et le nouveau sommet de $B$ à la liste des sommets à considérer et on itère en évitant les sommets déjà atteints (puisque ça ne sert à rien).
	On appelle l'étape d'agrandissement de la forêt le \emph{probing}.
	\label{def:hungarianforest}
\end{definition}

On regarde toutes les arêtes au plus une fois, ce qui donne une complexité $\O(\abs{E})$.

\begin{proposition}
	Si on ne trouve pas de chemin augmentant, on a un couplage maximal.
	\label{prop:maxcouple}
\end{proposition}

On va s'intéresser aux floraisons (cycles de longueur impaire) qui peuvent apparaître en créant l'équivalent d'une forêt hongroise dans un graphe non biparti.
Ceci nous mène à l'algorithme suivant.
\begin{algorithm}
	\caption{Edmonds Blossom Algorithm}
	\begin{description}
		\item[Initialization] (étant donné un couplage initial $M$)\\
			$A = \emptyset$\\
			$B = \left\{v \in V\mid \not\exists u, (u, v)\in M \right\}$ (ce seront les racines de la forêt hongroise)\\
		\item[Boucle] Tant qu'il existe $u \in B$ tel que $(u, v)$ n'a pas été probée:
		\begin{enumerate}
			\item Ou bien $v \in A$ et on ne fait rien
			\item Ou bien $v \notin A \cup B$, auquel cas $v$ est couplé à $w$ et on ajoute $(u, v)$ et $(v, w)$ à la forêt, en posant $A = A \cup \{v\}$ et $B = B \cup \{w\}$.
			\item Ou bien $v \in B$ et:
				\begin{enumerate}
					\item ou bien $u$ et $v$ sont dans des arbres différents: on augmente (en utilisant le chemin de la racine de $u$ à la racine de $v$) et on recommence l'algorithme en rouvrant toutes les floraisons
					\item ou bien on contracte la floraison constituées des chemins de $u$ et $v$ à leur plus petit ancêtre commun (LCA) et $(u, v)$.
						On traite ensuite cette floraison comme un sommet dans $B$.
				\end{enumerate}
		\end{enumerate}
	\end{description}
\end{algorithm}

\begin{thm}
	L'algorithme d'Edmonds trouve le couplage maximal.
	\label{thm:edmondsblossom}
\end{thm}

\begin{proposition}
	Le graphe après contraction a un chemin augmentant si et seulement si le graphe pré contraction a un chemin augmentant.
	\label{prop:blossomcontraction}
\end{proposition}
\begin{proof}
	On note $G'$ le graphe formé après la contraction d'une floraison dans $G$.
	\begin{itemize}
		\item Considérons un chemin augmentant dans $G'$ qui passe par la contraction. On a deux cas:
			\begin{enumerate}
				\item Si la floraison est une des extrémités du chemin, on sait que dans la floraison (qui a $2k + 1$ sommets) il n'y aura qu'un noeud d'attache.
					Ce sommet n'est pas dans $M$ car il s'agit d'une arête finale d'un chemin augmentant dans $G'$.
					Ainsi, si le sommet est la base de la floraison, le chemin s'arrête et on renvoit le chemin original.
					Sinon, on peut étendre notre chemin selon des arêtes alternants jusqu'à avoir atteint le sommet de base.
				\item Sinon, on définit $v_{L}$ et $v_{R}$ les sommets adjacents à la floraison.
					On peut toujours prendre un chemin de $v_{L}$ à $v_{R}$ de longueur paire.
					Ainsi, on trouve un chemin augmentant dans $G$, puisque seulement l'une des deux $v_{L}$ et $v_{R}$ est couplée à la floraison et donc à la base de la floraison.
			\end{enumerate}
		\item Si on a chemin augmentant dans $G$ qui passe par la floraison, on enlève un nombre pair d'arêtes du chemin donc le chemin dans $G'$ reste un chemin augmentant.
	\end{itemize}
\end{proof}



On définit quelques invariants pour l'analyse de l'algorithme:
\begin{itemize}
	\item Tous les sommets de $A$ sont reliés à des sommets de $B$
	\item Une floraison est toujours dans $B$
	\item Si $G'$ est le graphe dérivé d'une suite de contraction de floraisons, alors $G'$ a un chemin augmentant si et seulement si $G$ a un chemin augmentant.
		Ceci découle trivialement de \ref{prop:blossomcontraction}.
	\item Si on trouve un chemin augmentant dans $G'$ alors on peut rouvrir toutes les floraisons pour augmenter la taille de $M$ dans $G$.
	\item Chaque noeud dans $B$ (resp. $A$) peut revenir à sa racine par un chemin alternant de longueur paire (resp. impaire).
\end{itemize}

Soit $U$ l'ensemble des sommets qui ne sont pas dans la forêt hongroise.

\begin{proof}[Démonstration de \ref{thm:edmondsblossom}]
	\begin{description}
		\item[Preuve 1] On montre qu'à la fin de cet algorithme, dans $G'$ (possiblement après beaucoup de contractions), il n'y a pas de chemin augmentant.\\
				À la fin de l'algorithme d'Edmonds, il n'y a pas d'arêtes dans $B \times \left(B \cup U \right)$ (il peut y avoir des arêtes dans $A \times \left( A\cup U \right)$).
				Sinon, on aurait un moyen d'ajouter une arête à la forêt hongroise.
				Il n'y a donc pas de chemin augmentant dans $G'$.
		 \item[Preuve 2] Par le théorème de Tutte-Berge.
			 \begin{thm}[Tutte-Berge Minimax]
				 \begin{equation*}
				 	\max_{M \ couplage} \abs{M} = \min_{A \subseteq V}\frac{\abs{V} - \left(oc\left(V \setminus A \right) - \abs{A} \right)}{2}
				\end{equation*}
				où $oc(V \setminus A)$ est le nombre de composantes connexes de taille impaire de $V \setminus A$
			 \end{thm}
			 L'algorithme d'Edmonds trouve un certain $A$ tel que:
			 \begin{equation*}
				 \abs{U} = oc\left(V\setminus A \right) - \abs{A}
			 \end{equation*}
			 En effet, tout élément de $A$ est matché à un élément de $B$ non racine.
			 Les éléments non matchés étant les racines d'arbres de la forêt, ils sont dans $B$.
			 En enlevant les éléments de $A$, on crée une composante connexe de taille impaire par élément de $B$.
			 Comme par ailleurs, $\abs{B} - \abs{A}$ est exactement l'ensemble des sommets non couplés, on a le résultat.
			 En a donc bien le résultat.
	\end{description}
\end{proof}

\begin{thm}
	Cet algorithme demande un temps en $\O(mn^{2})$.
	\label{prop:edmondsblossomtimecomplexity}
\end{thm}

\subsection{Streaming Algorithms}
\begin{definition}
	Un algorithme de streaming est un algorithme qui ne peut pas stocker en mémoire toute son entrée en même temps.
	\label{def:streamingalgorithm}
\end{definition}

On se limite ici à un algorithme qui ne peut stocker que $\O(n\mathrm{polylog}(n))$ arêtes pour trouver un couplage maximal.

\begin{thm}
	Il y a un algorithme de streaming qui nécessite $\O\left(\frac{n}{\varepsilon}\log W \right)$ d'espace et renvoie une $\left(\frac{1}{2}-\varepsilon \right)$-approximation.
\end{thm}
\begin{proof}
\begin{algorithm}
	\caption{Algorithme de Streaming pour les Couplages Maximaux}
	\begin{description}
		\item[Initialisation] On pose $\pi(u) = 0$ pour tout $u \in V$
			On crée une pile $S$ vide.
		\item[Phase de Streaming]
			Tant qu'une nouvelle arête $e =(a, b)$ arrive:
			Si $w(e) > (1 + \epsilon)(\pi(a) + \pi(b))$:
			\begin{itemize}
				\item $\Delta = w(e) - (1 + \varepsilon)(\pi(a) + \pi(b))$
				\item $\pi(a) += \Delta$
				\item $\pi(b) += \Delta$
			\end{itemize}
		\item[Phase Finale] On effectue un algorithme glouton inverse en enlevant les arêtes de $S$ une par une et en les ajoutant dans $M$ si possible.
	\end{description}
	\label{alg:streamingmatching}
\end{algorithm}

\begin{lemme}
	On a:
	\begin{equation*}
		\left(\sum_{v \in V} \pi(v) \right)\left(1 + \varepsilon \right) \geq \max_{M \ couplage} w(M)
	\end{equation*}
	\label{lemma:streammatching1}
\end{lemme}
\begin{proof}
	On a le problème de programmation linéaire suivant:
	\begin{align*}
		\max \sum_{e \in E} w(e)\chi_{e}\\
		\forall v \in V \sum_{e \in \delta(v)} \chi_{e} \geq 1\\
		\forall e \in E \chi_{e} \geq 0
	\end{align*}
	dont le dual est:
	\begin{align*}
		\min \sum_{v \in V}\pi(v)\\
		\pi(a) + \pi(b) \geq w(e) \forall e=(a, b) \in E\\
		\pi(v) \geq 0 \forall v \in V
	\end{align*}
	On a le résultat par dualité faible en vérifiant que $(1 + \epsilon)\pi$ convient.
\end{proof}

\begin{lemme}
	Le couplage résultat $M$ vérifie l'inégalité:
	\begin{equation*}
		w(M) \geq \frac{1}{2}\left(\sum_{v\in V}\pi(v) \right)
	\end{equation*}
\end{lemme}
\begin{proof}
	L'algorithme va créer un pile de poids pour chaque noeuds (les $\Delta$ consécutifs des arêtes mettant en jeu le sommet).
	En particulier
\end{proof}

On a alors:
\begin{align*}
	w(M) \leq \frac{1}{2(1+\varepsilon)}opt\\
	\geq \left(\frac{1}{2} - 3\varepsilon \right)opt
\end{align*}

\medskip

Pour ce qui est l'espace, il y a toujours au plus $\log_{1 + \epsilon}W$ $\Delta$ dans la fonction de poids, et on a $n$ sommets.
En effet, à chaque fois qu'on ajoute un $\Delta$, la valeur augmente de $1 + \epsilon$ au moins.
On a le résultat en prenant un développement limité.
\end{proof}

\end{document}
