\documentclass{cours}

\newtheorem{openpb}{Open Problem}

\title{Algorithmique}
\author{Pierre Aboulker, Paul Jeanmaire, Tatiana Starikovskaya}
\date{\today}

\begin{document}
\part{Lecture 1 - 28/09}
\section{Organisation}
Mail Tatiana : \url{starikovskaya@di.ens.fr}
Homeworks are 30\% of the final grade, final (theory from lecture)
Textbooks : 
\begin{itemize}
    \item \textsl{Introduction to Algorithms} - Cormen, Leiserson, Riverst, Stein
    \item \textsl{Algorithms on strings, trees, and sequences} - Gusfield
    \item \textsl{Approximation Algorithms} - Vazirani
    \item \textsl{Parametrized Algorithms} - Cygan, Fomin, Kowalik, Lokshtanov, Marx, Pilipczuk, Pilipczuk, Saurabh
\end{itemize}

\section{Introduction}
Algorithm take Inputs and give an output.
\begin{openpb}[Mersenne Prime]
    Find a new prime of form $2^{n} - 1$
\end{openpb}
Algorithms do not depend on the language. Algorithms should be simple, fast to write and efficient.
Word RAM model : Two Parts : one with a constant number of registers of $w$ bits with direct access, and one with any number of registers, only with indirect access (pointers). 
Allows for elementary operations: basic arithmetic and bitwise operations on registers, conditionals, goto, copying registers, halt and malloc. 
To index the memory storing input of size $n$ with $n$ words, we need register length to verify $w \geq \log n$
Algorithms can always be rewritten using only elementary operations. 
Complexity : 
\begin{itemize}
    \item $Space(n)$ is the maximum number of memory words used for input of size $n$
    \item $Time(n)$ is the maximum number of \textsl{elementary} operations used for input of size $n$
\end{itemize}
Complexity Notations : 
\begin{itemize}
    \item $f \in \mathcal{O}(g)$ if $\exists n_0 \in \mathbb{N}, c \in \mathbb{R}_{+},\ f(n) \leq c \cdot g(n), \ \forall n \geq n_0$
    \item $f \in \Omega(g)$ if $\exists n_0 \in \mathbb{N}, c \in \mathbb{R}_{+},\ f(n) \geq c \cdot g(n), \ \forall n \geq n_0$
    \item $f \in \Theta(g)$ if $\exists n_0 \in \mathbb{N}, c_{1}, c_{2} \in \mathbb{R}_{+},\ c_{1} \cdot g(n) \leq f(n) \leq c_{2} \cdot g(n), \ \forall n \geq n_0$
\end{itemize}

\section{Data Structures}
\subsection{Introduction}
Way to store elements of a data base that is created to answer frequently asked queries using pre-processing.
We care about space used, construction, query and update time.
Can be viewed as an algorithm, which analysed on basics.
Containers are basic Data Structures, maintaining the following operations : 
\begin{enumerate}
    \item Random Access : given $i$, access $e_{i}$
    \item Access first/last element
    \item Insert an element anywher
    \item Delete any element
\end{enumerate} 

\subsection{Array}
An array is a pre-allocated contiguous memory area of a \emph{fixed} size. It has random access in $\mathcal{O}(1)$, but doesn't allow insertion nor deletion.

Linear Search : given an integer $x$ return $1$ if $e_{i} = x$ else 0.
\begin{algorithm}
    \caption{Linear Search in an Array. \\ Complexity : Time = $\mathcal{O}(n)$ | Space = $\mathcal{O}(n)$}
    \begin{algorithmic}
        \Input
        $x$
        \EndInput
    \end{algorithmic}
\end{algorithm}

\subsection{Doubly Linked List}
Memory area that does not have to be contiguous and consists of registers containing a value and two pointers to the previous and next elements.
It has random access in $\mathcal{O}(n)$, access/insertion/deletion at head/tail in $\mathcal{O}(1)$.
\begin{algorithm}
    \caption{Insertion in a Doubly Linked List \\ Complexity : $\mathcal{O}(1)$}
    \begin{algorithmic}
        \Input
        $ L, x$
        \EndInput
        \State $x.next \gets L.head$ 
        \If{$L.head \neq NIL$}
            \State $L.head.prev \gets x$
        \EndIf
        \State $L.head \gets x$
        \State $x.prev = Nil$
    \end{algorithmic}
\end{algorithm}


\subsection{Stack and Queue}
Stack : Last-In-First-Out data structure, abstract data structure. Access/insertion/deletion to top in $\mathcal{O}(1)$.

\begin{openpb}[Optimum Stack Generation]
    Given a finite alphabet $\Sigma$ and $X \in \Sigma^{n}$. Find a shortest sequence of stack operations push, pop, emit that prints out $X$. You must start and finish with an empty stack.
    Current best solution is in $\tilde{\mathcal{O}}(n^{2.8603})$.
\end{openpb}

Queue : First-In-First-Out abstract data structure. Access to front, back in $\mathcal{O}(1)$, deletion and insertion at front and back in $\mathcal{O}(1)$.

\section{Approaches to algorithm design}
Solve smalle sub-problems to solve a large one.

\subsection{Dynamic Programming}
Break the problem into many closely related sub-problems, memorize the result of the sub-problems to avoid repeated computation

Examples : 
\begin{algorithm}
    \caption{Recursive Fibonacci Numbers \\ Complexity: Exponential}
    \begin{algorithmic}
        \State RFibo($n$) :
        \Input
        $ n$
        \EndInput
        \If {$n \leq 1$}

            \Return $n$
        \EndIf\\
        \Return RFibo($n-1$) + RFibo($n-2$)
    \end{algorithmic}        
\end{algorithm}

\begin{algorithm}
    \caption{Dynamic Programming Fibonacci Numbers \\ Time = $\mathcal{O}(n)$ | Space = $\mathcal{O}(n)$}
    \begin{algorithmic}
        \Input
        $ n$
        \EndInput
        \State $Tab \gets zeros(n)$ \Comment $zeros(n)$ returns a $n$-array of zeros.
        \State $Tab[0] \gets 0$
        \State $Tab[1] \gets 1$
        \For{$i \gets 2$ to $n$} 
            \State $Tab[i] = Tab[i-1] + Tab[i-2]$
        \EndFor\\
        \Return Tab[n]
    \end{algorithmic}        
\end{algorithm}

Levenshtein Distance between two strings can be computed in $\mathcal{O}(mn)$ instead of exponential time. Based on \url{https://arxiv.org/pdf/1412.0348.pdf}, this is the best one can do. 
RNA folding : retrieving the 3D shape of RNA based on their representation as strings. Currently, we know it is possible to find $\mathcal{O}(n^3)$, in $\tilde{\mathcal{O}}(n^{2.8606})$ and if \emph{SETH} is true, there is no $\mathcal{O}(n^{\omega - \epsilon})$. We know $\omega \in \left[2, 2.3703\right)$

\begin{openpb}
    Is there a better Complexity for RNA folding ? What is the true value of $\omega$ ?
\end{openpb}

Knapsack problem : An optimization problem with bruteforce complexity $\mathcal{O}(2^{n})$.
\begin{algorithm}
    \caption{Knapsack : Dynamic Programming \\
    Time = $\mathcal{O}(nW)$ | Space = $\mathcal{O}(nW)$}
    \begin{algorithmic}
        \Input $\ W, w, v$ \Comment Capacity, weight and values vectors.
        \EndInput

        \State $KP = zeros(n, W)$

        \For {$i \gets 0$ to $n$}  
            \State $KP[i, 0] = 0$
        \EndFor

        \For {$w \gets 0$ to $W$}  
            \State $KP[0, w] = 0$
        \EndFor
    
        \For {$i \gets 0$ to $n$}  
            \For {$w \gets 0$ to $W$}  
                \If {$w < w_{i}$}
                    \State $KP[i, w] \gets KP[i-1, w]$
                \Else
                    \State $KP[i, w] = \max\begin{cases}
                        &KP[i-1, w] \\
                        &KP[i-1, w - w_{i}] + v_{i}
                    \end{cases}$.
                \EndIf
            \EndFor
        \EndFor\\
        \Return $KP[n, W]$
    \end{algorithmic}
\end{algorithm}

\subsection{Greedy Techniques}
Problems solvable with the greedy technique form a subset of those solvable with DP. Problems must have the optimal substrcture property.
Principle : choosing the best at the moment.


Example : The Fractional Knapsack Problem\\
Algorithm : Iteratively select the greatest value-per-weight ratio. \\
\begin{theorem}
    This algorithm returns the best solution, in time $\mathcal{O}(n \log n)$
\end{theorem}
\begin{proof}[By contradiction]
    Suppose we have $\frac{v_{1}}{w_{1}} \geq \ldots \geq \frac{v_{n}}{w_{n}}$. Let $ALG = p = (p_{1}, \ldots, p_{n})$ be the output by the algorithm and $OPT = q = (q_{1}, \ldots, q_{n})$ be optimal.\\
    Assume that $OPT \neq ALG$, let $i$ be the smallesst index such $p_{i} \neq q_{i}$. There is $p_{i} > q_{i}$ by construct. \\
    Thus, there exists $j > i$ such that $p_{j} < q_{j}$. We set $q' = (q_{1}^{'}, \ldots, q_{n}^{'}) = (q_{1}, \ldots, q_{i-1}, q_{i} + \epsilon, q_{i+1}, \ldots, q_{j} - \epsilon\frac{w_{i}}{w_{j}}, \ldots, q_{n})$\\
    $q^{'}$ is a feasible solution : $\sum\limits_{i = 1}^{n} q_{i}^{'} \cdot w_{i} = \sum\limits_{i = 1}^{n} q_{i} w_{i} \leq W$\\
    Yet, $\sum\limits_{i = 1}^{n} q_{i}^{'} \cdot v_{i} > \sum\limits_{i = 1}^{n} q_{i} \cdot v_{i}$, ce qui contredit la 
\end{proof}

\part{TD 1 - 29/09}
\section{Mathematical Complexity}
\subsection{Exercice 1}
\subsubsection{Question 1}
Non : prendre $f = 1$ et $g = exp$.
\subsection{Question 2}
Non, si g = h = f.
\subsubsection{Question 3}
Non : Si on a $f = n, g = n^{2} \in \Omega(f(n)), h = f \in \Theta(f(n))$ alors $g + h \neq \O(f(n))$

\subsection{Exercice 2}
On rappelle la formule de Stirling : \[n! \sim \left(\frac{n^{n}}{e^{n}}\right)\sqrt{2\pi n}\]
Immédiatement, on en déduit la première relation.

On a par ailleurs la seconde égalité en passant au logarithme, fonction continue en $+\infty$

\subsection{Exercice 3}
On rappelle les formules suivantes: 
\[\begin{cases}    
    (n+a)^{b} &= n^{b}(1 + \frac{a}{n})^{b}\\
    (1+\frac{a}{n})^{b} &= 1 + b\frac{a}{n} + o(\frac{a}{n}) \in \left[1; 1+ba\right]
\end{cases}\]
Immédiatement, on a la relation souhaité.

\section{Data Structures}
\subsection{Exercice 4}
Il suffit de diviser l'array en deux sous arrays de taille $n/2$, une array commençant en $i = 0$, une commençant en $j = -1$ et on stocke les deux indices de fin de la pile courant.

\subsection{Exercice 5}
\subsubsection{Question 1}
On définit un algorithme de $reverse$ de list en temps linéaire en ajoutant tous les éléments dans une pile puis en dépilant dans une liste. On effectue bien $2n = \O(n)$ opérations.\\
Il suffit alors de comparer les deux listes en temps linéaire.

\subsubsection{Question 2}
Pour une liste vide, ou d'un seul élément on renvoie True. 
On reverse en place la première moitié de la liste et on la compare à la seconde et normalement ça marche. 

\subsection{Exercice 6}
On utilise deux piles : On push dans la première, et on pop de la seconde. Lorsque la seconde pile est vide, on pop de la première et on push dans la seconde, ce qui permet bien de former une pile. 

\subsection{Exercice 7}
\subsubsection{Question 1}
On utilise les arrays standards et lorsqu'on dépasse la capacité, on double le nombre de cases, qu'on initialise à -1, en stockant l'indice du dernier élément de la liste. On a alors toujours une complexité en espace en $\O(n)$ puisqu'on a toujours au plus $2n$ cases. \\

\subsubsection{Question 2}
On effectue la suite suivante d'instructions, pour $n \in \N$ : \begin{enumerate}
    \item On ajoute $2n$ éléments
    \item On retire $n+1$ éléments
    \item On ajoute $1$ élément
    \item On recommence en modifiant $n$
\end{enumerate}

\subsubsection{Question 3}
Il suffit alors d'attendre de passer en dessous de la barre de 25\% du tableau rempli. On a bien tout de même une complexité en $\O(n)$.

 \section{Greedy Algorithms}
\subsection{Exercice 8}
\subsubsection{Question 1}
\begin{algorithm}
   \caption{Greedy Algorithm for Scheduling Problem} 
   \begin{algorithmic}
        \Input $\ a$ \Comment Vecteur de tuples correspondant aux activités 
        \EndInput
        \State $E \gets ListeVide()$
        \State $Sort(a, (fun : x, y \mapsto x[1] \leq y[1]))$ \Comment On trie les activités par date de fin croissante
        \State $s \gets PileVide()$
        \State $Push(a, s)$  \Comment On ajoute une à une les activités de $a$ dans une pile.
        \While ($s$)
            \State $ac \gets Pop(s)$
            \If ($ac$ est compatible)
                \State $Ajouter(E, ac)$
            \EndIf
        \EndWhile\\
        \Return $E$
   \end{algorithmic}
\end{algorithm}
\begin{proof}[Correction]
    On introduit une solution optimale, la plus proche possible de l'algo.     
\end{proof}

\subsubsection{Question 2}
On prend $T1 = \left[1, 2\right], T2 = \left[3, 4\right], T3 = \left[1.5, 2.5\right]$

\subsubsection{Question 3}
Bon on fait de la Programmation Dynamique. Relation de récurrence $\forall i DP(i)$ est le max des poids sur $\left\{T_{1}, \ldots, T_{i}\right\}$
\[
\begin{cases}
    DP(0) &= 0\\
    DP(i+1) &= \max(DP(i), w_{i+1} + DP(p(i+1))) \text{ où } p(i) \text{ est le dernier indice de la dernière tâche compatible} 
\end{cases}    
\]

\part{Lecture 2 - 5/10}

\section{Divide and Conquer}
Divide a problem into smaller ones to solve those, then combine the solutions to get a solution of the bigger problem.\\
Example : \textsl{Merge Sort} : Its complexity verifies $T(n) = T(\lceil n/2\rceil) + T(\lfloor n/2 \rfloor) + \O(n)$. From that we will derive that $T(n) = \O(n\log n)$


\section{Analysis of Recursive Algorithms}
We have recurrences we want to solve. We have three methods : 
\localtableofcontents

\subsection{Substitution Method}
The method :
\begin{enumerate}
    \item Guess the asymptotic of $T(n)$
    \item Show the answer via induction
\end{enumerate}
For \textsl{Merge Sort}: we guess $T(n) \leq c \cdot n\log_{2}n, \forall n \geq 2$. We choose $c$ that verifies that property until $n = 6$. \\
Substituting in the recurrence equation : \[T(n) \leq cn\log_{2}\frac{n}{2} + c\log_{2}\frac{n}{2} + c\frac{n+2}{2} + a\cdot n = c \dot n\log_{2}n + a \cdot n + c \cdot \log_{2}n - c\frac{n}{2}\]
If we then choose $c$ so that it is bigger than $20a$ we get : 
\[T(n) \leq cn\log_{2}n + a\cdot n - c\cdot n/20 \leq cn\log_{2}n\]

\subsection{Recursion-tree Method}
\begin{enumerate}
    \item Simplify the equation : 
    \begin{itemize}
        \item Delete floors and ceils
        \item Suppose $n$ is of a good form
    \end{itemize}
    \item Draw a tree, rooted with the added term and the recursive calls
    \item Start again, and recursively fill the tree
\end{enumerate}
We get a tree of depth $\log_{k}n$ if $n$ is divided by $k$ in recursive calls. We can now sum the values of the nodes, to get an approximation, and start verifying.

\section{Master Theorem}
\subsection{The Theorem}
\begin{theorem}[Master Theorem]\label{thm:MT}
    If we have recurrence equation $T(n) = aT(n/b) + f(n)$ where $a \geq 1, b > 1$ are integers, $f(n)$ is asymptotically positive. Let $r = \log_{b}a$, we have : 
    \begin{enumerate}
        \item If $f(n) = \O(n^{r-\epsilon})$ for some $\epsilon > 0$, then $T(n) = \Theta(n^r)$
        \item If $f(n) = \Theta(n^{r})$ then $T(n) = \Theta(n^{r}\log n)$
        \item If $f(n) = \Omega(n^{r + \epsilon})$ for some $\epsilon > 0$, and $af(n/b) \leq cf(n)$ for some constant $c < 1$ and all sufficiently large $n$ (regularity condition) then $T(n) = \Theta(f(n))$.
    \end{enumerate} 
\end{theorem}
\begin{remark}
    The Master Theorem \ref{thm:MT} does not cover all possible cases for $f(n)$. Example : $f(h) = h^{r}/\log h$
\end{remark}
\begin{remark}
    The Master Theorem \ref{thm:MT} is sometimes called \textsc{Théorème sur les Récurrences de Partition}
\end{remark}

\section{The Proof}
    Plan : \begin{itemize}
        \item Analyse the recurrence as if $T$ is defined over reals (continuous version)
        \item Prove the discrete version
    \end{itemize}

\subsection{Continuous Master Theorem}
\begin{proof}
    \begin{lemma}
        Define $T(n) = \begin{cases}
            \Theta(1) &\text{if } n \leq \hat{n}\\
            aT(n/b) + f(n) &\text{if } n > \hat{n}
        \end{cases}$
        Then \[T(n) = \Theta\left(n^{r}\right) + \sum_{k=0}^{\lceil \log_{b}(n/\hat{n})\rceil - 1}a^{k}f(n/b^{k})\]
    \end{lemma}
    \begin{proof}
        In the Recursion-Tree, stopped when the argument of $T$ is smaller than $\hat{n}$ which is when depth is $\lceil \log_{b}(n/\hat{n})\rceil - 1$, we get : 
        \[
            \begin{aligned}
            T(n) &\leq \sum_{k=0}^{\lceil \log_{b}(n/\hat{n})\rceil - 1}a^{k}f(n/b^{k}) + \Theta(a^{log_{b}(n/\hat{n})})\\ &= \sum_{k=0}^{\lceil \log_{b}(n/\hat{n})\rceil - 1}a^{k}f(n/b^{k}) + \Theta(a^{\log_{b}(n)})\\ &= \sum_{k=0}^{\lceil \log_{b}(n/\hat{n})\rceil - 1}a^{k}f(n/b^{k}) + \Theta(n^{\log_{b}(a)})
            \end{aligned}
        \]
    \end{proof}
    Back to the proof : 
    \begin{lemma}
        Define $g(n) = \Theta(n^{r}) + \sum_{k= 0}^{q} a^{k}f(n/b^{k})$ Then : 
        \begin{enumerate}
            \item If $f(n) = \O(n^{r - \epsilon})$ then $g(n) = \Theta(n^{r})$
            \item If $f(n) = \Theta(n^{r})$ then $g(n) = \Theta(n^{r}\log{n})$
            \item If $f(n) = \Omega(n^{r + \epsilon})$ and we have the regularity condition then $g = \Theta(f)$
        \end{enumerate}
    \end{lemma}
    \begin{proof}
        \begin{enumerate}
            \item Case 1 : \[
            g(n)  
            \begin{aligned}
                &= \Theta(n^{r}) + \sum_{k= 0}^{q} a^{k}f(n/b^{k}) \\ 
                &= \Theta(n^{r}) + \O\left(\sum_{k= 0}^{q} a^{k}(n/b^{k})^{r - \epsilon}\right)\\
            \end{aligned}
            \]
            However : 
            \[
                \begin{aligned}
                    \sum_{k= 0}^{q} a^{k}(n/b^{k})^{r - \epsilon} &= n^{r-\epsilon}\sum_{k = 0}^{q} (ab^{\epsilon}/b^{r})^{k}\\
                    &= n^{r-\epsilon}\sum_{k=0}^{\lceil \log_{b}(n/\hat{n})\rceil - 1}(b^{\epsilon})^{k} = \O(n^{r - \epsilon}(n/\hat{n})^{epsilon})
                \end{aligned}    
            \]
            Thus : $g(n) = \Theta(n^{r})$
            % --------------------------------------------------------------------------
            \item Case 2 : We have : 
            \[
            \begin{aligned}
                g(n) &= \Theta(n^{r}) + \sum_{k=0}^{q} a^{k}f(n/b^{k})\\
                &= \Theta(n^{r}) + \Theta\left(\sum_{k=0}^{q}a^{k}(n/b^{k})^{r}\right)\\
            \end{aligned}    
            \]
            However : 
            \[
              \begin{aligned}
                \sum_{k=0}^{q}a^{k}(n/b^{k})^{r} &= n^{r}\sum_{k = 0}^{q} (a/b^{r})^{k}\\
                &= n^{r}\sum_{k = 0}^{\lceil \log_{b}(n/\hat{n})\rceil - 1}1 = n^{r}\Theta(\log_{b}n/\hat{n})
              \end{aligned}  
            \]
            % ----------------------------------------------------------------------------
            \item Case 3 : By induction on $k$ : $a^{k}f(n/b^{k}) \leq c^{k}f(n)$. Thus : 
            \[
                \sum_{k = 0}^{q}a^{k}f(n/b^{k}) \leq \sum_{k = 0}^{q}c^{k}f(n) = f(n) \sum_{k= 0}^{q}c^{k} = \Theta(f(n))
            \]
        \end{enumerate}
    \end{proof}
    We thus have proved the continuous Master Theorem.
\end{proof}

    \subsection{Discrete Master Theorem}
    We have now showed the continuous Master Theorem, following \textsc{William Kuszmaul, Charles E. Leiserson}, \href{https://epubs.siam.org/doi/pdf/10.1137/1.9781611976496.15}{\textit{Floors and Ceilings in Divide-and-Conquer Recurrences}}, Symposium on Simplicity in Algorithms 2021.\\
        \begin{proof}
            See slides below
            \includepdf[nup = 1x2, pages = 32-45]{Lecture_2.pdf}    
        \end{proof}
        \begin{remark}[Remarks on the Proof]
            \begin{itemize}
                \item Lemmas 1 to 3 serve to show that the argument does not go too far when it is rounded up or down.
                \item Slide 36 Last Line : $\frac{2}{\beta^{i} - 1} < \frac{4}{\beta^{i}}$ for $i \geq i_{0}$. Thus : $\sum_{i = 1}^{\infty} \frac{2}{\beta^{i} - 1} < \sum_{i = 1}^{\infty}\frac{4}{\beta^{i}} + \sum_{i = 0}^{i_0}\frac{2}{\beta^{i} - 1}$ and that last sum is $\O(1)$
                \item Slide 37 Line 3 : The first inequalities comes from the Recursion-Tree, so that we can ensure the argument does not deviate to much, by the second inequalities. 
            \end{itemize}
        \end{remark}
    \subsection{Use Cases}
    Using the Master Theorem we can show the complexity of many algorithms :
    \begin{enumerate}
        \item Merge Sort Complexity : $T(n) = T(\lceil n/2 \rceil) + T(\lfloor n/2 \rfloor) + \O(n) = \Theta(n\log n)$
        \item Strassen's Algorithm for Matrix Multiplication : $T(n) = 7 T(n/2) + \Theta(n^{2}) \Rightarrow T(n) = \O(n^{\log_{2}7}) = \O(n^{2.8074})$
    \end{enumerate}

\section{Fast Multiplication of Polynomials}
The sum of two degree $n$ polynomials can be done in $\O(n)$, Horner's rule for evaluation produces $\O(n)$ complexity. The naïve product can be done in $\O(n^2)$\\
Remembering Lagrange's Theorem on Polynomials (or Vandermonde's Determinant, or anything really), degree $n$ polynomials are entirely represented by their point-value reprensentation over $n$ distinct points $(x_{i}, y_{i})$.
Then, by converting the coefficient reprensentation to point-value representation, then by point-wise multiplicating the polynomials, then by going back to the coefficient representation, we can have a better algorithm.

\subsection{Point-Value Multiplication}
It is easily done in $\O(n)$ if both polynomials are represented over the same axis.

\subsection{Coefficient to Point-Value Conversion - Fast Fourier Transform}
For $P = \sum_{i = 0}^{n-1} a_{i}x^{i}$, we define : \[
  \begin{aligned}
    &P_{odd}(x) &= a_{n-1}x^{n/2-1} + a_{n-3}x^{n/2-3} + \ldots + a_{1}x\\
    &P_{even}(x) &= a_{n-2}x^{n/2-2} + a_{n-4}x^{n/2-4} + \ldots = a_{2}x^{2/2} + a_{0}
  \end{aligned}  
\]
\begin{enumerate}
    \item We have : $P = xP_{odd}(x^{2}) + P_{even}(x^{2})$
    \item We evaluate $P_{odd}, P_{even}$ at $(\omega_{n}^{i})^{2}$ recursively by the halving property.
    \item We combine the result.
\end{enumerate}

\subsection{Point-Value to Coefficient Conversion - Inverse Fast Fourier Transform}
\begin{theorem}
    $V_{n}^{-1}[i, j] = \omega_{n}^{-ij}/n$
\end{theorem}



\end{document}
