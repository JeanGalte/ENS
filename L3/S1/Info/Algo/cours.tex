\documentclass{cours}

\newtheorem{openpb}{Open Problem}

\title{Algorithmique}
\author{Pierre Aboulker, Paul Jeanmaire, Tatiana Starikovskaya}
\date{\today}

\begin{document}
\part[First Algorithms]{Lecture 1 - 28/09}
\localtableofcontents
\section{Organisation}
Mail Tatiana : \url{starikovskaya@di.ens.fr}
Homeworks are 30\% of the final grade, final (theory from lecture)
Textbooks : 
\begin{itemize}
    \item \textsl{Introduction to Algorithms} - Cormen, Leiserson, Riverst, Stein
    \item \textsl{Algorithms on strings, trees, and sequences} - Gusfield
    \item \textsl{Approximation Algorithms} - Vazirani
    \item \textsl{Parametrized Algorithms} - Cygan, Fomin, Kowalik, Lokshtanov, Marx, Pilipczuk, Pilipczuk, Saurabh
\end{itemize}

\section{Introduction}
Algorithm take Inputs and give an output.
\begin{openpb}[Mersenne Prime]
    Find a new prime of form $2^{n} - 1$
\end{openpb}
Algorithms do not depend on the language. Algorithms should be simple, fast to write and efficient.
Word RAM model : Two Parts : one with a constant number of registers of $w$ bits with direct access, and one with any number of registers, only with indirect access (pointers). 
Allows for elementary operations: basic arithmetic and bitwise operations on registers, conditionals, goto, copying registers, halt and malloc. 
To index the memory storing input of size $n$ with $n$ words, we need register length to verify $w \geq \log n$
Algorithms can always be rewritten using only elementary operations. 
Complexity : 
\begin{itemize}
    \item $Space(n)$ is the maximum number of memory words used for input of size $n$
    \item $Time(n)$ is the maximum number of \textsl{elementary} operations used for input of size $n$
\end{itemize}
Complexity Notations : 
\begin{itemize}
    \item $f \in \mathcal{O}(g)$ if $\exists n_0 \in \mathbb{N}, c \in \mathbb{R}_{+},\ f(n) \leq c \cdot g(n), \ \forall n \geq n_0$
    \item $f \in \Omega(g)$ if $\exists n_0 \in \mathbb{N}, c \in \mathbb{R}_{+},\ f(n) \geq c \cdot g(n), \ \forall n \geq n_0$
    \item $f \in \Theta(g)$ if $\exists n_0 \in \mathbb{N}, c_{1}, c_{2} \in \mathbb{R}_{+},\ c_{1} \cdot g(n) \leq f(n) \leq c_{2} \cdot g(n), \ \forall n \geq n_0$
\end{itemize}

\section{Data Structures}
\subsection{Introduction}
Way to store elements of a data base that is created to answer frequently asked queries using pre-processing.
We care about space used, construction, query and update time.
Can be viewed as an algorithm, which analysed on basics.
Containers are basic Data Structures, maintaining the following operations : 
\begin{enumerate}
    \item Random Access : given $i$, access $e_{i}$
    \item Access first/last element
    \item Insert an element anywher
    \item Delete any element
\end{enumerate} 

\subsection{Array}
An array is a pre-allocated contiguous memory area of a \emph{fixed} size. It has random access in $\mathcal{O}(1)$, but doesn't allow insertion nor deletion.

Linear Search : given an integer $x$ return $1$ if $e_{i} = x$ else 0.
\begin{algorithm}
    \caption{Linear Search in an Array. \\ Complexity : Time = $\mathcal{O}(n)$ | Space = $\mathcal{O}(n)$}
    \begin{algorithmic}
        \Input
        $x$
        \EndInput
    \end{algorithmic}
\end{algorithm}

\subsection{Doubly Linked List}
Memory area that does not have to be contiguous and consists of registers containing a value and two pointers to the previous and next elements.
It has random access in $\mathcal{O}(n)$, access/insertion/deletion at head/tail in $\mathcal{O}(1)$.
\begin{algorithm}
    \caption{Insertion in a Doubly Linked List \\ Complexity : $\mathcal{O}(1)$}
    \begin{algorithmic}
        \Input
        $ L, x$
        \EndInput
        \State $x.next \gets L.head$ 
        \If{$L.head \neq NIL$}
            \State $L.head.prev \gets x$
        \EndIf
        \State $L.head \gets x$
        \State $x.prev = Nil$
    \end{algorithmic}
\end{algorithm}


\subsection{Stack and Queue}
Stack : Last-In-First-Out data structure, abstract data structure. Access/insertion/deletion to top in $\mathcal{O}(1)$.

\begin{openpb}[Optimum Stack Generation]
    Given a finite alphabet $\Sigma$ and $X \in \Sigma^{n}$. Find a shortest sequence of stack operations push, pop, emit that prints out $X$. You must start and finish with an empty stack.
    Current best solution is in $\tilde{\mathcal{O}}(n^{2.8603})$.
\end{openpb}

Queue : First-In-First-Out abstract data structure. Access to front, back in $\mathcal{O}(1)$, deletion and insertion at front and back in $\mathcal{O}(1)$.

\section{Approaches to algorithm design}
Solve smalle sub-problems to solve a large one.

\subsection{Dynamic Programming}
Break the problem into many closely related sub-problems, memorize the result of the sub-problems to avoid repeated computation

Examples : 
\begin{algorithm}
    \caption{Recursive Fibonacci Numbers \\ Complexity: Exponential}
    \begin{algorithmic}
        \State RFibo($n$) :
        \Input
        $ n$
        \EndInput
        \If {$n \leq 1$}

            \Return $n$
        \EndIf\\
        \Return RFibo($n-1$) + RFibo($n-2$)
    \end{algorithmic}        
\end{algorithm}

\begin{algorithm}
    \caption{Dynamic Programming Fibonacci Numbers \\ Time = $\mathcal{O}(n)$ | Space = $\mathcal{O}(n)$}
    \begin{algorithmic}
        \Input
        $ n$
        \EndInput
        \State $Tab \gets zeros(n)$ \Comment $zeros(n)$ returns a $n$-array of zeros.
        \State $Tab[0] \gets 0$
        \State $Tab[1] \gets 1$
        \For{$i \gets 2$ to $n$} 
            \State $Tab[i] = Tab[i-1] + Tab[i-2]$
        \EndFor\\
        \Return Tab[n]
    \end{algorithmic}        
\end{algorithm}

Levenshtein Distance between two strings can be computed in $\mathcal{O}(mn)$ instead of exponential time. Based on \url{https://arxiv.org/pdf/1412.0348.pdf}, this is the best one can do. 
RNA folding : retrieving the 3D shape of RNA based on their representation as strings. Currently, we know it is possible to find $\mathcal{O}(n^3)$, in $\tilde{\mathcal{O}}(n^{2.8606})$ and if \emph{SETH} is true, there is no $\mathcal{O}(n^{\omega - \epsilon})$. We know $\omega \in \left[2, 2.3703\right)$

\begin{openpb}
    Is there a better Complexity for RNA folding ? What is the true value of $\omega$ ?
\end{openpb}

Knapsack problem : An optimization problem with bruteforce complexity $\mathcal{O}(2^{n})$.
\begin{algorithm}
    \caption{Knapsack : Dynamic Programming \\
    Time = $\mathcal{O}(nW)$ | Space = $\mathcal{O}(nW)$}
    \begin{algorithmic}
        \Input $\ W, w, v$ \Comment Capacity, weight and values vectors.
        \EndInput

        \State $KP = zeros(n, W)$

        \For {$i \gets 0$ to $n$}  
            \State $KP[i, 0] = 0$
        \EndFor

        \For {$w \gets 0$ to $W$}  
            \State $KP[0, w] = 0$
        \EndFor
    
        \For {$i \gets 0$ to $n$}  
            \For {$w \gets 0$ to $W$}  
                \If {$w < w_{i}$}
                    \State $KP[i, w] \gets KP[i-1, w]$
                \Else
                    \State $KP[i, w] = \max\begin{cases}
                        &KP[i-1, w] \\
                        &KP[i-1, w - w_{i}] + v_{i}
                    \end{cases}$.
                \EndIf
            \EndFor
        \EndFor\\
        \Return $KP[n, W]$
    \end{algorithmic}
\end{algorithm}

\subsection{Greedy Techniques}
Problems solvable with the greedy technique form a subset of those solvable with DP. Problems must have the optimal substrcture property.
Principle : choosing the best at the moment.


Example : The Fractional Knapsack Problem\\
Algorithm : Iteratively select the greatest value-per-weight ratio. \\
\begin{theorem}
    This algorithm returns the best solution, in time $\mathcal{O}(n \log n)$
\end{theorem}
\begin{proof}[By contradiction]
    Suppose we have $\frac{v_{1}}{w_{1}} \geq \ldots \geq \frac{v_{n}}{w_{n}}$. Let $ALG = p = (p_{1}, \ldots, p_{n})$ be the output by the algorithm and $OPT = q = (q_{1}, \ldots, q_{n})$ be optimal.\\
    Assume that $OPT \neq ALG$, let $i$ be the smallesst index such $p_{i} \neq q_{i}$. There is $p_{i} > q_{i}$ by construct. \\
    Thus, there exists $j > i$ such that $p_{j} < q_{j}$. We set $q' = (q_{1}^{'}, \ldots, q_{n}^{'}) = (q_{1}, \ldots, q_{i-1}, q_{i} + \epsilon, q_{i+1}, \ldots, q_{j} - \epsilon\frac{w_{i}}{w_{j}}, \ldots, q_{n})$\\
    $q^{'}$ is a feasible solution : $\sum\limits_{i = 1}^{n} q_{i}^{'} \cdot w_{i} = \sum\limits_{i = 1}^{n} q_{i} w_{i} \leq W$\\
    Yet, $\sum\limits_{i = 1}^{n} q_{i}^{'} \cdot v_{i} > \sum\limits_{i = 1}^{n} q_{i} \cdot v_{i}$, ce qui contredit la 
\end{proof}

\part{TD 1 - 29/09}
\localtableofcontents
\section{Mathematical Complexity}
\subsection{Exercice 1}
\subsubsection{Question 1}
Non : prendre $f = 1$ et $g = exp$.
\subsection{Question 2}
Non, si g = h = f.
\subsubsection{Question 3}
Non : Si on a $f = n, g = n^{2} \in \Omega(f(n)), h = f \in \Theta(f(n))$ alors $g + h \neq \O(f(n))$

\subsection{Exercice 2}
On rappelle la formule de Stirling : \[n! \sim \left(\frac{n^{n}}{e^{n}}\right)\sqrt{2\pi n}\]
Immédiatement, on en déduit la première relation.

On a par ailleurs la seconde égalité en passant au logarithme, fonction continue en $+\infty$

\subsection{Exercice 3}
On rappelle les formules suivantes: 
\[\begin{cases}    
    (n+a)^{b} &= n^{b}(1 + \frac{a}{n})^{b}\\
    (1+\frac{a}{n})^{b} &= 1 + b\frac{a}{n} + o(\frac{a}{n}) \in \left[1; 1+ba\right]
\end{cases}\]
Immédiatement, on a la relation souhaité.

\section{Data Structures}
\subsection{Exercice 4}
Il suffit de diviser l'array en deux sous arrays de taille $n/2$, une array commençant en $i = 0$, une commençant en $j = -1$ et on stocke les deux indices de fin de la pile courant.

\subsection{Exercice 5}
\subsubsection{Question 1}
On définit un algorithme de $reverse$ de list en temps linéaire en ajoutant tous les éléments dans une pile puis en dépilant dans une liste. On effectue bien $2n = \O(n)$ opérations.\\
Il suffit alors de comparer les deux listes en temps linéaire.

\subsubsection{Question 2}
Pour une liste vide, ou d'un seul élément on renvoie True. 
On reverse en place la première moitié de la liste et on la compare à la seconde et normalement ça marche. 

\subsection{Exercice 6}
On utilise deux piles : On push dans la première, et on pop de la seconde. Lorsque la seconde pile est vide, on pop de la première et on push dans la seconde, ce qui permet bien de former une pile. 

\subsection{Exercice 7}
\subsubsection{Question 1}
On utilise les arrays standards et lorsqu'on dépasse la capacité, on double le nombre de cases, qu'on initialise à -1, en stockant l'indice du dernier élément de la liste. On a alors toujours une complexité en espace en $\O(n)$ puisqu'on a toujours au plus $2n$ cases. \\

\subsubsection{Question 2}
On effectue la suite suivante d'instructions, pour $n \in \N$ : \begin{enumerate}
    \item On ajoute $2n$ éléments
    \item On retire $n+1$ éléments
    \item On ajoute $1$ élément
    \item On recommence en modifiant $n$
\end{enumerate}

\subsubsection{Question 3}
Il suffit alors d'attendre de passer en dessous de la barre de 25\% du tableau rempli. On a bien tout de même une complexité en $\O(n)$.

 \section{Greedy Algorithms}
\subsection{Exercice 8}
\subsubsection{Question 1}
\begin{algorithm}
   \caption{Greedy Algorithm for Scheduling Problem} 
   \begin{algorithmic}
        \Input $\ a$ \Comment Vecteur de tuples correspondant aux activités 
        \EndInput
        \State $E \gets ListeVide()$
        \State $Sort(a, (fun : x, y \mapsto x[1] \leq y[1]))$ \Comment On trie les activités par date de fin croissante
        \State $s \gets PileVide()$
        \State $Push(a, s)$  \Comment On ajoute une à une les activités de $a$ dans une pile.
        \While ($s$)
            \State $ac \gets Pop(s)$
            \If ($ac$ est compatible)
                \State $Ajouter(E, ac)$
            \EndIf
        \EndWhile\\
        \Return $E$
   \end{algorithmic}
\end{algorithm}
\begin{proof}[Correction]
    On introduit une solution optimale, la plus proche possible de l'algo.     
\end{proof}

\subsubsection{Question 2}
On prend $T1 = \left[1, 2\right], T2 = \left[3, 4\right], T3 = \left[1.5, 2.5\right]$

\subsubsection{Question 3}
Bon on fait de la Programmation Dynamique. Relation de récurrence $\forall i DP(i)$ est le max des poids sur $\left\{T_{1}, \ldots, T_{i}\right\}$
\[
\begin{cases}
    DP(0) &= 0\\
    DP(i+1) &= \max(DP(i), w_{i+1} + DP(p(i+1))) \text{ où } p(i) \text{ est le dernier indice de la dernière tâche compatible} 
\end{cases}    
\]

\part[Recursion]{Lecture 2 - 5/10}
\localtableofcontents
\section{Divide and Conquer}
Divide a problem into smaller ones to solve those, then combine the solutions to get a solution of the bigger problem.\\
Example : \textsl{Merge Sort} : Its complexity verifies $T(n) = T(\lceil n/2\rceil) + T(\lfloor n/2 \rfloor) + \O(n)$. From that we will derive that $T(n) = \O(n\log n)$

\section{Analysis of Recursive Algorithms}
We have recurrences we want to solve. We have three methods : 


\subsection{Substitution Method}
The method :
\begin{enumerate}
    \item Guess the asymptotic of $T(n)$
    \item Show the answer via induction
\end{enumerate}
For \textsl{Merge Sort}: we guess $T(n) \leq c \cdot n\log_{2}n, \forall n \geq 2$. We choose $c$ that verifies that property until $n = 6$. \\
Substituting in the recurrence equation : \[T(n) \leq cn\log_{2}\frac{n}{2} + c\log_{2}\frac{n}{2} + c\frac{n+2}{2} + a\cdot n = c \dot n\log_{2}n + a \cdot n + c \cdot \log_{2}n - c\frac{n}{2}\]
If we then choose $c$ so that it is bigger than $20a$ we get : 
\[T(n) \leq cn\log_{2}n + a\cdot n - c\cdot n/20 \leq cn\log_{2}n\]

\subsection{Recursion-tree Method}
\begin{enumerate}
    \item Simplify the equation : 
    \begin{itemize}
        \item Delete floors and ceils
        \item Suppose $n$ is of a good form
    \end{itemize}
    \item Draw a tree, rooted with the added term and the recursive calls
    \item Start again, and recursively fill the tree
\end{enumerate}
We get a tree of depth $\log_{k}n$ if $n$ is divided by $k$ in recursive calls. We can now sum the values of the nodes, to get an approximation, and start verifying.

\section{Master Theorem}
\subsection{The Theorem}
\begin{theorem}[Master Theorem]\label{thm:MT}
    If we have recurrence equation $T(n) = aT(n/b) + f(n)$ where $a \geq 1, b > 1$ are integers, $f(n)$ is asymptotically positive. Let $r = \log_{b}a$, we have : 
    \begin{enumerate}
        \item If $f(n) = \O(n^{r-\epsilon})$ for some $\epsilon > 0$, then $T(n) = \Theta(n^r)$
        \item If $f(n) = \Theta(n^{r})$ then $T(n) = \Theta(n^{r}\log n)$
        \item If $f(n) = \Omega(n^{r + \epsilon})$ for some $\epsilon > 0$, and $af(n/b) \leq cf(n)$ for some constant $c < 1$ and all sufficiently large $n$ (regularity condition) then $T(n) = \Theta(f(n))$.
    \end{enumerate} 
\end{theorem}
\begin{remark}
    The Master Theorem \ref{thm:MT} does not cover all possible cases for $f(n)$. Example : $f(h) = h^{r}/\log h$
\end{remark}
\begin{remark}
    The Master Theorem \ref{thm:MT} is sometimes called \textsc{Théorème sur les Récurrences de Partition}
\end{remark}

\section{The Proof}
    Plan : \begin{itemize}
        \item Analyse the recurrence as if $T$ is defined over reals (continuous version)
        \item Prove the discrete version
    \end{itemize}

\subsection{Continuous Master Theorem}
\begin{proof}
    \begin{lemma}
        Define $T(n) = \begin{cases}
            \Theta(1) &\text{if } n \leq \hat{n}\\
            aT(n/b) + f(n) &\text{if } n > \hat{n}
        \end{cases}$
        Then \[T(n) = \Theta\left(n^{r}\right) + \sum_{k=0}^{\lceil \log_{b}(n/\hat{n})\rceil - 1}a^{k}f(n/b^{k})\]
    \end{lemma}
    \begin{proof}
        In the Recursion-Tree, stopped when the argument of $T$ is smaller than $\hat{n}$ which is when depth is $\lceil \log_{b}(n/\hat{n})\rceil - 1$, we get : 
        \[
            \begin{aligned}
            T(n) &\leq \sum_{k=0}^{\lceil \log_{b}(n/\hat{n})\rceil - 1}a^{k}f(n/b^{k}) + \Theta(a^{log_{b}(n/\hat{n})})\\ &= \sum_{k=0}^{\lceil \log_{b}(n/\hat{n})\rceil - 1}a^{k}f(n/b^{k}) + \Theta(a^{\log_{b}(n)})\\ &= \sum_{k=0}^{\lceil \log_{b}(n/\hat{n})\rceil - 1}a^{k}f(n/b^{k}) + \Theta(n^{\log_{b}(a)})
            \end{aligned}
        \]
    \end{proof}
    Back to the proof : 
    \begin{lemma}
        Define $g(n) = \Theta(n^{r}) + \sum_{k= 0}^{q} a^{k}f(n/b^{k})$ Then : 
        \begin{enumerate}
            \item If $f(n) = \O(n^{r - \epsilon})$ then $g(n) = \Theta(n^{r})$
            \item If $f(n) = \Theta(n^{r})$ then $g(n) = \Theta(n^{r}\log{n})$
            \item If $f(n) = \Omega(n^{r + \epsilon})$ and we have the regularity condition then $g = \Theta(f)$
        \end{enumerate}
    \end{lemma}
    \begin{proof}
        \begin{enumerate}
            \item Case 1 : \[
            g(n)  
            \begin{aligned}
                &= \Theta(n^{r}) + \sum_{k= 0}^{q} a^{k}f(n/b^{k}) \\ 
                &= \Theta(n^{r}) + \O\left(\sum_{k= 0}^{q} a^{k}(n/b^{k})^{r - \epsilon}\right)\\
            \end{aligned}
            \]
            However : 
            \[
                \begin{aligned}
                    \sum_{k= 0}^{q} a^{k}(n/b^{k})^{r - \epsilon} &= n^{r-\epsilon}\sum_{k = 0}^{q} (ab^{\epsilon}/b^{r})^{k}\\
                    &= n^{r-\epsilon}\sum_{k=0}^{\lceil \log_{b}(n/\hat{n})\rceil - 1}(b^{\epsilon})^{k} = \O(n^{r - \epsilon}(n/\hat{n})^{epsilon})
                \end{aligned}    
            \]
            Thus : $g(n) = \Theta(n^{r})$
            % --------------------------------------------------------------------------
            \item Case 2 : We have : 
            \[
            \begin{aligned}
                g(n) &= \Theta(n^{r}) + \sum_{k=0}^{q} a^{k}f(n/b^{k})\\
                &= \Theta(n^{r}) + \Theta\left(\sum_{k=0}^{q}a^{k}(n/b^{k})^{r}\right)\\
            \end{aligned}    
            \]
            However : 
            \[
              \begin{aligned}
                \sum_{k=0}^{q}a^{k}(n/b^{k})^{r} &= n^{r}\sum_{k = 0}^{q} (a/b^{r})^{k}\\
                &= n^{r}\sum_{k = 0}^{\lceil \log_{b}(n/\hat{n})\rceil - 1}1 = n^{r}\Theta(\log_{b}n/\hat{n})
              \end{aligned}  
            \]
            % ----------------------------------------------------------------------------
            \item Case 3 : By induction on $k$ : $a^{k}f(n/b^{k}) \leq c^{k}f(n)$. Thus : 
            \[
                \sum_{k = 0}^{q}a^{k}f(n/b^{k}) \leq \sum_{k = 0}^{q}c^{k}f(n) = f(n) \sum_{k= 0}^{q}c^{k} = \Theta(f(n))
            \]
        \end{enumerate}
    \end{proof}
    We thus have proved the continuous Master Theorem.
\end{proof}

    \subsection{Discrete Master Theorem}
    We have now showed the continuous Master Theorem, following \textsc{William Kuszmaul, Charles E. Leiserson}, \href{https://epubs.siam.org/doi/pdf/10.1137/1.9781611976496.15}{\textit{Floors and Ceilings in Divide-and-Conquer Recurrences}}, Symposium on Simplicity in Algorithms 2021.\\
        \begin{proof}
            See slides below
            \includepdf[nup = 1x2, pages = 32-45]{Lecture_2.pdf}    
        \end{proof}
        \begin{remark}[Remarks on the Proof]
            \begin{itemize}
                \item Lemmas 1 to 3 serve to show that the argument does not go too far when it is rounded up or down.
                \item Slide 36 Last Line : $\frac{2}{\beta^{i} - 1} < \frac{4}{\beta^{i}}$ for $i \geq i_{0}$. Thus : $\sum_{i = 1}^{\infty} \frac{2}{\beta^{i} - 1} < \sum_{i = 1}^{\infty}\frac{4}{\beta^{i}} + \sum_{i = 0}^{i_0}\frac{2}{\beta^{i} - 1}$ and that last sum is $\O(1)$
                \item Slide 37 Line 3 : The first inequalities comes from the Recursion-Tree, so that we can ensure the argument does not deviate to much, by the second inequalities. 
            \end{itemize}
        \end{remark}
    \subsection{Use Cases}
    Using the Master Theorem we can show the complexity of many algorithms :
    \begin{enumerate}
        \item Merge Sort Complexity : $T(n) = T(\lceil n/2 \rceil) + T(\lfloor n/2 \rfloor) + \O(n) = \Theta(n\log n)$
        \item Strassen's Algorithm for Matrix Multiplication : $T(n) = 7 T(n/2) + \Theta(n^{2}) \Rightarrow T(n) = \O(n^{\log_{2}7}) = \O(n^{2.8074})$
    \end{enumerate}

\section{Fast Multiplication of Polynomials}
The sum of two degree $n$ polynomials can be done in $\O(n)$, Horner's rule for evaluation produces $\O(n)$ complexity. The naïve product can be done in $\O(n^2)$\\
Remembering Lagrange's Theorem on Polynomials (or Vandermonde's Determinant, or anything really), degree $n$ polynomials are entirely represented by their point-value reprensentation over $n$ distinct points $(x_{i}, y_{i})$.
Then, by converting the coefficient reprensentation to point-value representation, then by point-wise multiplicating the polynomials, then by going back to the coefficient representation, we can have a better algorithm.

\subsection{Point-Value Multiplication}
It is easily done in $\O(n)$ if both polynomials are represented over the same axis.

\subsection{Coefficient to Point-Value Conversion - Fast Fourier Transform}
For $P = \sum_{i = 0}^{n-1} a_{i}x^{i}$, we define : \[
  \begin{aligned}
    &P_{odd}(x) &= a_{n-1}x^{n/2-1} + a_{n-3}x^{n/2-3} + \ldots + a_{1}x\\
    &P_{even}(x) &= a_{n-2}x^{n/2-2} + a_{n-4}x^{n/2-4} + \ldots+ a_{2}x^{2/2} + a_{0}
  \end{aligned}  
\]
\begin{enumerate}
    \item We have : $P = xP_{odd}(x^{2}) + P_{even}(x^{2})$
    \item We evaluate $P_{odd}, P_{even}$ at $(\omega_{n}^{i})^{2}$ recursively by the halving property.
    \item We combine the result.
\end{enumerate}

\subsection{Point-Value to Coefficient Conversion - Inverse Fast Fourier Transform}
\begin{theorem}
    $V_{n}^{-1}[i, j] = \omega_{n}^{-ij}/n$
\end{theorem}

\part[Hashing]{Lecture 3 - 12/1}
\localtableofcontents

Answering queries of appartenance on a set, maintaining a dictionary. Python dictionaries do not have upper bound guarantees, we shouldn't use them. \\
Suppose we have a set $S$ over a universe $U$. We dnote by $n$ the number of keys, $m$ the size of the hash table

\section{Naïve Array-based implementation}
Here we assume that no two objects have equal keys. We store $1$ in a bitvector of length $|U|$ at each position $i$ such that $i$ is in $S$.\\
This takes $\O(|U|)$ space, for $\O(1)$ search time, and $\O(1)$ modification time. 

\section{Chained Hash Tables}
We give ourselves a function $h : U \rightarrow \left[1, m\right]$. We send the keys to their image by $h$ in a table. However, since we have no guarantee that $\left|U\right| \leq m$, there might be collisions.\\
To deal with that, instead of storing in the table a boolean, we store a list of all the keys corresponding to $h(k)$. \\
Then we insert a key in constant time $\O(1)$, search a key in time $\O(\left|h[key]\right|)$ and delete a key in time $\O(\left|h[key]\right|)$ since in the worst case the key is at the end of the list.\\

Further analysis leads to the following theorem. 
\begin{theorem}[Simple Uniform Hashing Assumption]
    Assuming SUHA : \og \textit{$h$ equally distributes the keys into the table slots}\fg, and assuming $h(x)$ can be computed in $\O(1)$, $E\left[T_{search}(n)\right] = \O\left(1 + \frac{n}{m}\right)$, and same for deletion time. \\
    Formally, SUHA is : 
    \[
        \begin{aligned}
            \forall y \in \left[1, \abs{T}\right] & \mathbb{P}\left(h(x) = y\right) = \frac{1}{\abs{T}}\\
            \forall y_{1}, y_{2} \in \left[1, \abs{T}\right]^{2} & \mathbb{P}\left(h(x_{1}) = y_{1},\ h(x_{2}) = y_{2}\right) = \frac{1}{\abs{t}^{2}}
        \end{aligned}    
    \]
\end{theorem}
\begin{proof}
    \begin{enumerate}
        \item Unsuccessful Search : Suppose that $k_{0}, \ldots, k_{n-1}$ are keys in the dictionary and we perform an unsuccessful search for a key $k$.
        The number of comparisons is : $\sum_{i = 0}^{n-1} \mathds{1}_{h(k) = h(k_{i})}$. Then, the expected time is : $\mathbb{E}[T_{search}] = \frac{n}{\abs{T}}$ by SUHA.
        \item Successful Search : Suppose keys were introduced in order $k_{0}, \ldots, k_{n-1}$. $k_{i}$ appears before any of $k_{0}, \ldots, k_{i-1}$ and after any of $k_{i+1}, \ldots, k_{n-1}$ that are in the same linked list. Then, to search for $k_{i}$, we need $\sum_{j = i + 1}^{n- 1} \mathds{1}_{h(k_{j}) = h(k_{i})}$. Under SUHA, the expectation of each of these variables is $\frac{1}{\abs{T}}$. Then, the average expected search time is : $\frac{1}{n}\sum_{i = 0}^{n-1} = 1 + \frac{1}{n}\sum_{i = 0}^{n-1}\frac{(n-1-i)}{\abs{T}} = \O(1 + \frac{n}{\abs{T}})$
    \end{enumerate}
    This concludes the proof of the theorem. 
\end{proof}

Good hash functions are functions that distribute the keys evenly. Yet, we do not know what the keys are, and thus will need various heuristics to answer this question. At least, without loss of generality, we can assume that keys are integers.

\subsection{Heuristic Hash Functions}
    \begin{itemize}
        \item Division Method : $h(k) = k \mod m$. It is better to choose $m$ to be a prime number, and avoid $m = 2^{p}$.
        \item Multiplication Method : $h(k) = \left\lfloor m \left\{k\cdot A\right\}\right\rfloor$. $A \in (0, 1)$ and $m = 2^{p}$.
    \end{itemize}
Yet, fixing the function can allow anyone to construct a probability distribution for which the function will be "bad".

\subsection{Universal Family of Hash Functions}
$H = \left\{h : U \rightarrow \left[0, \abs{T}-1\right]\right\}$ is Universal if :
\[
    \forall k_{1} \neq k_{2} \in U, \ \abs{\left\{h \in H \mid h(k_{1}) = h(k_{2})\right\}} \leq \frac{\abs{H}}{m}
\]
\begin{theorem}
    If $h$ is a hash function chosen uniformly at random from a universal family of hash functions. Suppose that $h(k)$ can be computed in constant time and there are at most $n$ keys. Then the expected search time is $\O(1 + \frac{n}{\abs{T}})$
\end{theorem}
\begin{proof}
    Same as the case when $h$ satisfies SUHA. Observe that the probability comes this time from choosing the function. 
\end{proof}

\begin{theorem}
    Let $p \in \mathcal{P}$ such that $U \subseteq \left[0, p-1\right]$. Then $H = \left\{h_{a, b}(k) = \left(\left(ak + b\right) \mod p\right) \mod \abs{T} \mid a \in \Z_{p}^{*}, \ b \in \Z_{p} \right\}$ is a universal family.
\end{theorem}
\begin{proof}
    Let $k_{1} \neq k_{2} \in U$. Let $l_{i} = (ak_{i} + b) \mod p$. We have $l_{1} \neq l_{2}$ and $ a = ((l_{1} - l_{2})((k_{1}-k_{2})^{-1} \mod p) \mod p) \text{ and } b = (l_{1} - ak_{1}) \mod p$. There is then one-to-one mapping between $(a, b)$ and $(l_{1}, l_{2})$. The number of $h \in H$ such that $h(k_{1}) = h(k_{2})$ is :
    \[
        \abs{\left\{(l_{1}, l_{2})\mid l_{1} \neq l_{2} \in \Z_{p}, \ l_{1} = l_{2} \mod m \right\}} \leq \frac{p(p-1)}{\abs{T}} \leq \frac{\abs{H}}{\abs{T}}
    \]
\end{proof}


\section{Open Addressing}
Elements are stored in the table. To insert, we probe the hash table until we find $x$ or an empty slot. If we find an empty slot, insert $x$ here. To define which slots to probe, we use a hash function that depends on the key and the probe number. To search, we probe the hash table until we either find $x$ (return YES) or an empty slot (return NO). In the analysis, we will assume $h$ to be uniform. 

\begin{theorem}[Analysis]
    Given an open-address hash-table with load factor $\alpha = \frac{n}{\abs{T}} < 1$, the expected number of probes in an unsuccessfulsearch is at most $\frac{1}{1-\alpha}$, assuming uniform hashing. 
\end{theorem}
\begin{proof}
    An unsuccessful search on $x$ means that every probed slot except the last one is occupied and does not contain $x$, and the last one is empty. 
    We define $A_{i}$ the event \og The $i$-th probe occurs and is occupied.\fg. By Bayes's Theorem, we must estimate : 
    \[
        \mathbb{P}[\text{\# of probes} \geq i] = \prod_{k = 1}^{i - 1} \mathbb{P}[A_{k} \mid \bigcap\limits_{j = 1}^{k - 1} A_{j}]
    \]
    But we have : $\mathbb{P}[A_{j}\mid A_{1} \cap \ldots \cap A_{j - 1}] = \frac{n - j + 2}{\abs{T} - j + 2}$
    So we have : $\mathbb{P}[\text{\# of probes} \geq i] \leq \frac{n}{\abs{T}}^{i - 1} = \alpha^{i - 1}$
    Then, the expected number of probes is : 
    \[
        \sum_{i = 1}^{+\infty} \mathbb{P}[\text{\# of probes} \geq i] \leq \sum_{i = 1}^{+\infty} \alpha^{i - 1} = \frac{1}{1 - \alpha}
    \]
\end{proof}
\begin{corollary}
    The expected number of probes during insertion is at most $\frac{1}{1 - \alpha}$.
\end{corollary}
\begin{proof}
    If we insert $x$ we first ran an unsuccessful search for it
\end{proof}

\begin{theorem}
    The expected number of probes during a Successful search is at most $\frac{1}{\alpha}\ln\frac{1}{1- \alpha}$.
\end{theorem}
\begin{proof}
    A successful search for $x$ probes the same sequence of slots as insertion.\\
    If $x$ is the $i$-th element inserted into the table, insertion probes less than $\frac{1}{1 - \frac{i}{\abs{T}}}$ slots in expectation.
    Therefore, the expected time of a successful search is at most : 
    \[
        \frac{1}{n}\sum_{i = 0}^{n - 1}\frac{\abs{T}}{\abs{T} - i} = \frac{\abs{T}}{n}\sum_{i = 0}^{n - 1}\frac{1}{\abs{T} - i} = \sum_{i = 0}^{\abs{T}}\frac{1}{i} - \sum_{i = 0}^{\abs{T}-n} \frac{1}{i} \leq \frac{\abs{T}}{n}\ln\frac{\abs{T}}{\abs{T} - n} = \frac{1}{\alpha}\ln\frac{1}{1 - \alpha}
    \]
\end{proof}

This is hard to implement, so we will use heuristics.

\subsection{Heuristic hash functions}
Let $h^{'}, h^{''}$ be two auxiliary hash functions. 
\begin{itemize}
    \item Linear Probing : $h(k, i) = (h^{'}(k) + i) \mod \abs{T}$
    This is easy to implement but it suffers from clustering. 
    \item Quadratic Probing : $h(k, i) = (h^{'}(k) + c_{1}i + c_{2}i^{2}) \mod \abs{T}$
    We must choose the constants $c_{1}$ and $c_{2}$ carefully, and this still suffers from clustering. 
    \item Double Hashing : $h(k, i) = (h^{'}(k) + i h^{''}(k)) \mod \abs{T}$
    To use the whole table, $h^{''}(k)$ must be relatively prime to $m$, e.g. $h^{''}(k)$ is always odd, $m = 2^{i}$.
\end{itemize}

\section{Cuckoo Hashing}
Hashing scheme with search time constant in the worst case, as it maintains a hash function without collisions to achieve perfect hashing. This is possible if the set of keys is static.
Assume that we have two hash functions $h_{1}, h_{2}$ that satisfy SUHA. We store $x$ in either $T[h_{1}(x)]$ or $T[h_{2}(x)]$. Search for $x$ is done in constant time.

\begin{algorithmic}
    \Function{Insert}x
    \If {$ x = T[h_{1}(x)] \text{ or } x = T[h_{2}(x)]$}
        \Return
    \EndIf

    \State $pos \gets h_{1}(x)$
    \For {$i \gets 0 \text{ to } n$}
        \If $T[pos] = Null$
            \State $T[pos] = x$; \Return
        \EndIf

        \State $x \longleftrightarrow T[pos]$
        \If {$pos = h_{1}(x)$} 
            \State $pos \gets h_{2}(x)$
        \Else 
            \State $pos \gets h_{1}(x)$
        \EndIf
    \EndFor
    \State \textsc{Rehash}
    \State \textsc{Insert}(x)
    \EndFunction
\end{algorithmic}
\begin{theorem}
    This insertion is done in constant time.
\end{theorem}
\begin{lemma}
    Suppose that $\abs{T} \geq c \cdot n$ for some $c > 1$. For any $i, j$, the probability that there exists a path from $i$ to $j$ of length $l \geq 1$ which is a shortest path from $i$ to $j$ is at most $\frac{1}{c^{l}\cdot \abs{T}}$
\end{lemma}
\begin{proof}
    By induction on $l$ :
    \begin{itemize}
        \item Initialization : By SUHA, $\mathbb{P}[h_{1, 2}(x) = y] = \frac{1}{\abs{T}}$. Thus $\mathbb{P}[\text{there is an edge from }i \text{ to } j] = \frac{n}{\abs{T}^{2}} \leq \frac{1}{c\abs{T}}$
        \item Heredity : For $l \geq 1$ there must exist $k$ such that there is a path of length $l - 1$ from $i$ to $k$ and an edge from $k$ to $j$.
    \end{itemize}
\end{proof}
\begin{proof}
    We define the bucket of $x$ as all the cells that can be reached either from $h_{1}(x)$ or $h_{2}(x)$. $x, y$ are in the same bucket if and only if there is a path from $\left\{h_{1}(x), h_{2}(x)\right\}$ to $\left\{h_{1}(y), h_{2}(y)\right\}$. Then :
\[
    \mathbb{P}[x, y \text{ are in the same bucket}] \leq 4 \sum_{l = 1}^{\infty}\frac{1}{c^{l}\abs{T}} = \frac{4}{(c - 1)\abs{T}}
\]
So : 
\[
    \mathbb{E}[\abs{\text{bucket of } x}] = \mathbb{E}\sum X_{x, y} = \sum \mathbb{E}[X_{x, y}] = \sum \mathbb{P}[x, y \text{ are in the same bucket}] \leq \frac{4}{c - 1}
\]    
Hence, in the absence of rehash, expected insertion time in constant.\\

The probability that we need a rehash is at most probability that there is a cycle, i.e. a path from $i$ to $i$ : $\frac{4}{c - 1} \leq \frac{1}{2}$.
The probability that we will need $d$ rehashes is then at most $\frac{1}{2^{d}}$.
Thus, the expected time per insertion is : 
\[
    \frac{1}{n}\cdot\O(n)\sum_{d = 1}^{+\infty}\frac{1}{2^{d}} = \O(1)
\]
\end{proof}

\subsection{Rolling Hash Functions}
\begin{definition}
    The Karp-Rabin fingerprint of a string $S = s_{1}\ldots s_{m}$ is : 
    \[
        \phi(s_{1}\ldots s_{m}) = \sum_{i = 1}^{m}s_{i}r^{m - 1}\mod p    
    \]
    where $p$ is a big enough prime and $r \in \mathbb{F}_{p}$.
\end{definition}
\begin{proposition}
    \begin{itemize}
        \item If S = T, then $\phi(S) = \phi(T)$
        \item Else, $\phi(S) \neq \phi(T)$ with high probability.
    \end{itemize}
\end{proposition}
\begin{proof}
    Let $\sigma$ be the size of the alphabet, $p \geq max \left\{\sigma, n^{c}\right\}$ where $c > 1$ is a constant. We have: 
    \[
        \phi(S) = \phi(T) \Leftrightarrow \sum_{i = 1}^{m}(s_{i} - t_{i}) \cdot r^{m - i} \mod p = 0
    \]
    Hence, $r$ is a root of $P(x) = \sum_{i = 1}^{m}(s_{i} - t_{i}) \cdot x^{m-i}$ a polynomial over $\mathbb{F}_{p}$. The probability of such event is at most $\frac{m}{p} \leq \frac{1}{n^{c-1}}$.
\end{proof}
The Karp-Rabin algorithm is as follows : 
\begin{itemize}
    \item Compute the fingerprint of the pattern
    \item Compare it with the fingerprint of each $m$-length substring of the text. If the fingerprint is equal to the fingerprint of a substring, report it as an occurrence
\end{itemize}
\begin{proposition}
    We have : 
    \[
        \phi(s_{1}\ldots s_{j+1}) = \phi(s_{1}\ldots s_{j})\cdot r + s_{j+1} \mod p    
    \]
\end{proposition}
\begin{proof}
    Observe that : 
    \[
    \begin{aligned}
        \phi(s_{1}\ldots s_{j+1}) &= \sum_{i = 1}^{j+1}s_{i}r^{j+1-i} \mod p \\
        \
    \end{aligned}    
    \]
\end{proof}

\section{TD 3 : Hashing - 13/10}
\subsection{Exercise 1}
We can expect to have $\frac{n(n-1)}{m} - 1$ collisions, i.e. if we consider the sets of pairs of values with collisions, we can expect its cardinality to be $2\times (\frac{n(n-1)}{m} - 1)$.

\subsection{Exercise 2}
By definition of $h$, the sets $\left(h^{-1}(i)\right)_{i \in \left[1, m\right]}$ form a partition of $U$. Thus, since $\abs{U} > nm$, there exists $i$ such that $\abs{h^{-1}(i)} > n$, giving us the result. 

\subsection{Exercise 3}
We get, by sum, if $k = k_{n}\ldots k_{0}$, $h(k) = \sum_{i = 0}^{n} k_{i}*(2^{p})^{i} \mod 2^{p}-1 = \left[\sum_{i = 0}^{n} k_{i}\right] \mod 2^{p}-1$. This formula shows the order of the caracters doesn't matter in the value by the hash function.

\subsection{Exercise 4}
It is sufficient to find the cardinality of the set $\left\{ih_{2}(k) \mid i \in \left[0, m-1\right]\right\}$. This is, by Euler's formula $m$ if and only if $h_{2}(k) \wedge m = 1$

\subsection{Exercise 5}

\subsection{Exercise 6}
\subsubsection{Question a.}
We know that the number of probes during insertion is the number of probes seen during search.\\
An unsuccessful search on $x$ means that every probed slot except the last one is occupied and does not contain $x$, and the last one is empty. 
We define $A_{i}$ the event \og The $i$-th probe occurs and is occupied.\fg. By Bayes's Theorem, we must estimate : 
\[
    \mathbb{P}[\text{\# of probes} \geq i] = \prod_{k = 1}^{i - 1} \mathbb{P}[A_{k} \mid \bigcap\limits_{j = 1}^{k - 1} A_{j}]
\]
But we have : $\mathbb{P}[A_{j}\mid A_{1} \cap \ldots \cap A_{j - 1}] = \frac{n - j + 2}{\abs{T} - j + 2}$\\
So we have : $\mathbb{P}[\text{\# of probes} \geq i] \leq \frac{n}{\abs{T}}^{i - 1} = \alpha^{i - 1}$\\
Here, $\alpha \leq \frac{1}{2}$ thus we have the result. 

Then, we get that the probability of the $i$-th insertion requiring strictly more than $2\log_{2}(n)$ probes is at most $\frac{1}{n^{2}}$


\subsubsection{Question b.}
We have : $\mathbb{P}[X > 2\log_{2}(n)] = \mathbb{P}\left[\bigcup_{i=1}^{n}{X_{i} > 2 \log_{2}(n)}\right] < \sum_{i = 1}^{n}\mathbb{P}\left[X_{i} > 2\log_{2}(n)\right] = \frac{1}{n}$. 
The last inequality is strict as these events are not independent. 

\subsubsection{Question c.}
The expected length of the longest probe sequence is :
\[
        \mathbb{E}[X] \leq \mathbb{P}[X \leq 2\log_{2}(n)]\times 2\log_{2}(n) + \mathbb{P}[X > 2\log_{2}(n)]\times n = \O(\log_{2}(n))
\]

\subsection{Exercise 7}
\subsubsection{Question a.}
If $H$ is $2$-independent, then for each pair of distinct keys $x_{1}, x_{2}$ and $y_{1}, y_{2}$ is $\mathbb{P}[h(x_{1}) = y_{1}, h(x_{2}) = y_{2}] = \frac{1}{m^{2}}$. 
If we fix $y_{1} = y_{2}$, we have : $\mathbb{P}[h(x_{1}) =y_{1}= h(x_{2})] = \frac{1}{m^{2}}$. By summing over all values for $y_{1}$ :
$\mathbb{P}[h(x_{1}) = h(x_{2})] = \frac{1}{m}$.\\
However, as $H$ is universal if and only if : $\mathbb{P}[h(x_{1} = h(x_{2}))\mid h \gets^{s} H] \leq \frac{1}{m}$, we have the result. 

\subsubsection{Question b.}
By the same argument as in the class, $H$ is universal because we have a one-to-one mapping between $U$ and $H$. Moreover, with $x = (0, \ldots, 0) \mod p$, we get the result. 

\subsubsection{Question c.}
Again this family is universal. We have : $\mathbb{P}[h^{'}_{a, b}(x_{1}) = y_{1} \wedge h^{'}_{a, b}(x_{2}) = y_{2}] = \frac{1}{m^{2}}$ since $a$ and $b$ are uniquely determined by $h^{'}(a, b)$. 

\subsubsection{Question d.}
Given the fact that we $H$ is $2$-independent, we get the result since this comes from : $\mathbb{P}[h(m) = t \wedge h(m_{0}) = t_{0}] = \frac{1}{p}^{2}$. But, knowing the family, since this family is universal, the adversary might have better knowledge of which functions have same image over $m$, he can succeed to fool Bob with probability at most $\frac{1}{p}$.

\subsection{Exercise 8.}
\subsubsection{Question a.}
This is Langrange's Theorem. 

\subsubsection{Question b.}
As there is an even number of elements in $\mathbb{F}$, there are as many elements in $\mathbb{F}$ ordered with an even and an odd number. Then, by question 1, since there is a one-to-one mapping between $\mathbb{F}^{4}$ and $G$, there are, for each tuple in $\left\{\pm 1\right\}^{4}$, $2^{4} = m^{4}$ functions that suit. Thus, $H$ is 4-independent. 

\part[Integer Sets]{Lecture 4 : 26/10}
\section{Binary Search Trees}
\begin{definition}
    A Binary Tree is a rooted tree with every node having at most two children. Binary search trees are binary trees verifying the following : 
    \begin{center}
        If a node contains element $l$, the left subtree rooted at the left child contains only elements $\leq l$ and the right subtree elements $> l$.
    \end{center}
\end{definition}

\begin{proposition}
    To access a given element, the time needed is $\O(h)$ time where $h$ is the height of the tree.
\end{proposition}
$h$ is not necessarily small, there are many implementations to bound $h$.

\subsection{Red-Black Trees}
\begin{definition}
    A red-black tree is a binary tree that satisfies the following red-black properties :
    \begin{enumerate}
        \item Every node has a color : red or black
        \item The root is black
        \item Every leaf (NIL) is black
        \item If a node is red, both its children are black
        \item For each node, all paths from the node to descendant leaves contain the same number of black nodes. 
    \end{enumerate}
\end{definition}
\begin{lemma}
    The height of a red-black tree with $n$ nodes is at most $2\log(n+1)$.
\end{lemma}
\begin{proof}
    We introduce $bh(x)$ the number of black nodes in a path from $x$ to a leaf.\\
    By induction on $bh(x)$, the subtree rooted at $x$ contains at least $2^{bh(x)} - 1$ : 
    \begin{itemize}
        \item Initialization : $bh(x) = 0$ implies the tree has more than $0$ nodes which is true.
        \item Heredity : 
        \begin{itemize}
            \item If $x$ is black, let $y_{1}, y_{2}$ denote its children. We have $bh(y_{1}) = bh(x) - 1 = bh(y_{2}) - 1$. By summing, we get the result.
            \item If $x$ is red, both of its children are black by rule 4. Let us denote the grand children of $x$ by $z_{1}, \ldots, z_{4}$. We have : $bh(z_{i}) = bh(x) - 1$. By sum, we get the result.
        \end{itemize} 
    \end{itemize}
    Then if $h$ the black height of the tree, $n\geq 2^{h}-1$ and $h \leq \log{n+1}$. Since in any root-to-leaf path, the number of nodes is at most twice the number of black nodes per rule 4, the lemma follows. 
\end{proof}

\begin{proposition}[Insertion]
    We insert a node into a n-node red-black tree by : 
    \begin{itemize}
        \item Inserting the node into $T$ as if it were an ordinary binary search tree
        \item We color it red
        \item We perform a number of rotations and node recolouring.
    \end{itemize}
    This is done in $\O(\log n)$.
\end{proposition}

We define the left rotation on a node $x$ whose right child is not null like this : 
\[Tree(x , \alpha, Tree(y, \beta, \gamma)) \mapsto Tree(y, Tree(x, \alpha, \beta), \gamma)\]
We define right rotations to be the reverse operation.
\begin{proposition}
    Rotations do not break the BST properties.
\end{proposition}
\begin{proof}[Only on right rotations]
    Elements in $\gamma$ are greater than $y$, in $\beta$ are greater than $x$ and lower or equal than $y$, and elements in $\alpha$ are lower or equal than $x$. After the rotation, we still have those properties.
\end{proof}

\begin{proposition}[Restoring red-black properties]
    Rule $1$ is never violated. Rules $2$ and $3$ can be fixed in $\O(1)$ time. Rule $5$ is never violated as we colour the new node red.\\
    We will try and maintain the invariant that the properties are broken in at most one node. There are three cases in which Rule $4$ can be violated : Let $z$ be the node that violates the red-black properties. 
    \begin{enumerate}
        \item $z$'s uncle $y$ is red $\Rightarrow$ We colour $z.p$ black, colour $y$ black and $z.p.p$ red. We moved the problematic node up by two layers.
        \item $z$'s uncle $y$ is black and $z$ is a right child $\Rightarrow$ We left-rotate around the edge $z.p$-$z$, we then have $z$'s uncle $y$ to be black and $z$ to be a left child, i.e. case 3.
        \item $z$'s uncle $y$ is black and $z$ is a left child $\Rightarrow$ We color $z.p$ black, colour $z.p.p$ red and right-rotate.
    \end{enumerate}
\end{proposition}

\begin{proposition}[Complexity]
    Red black tree use $\O(n)$ space, $\O(\log n)$ in time for insertion and deletion in the worst case. % TODO : add deletion.
\end{proposition}

\subsection{Treaps}
\begin{definition}
    Treaps are binary trees with every node having a key and a priority verifying :
    \begin{itemize}
        \item a binary search tree for the keys
        \item a min-heap for the priorities
    \end{itemize}
\end{definition}

\begin{proposition}[Search]
    The usual algorithm yields time in $\O(depth of the node)$ for a successful search and $\O(\max \left(\textmd{depth}(v^{-}), \textmd{depth}(v^{+})\right))$ where $\textmd{key}(v^{-})$ is the predecessor of the searched key.
\end{proposition}

\begin{proposition}[Insertion]
    The standard BST algorithm with added rotations to fix the heap properties (if $p(z) \leq p(z.p)$, rotate auround $(z, z.p)$) yields time complexity in $\O(\max\left(\textmd{depth}(v^{-}), \textmd{depth}(v^{+})\right))$.
\end{proposition}

\begin{proposition}[Deletion]
    We run the insertion algorithm backwards : let $\textmd{light}(z)$ be the child of $z$ with smaller priority. The choice of this edge preserves the heap property everywhere except at $z$. As long as $z$ is not a leaft, rotate around $(z, \textmd{ligth}(z))$, then chop it off.
\end{proposition}

\begin{proposition}[Split]
    If we want to split the tree along a pivot $\pi$, we can insert a new node $z$ with key $\pi$ and priority $-\infty$. After the insertion, this new node is the root since it has the smallest priority, and all keys in the left subtree are smaller than the pivot and this subtree is a treap.
\end{proposition}

\begin{proposition}[Merge]
    Suppose that we want to merge two treaps $T_{-}$ and $T_{+}$, where keys in the first tree are smaller than the keys in the second tree. We create a dummy root with priority $-\infty$ and then delete it. 
\end{proposition}

\begin{proposition}[Complexity]
    \begin{itemize}
        \item Time for a successful search : $\O(\textmd{depth(v)})$ where $\textmd{key}(v) = k$.
        \item Time for an unsuccessful search : $\O(\max \left(\textmd{depth}(v^{-}), \textmd{depth}(v^{+})\right))$ where $\textmd{key}(v^{-})$ is the predecessor of the searched key.
        \item Insertion Time : $\O(\max \left(\textmd{depth}(v^{-}), \textmd{depth}(v^{+})\right))$ where $\textmd{key}(v^{-})$ is the predecessor of the searched key.
        \item Deletion Time : $\O(\text{tree depth})$
        \item Split/Merge Time : same as insertion / deletion
    \end{itemize}
\end{proposition}

We will choose the priorities to be independently and uniformly distributed continuous random variables. We denote by $x_{1}, \ldots, x_{n}$ the nodes of the tree in the increasing order of the keys.
We denote  by $i \uparrow k$, the event $x_{i}$ if a proper ancestor of $x_{k}$ : $\textmd{depth}(x_{k}) = \sum_{i = 1}^{i = n} \left[i \uparrow k\right]$

\begin{lemma}
    Define \[
        X(i, k) = \left\{\begin{aligned} &x_{i},\ldots, x_{k} &\text{ if } i < k \\
            & x_{k},\ldots, x_{i} & \text{ otherwise}.
        \end{aligned}\right.\]

    For all $i \neq k$ we have $i \uparrow k$ iff $x_{i}$ has the smallest priority in $X(i, k)$.
\end{lemma}
\begin{proof}
    \begin{enumerate}
        \item If $x_{i}$ is the root, then $x_{i}$ has the smallest priority
        \item If $x_{k}$ is the root, then $x_{i}$ is not an ancestor of $x_{k}$ and does not have the smallest priority
        \item If $x_{j}$ is the root with $i < j < k$, then $x_{i}$ is not an ancestor of $x_{k}$ and does not have the smallest priority.
        \item If $x_{j}$ is the root and either $i < k < j$ or $i < j < k$ then $x_{i}$ and $x_{j}$ are in the same subtree and the claim follow by induction.
    \end{enumerate}
\end{proof}

\begin{corollary}
    \[\mathbb{P}\left[i \uparrow k\right] = 
    \left\{
        \begin{aligned}
            \frac{1}{k - i + 1} &\text{ if } i < k\\
            0 & \text{ if } i = k\\
            \frac{1}{i - k + 1} & \text{ if } i > k
        \end{aligned}
    \right.
    \]
    Then : 
    \[
        \mathbb{E}\left[\textmd{depth}(x_{k})\right] = \sum_{i = 1}^{i = n}\mathbb{P}\left[i \uparrow k\right] = \sum_{i = 1}^{k - 1}\frac{1}{k - i + 1} + \sum_{k+1}^{n}\frac{1}{i - k + 1} = H_{k} - 1 + H_{n - k + 1} - 1 < 2 \ln n - 2
    \]
    Thus, all treap operations take $\O(\log n)$ time in expectation
\end{corollary}

\begin{remark}
    The fastest sorting algorithm is Quicksort, and it can be seen as inputing all the values in the list in a treap then taking the infix order of the treap. 
\end{remark}

\begin{remark}
    Tango trees are $\O(\log \log n)$-competitive and Splay trees are conjecture to be fastest in $\O(1)$-competitive, but we do not know if this is true. 
\end{remark}

\section{Lower Bound for Sorting}
The question here is : can we sort in $\O(n\log n)$ or better ?
If comparisons only are allowed then no : indeed, any comparison-based sorting program can be thought of as defining a decision tree of possible executions. Running the same program twice on the same permutation causes it to do the exact same thing bu running it on different permutations of the same data causes a different sequence of comparisons to be made on each. 

\subsection{Decision Tree}
\begin{definition}
    A decision tree for a sorting algorithm is a binary tree that shows the possible executions of an algorithm on a set.
\end{definition}

\begin{proposition}
    The minimum height of a decision tree is the worst-case complexity of comparison-based sorting. 
\end{proposition}
\begin{lemma}
    The height of any decision tree is $\Omega(n \log n)$
\end{lemma}
\begin{proof}
    Since any two different permutations of $n$ elements require a different sequence of steps to sort, there must be at least $n!$ different paths from the root to leaves in the decision tree. 
    Thus there must be at least $n!$ different leaves in this binary tree. Since a binary tree of height $h$ has at most $2^{h}$ leaves, we know that : $n! \leq 2^{h}$ i.e. $h\geq \log(n!)$. Finally, we have $\log n! = \Omega (n \log n)$.
\end{proof}
\begin{theorem}
    Any comparison-based sorting algorithm must use $\Omega(n\log n)$ time.
\end{theorem}

\section{Predecessor Problem}
We want to maintain a set $S$ of integers from a universe $U = \left[1, u\right]$ subject to insertions, deletions, predecessor and succesor queries. This problem is harder than dictionaries and hashing. BTSs give a solution in $\O(n)$ space and $\Theta(\log n)$ time. 

\subsection{van Emde Boas Trees}
If the time per operation satisfies : $T(u) = T(\sqrt{u}) + \O(1)$, by Substitution, $T(u) = \O(\log \log u)$. We will split $U$ into $\sqrt{u}$ chunks of size $\sqrt{u}$ size, and the recurse. 
\begin{definition}[Recursive van Emde Boas Trees]
    \begin{itemize}
        \item \textmd{T.summary} is a vEB-tree of size $\sqrt{u}$ containing all $i$ such that the $i$-th chunk is not empty. 
        \item For each $1 \leq i \leq \sqrt{u}$, \textmd{T.chunk[$i$]} is a vEB-tree of size $\sqrt{u}$ containing $x \textmd{mod} \sqrt{u}$ for each $x$ in the $i$-th tree. 
        \item \textmd{T.min} is the smallest element in $T$, not stored recursively. 
    \end{itemize}    
    We represent each integer $x = \langle c, i \rangle$ where 
    \begin{itemize}
        \item $c$ is the chunk coordinates : $c = x // \sqrt{u}$
        \item $i$ is the position of $x$ in the chunk : $i = x \textmd{mod} \sqrt{u}$
    \end{itemize}
\end{definition}

\begin{proposition}[Successor Operation]
    With the following algorithm we get the wanted complexity : 
    \begin{algorithm}
        \caption{Successor Complexity Verifies : $T(u) = T(\sqrt{u}) + \O(1)$}
        \begin{algorithmic}
            \Function \textmd{Successor}
                \Input $\ \ (T, x = \langle c, i \rangle)$
                \EndInput
                \If {$x < T.min$} 
                    \State \Return $T.min$
                \EndIf
                \If {$i < T.chunk[c].max$}
                    \State \Return $\langle c, \textmd{Successor}\left(T.chunk[c], i\right) \rangle$
                \Else 
                    \State $c^{'} = \textmd{Successor}(T.summary, c)$
                    \State \Return $\langle c^{'}, T.chunk[c^{'}].min\rangle$
                \EndIf
            \EndFunction
        \end{algorithmic}
    \end{algorithm}
\end{proposition}

\begin{proposition}[Insertion]
    The algorithm below gets the correct complexity.
    \begin{algorithm}
        \caption{Insertion Complexity Verifies : $T(u) = T(\sqrt{u}) + \O(1)$}
        \begin{algorithmic}
            \Function \textmd{Insert}
                \Input $\ \ (T, x = \langle c, i \rangle)$
                \EndInput
                \If {$T.min = None$} 
                    \State $T.min = T.max = x$
                    \State \Return
                \EndIf
                \If {$x < T.min$}
                    \State $\textmd{swap}(x, T.min)$
                \EndIf
                \If {$x > T.max$}
                    \State $T.max = x$
                \EndIf
                \If {$T.chunk[c].min = None$}
                    \State $\textmd{Insert}(T.summary, c)$
                \EndIf
                \State $\textmd{Insert}(T.chunk[c], i)$
            \EndFunction
        \end{algorithmic}
    \end{algorithm}    
\end{proposition}

\begin{proposition}[Deletion]
    The algorithm below gets the correct complextiy.
    \begin{algorithm}
        \caption{Deletion Complexity Verifies : $T(u) = T(\sqrt{u}) + \O(1)$}
        \begin{algorithmic}
            \Function \textmd{Delete}
                \Input $\ \ (T, x = \langle c, i \rangle)$
                \EndInput
                \If {$x = T.min$} 
                    \State $c = T.summary.min$
                    \If {c = None}
                        \State $T.min = None$. \Return
                    \EndIf
                    \State $x = T.min = \langle c, i = T.chunk[c].min \rangle$
                \EndIf
                \State $\textmd{Delete}(T.chunk[c], i)$
                \If {$T.chunk[c].min = None$}
                    \State $\textmd{Delete}(T.summary, c)$
                \EndIf
                \If {$T.summary.min = None$}
                    \State $T.max = T.min$
                \Else 
                    \State {$c^{'} = T.summary.max$}
                    \State {$T.max = \langle c^{'}, T.chunk[c^{'}].max \rangle$}
                \EndIf
            \EndFunction
        \end{algorithmic}
    \end{algorithm}
\end{proposition}

\begin{proposition}[Complexity]
    \begin{itemize}
        \item All of these operations' complexities verify $T(u) = T(\sqrt{u}) + \O(1)$ and thus have time complexity $\O(\log \log u)$
        \item Space complexity satifies : $S(u) = \left(\sqrt{u} + 1\right)\dot S(\sqrt{u}) + \O(1)$ and therefore $S(u) = \O(u)$.
    \end{itemize}
\end{proposition}

\begin{proposition}[Original van Emde Boad trees]
    For $\textmd{Successor}(x)$, as the path from the root to $x$ is monotone, binary searching the path to find the lowest 1 gives us either the predecessor or the successor of $x$. By storing all nodes in an array of size $\O(u)$ to allow efficient binary search; a pointer from each node to the maximum and minimum of their subtree; all the elements as a doubly-linked list, we find the successor and the predecessor of $x$ in $\O(\log \log u)$ time.\\
    Update is done in $\O(\log u)$ time, since we only need to update the element-to-root path, in $\Theta(u)$ space
\end{proposition}

\subsection{Improvements}
\subsubsection{x-fast trees}
\begin{definition}
    In an x-fast tree, we store every root-to-green node (nodes representing an element from the set) path, viewed in binary (left = 0, right = 1), via Cuckoo Hashing.
\end{definition}
\begin{proposition}
    Predecessor queries are done in $\O(\log \log u)$ time, updates in $\O(\log u)$ expected amortised time, but this tree only takes $\O(n \log u)$ space.
\end{proposition}
\begin{proof}
    To maintain successor and predecessor, we use the vEB-tree algorithm, giving us the same complexity, and same for updates. Yet, since we only store the root-to-element paths which are of $\log u$ length, we need $\O(n\log u)$ space.
\end{proof}

\subsubsection{y-fast trees}
\begin{definition}
    We maintain elements in groups of size in $\left[\frac{\log u}{4}, 2\log u\right]$. For each group, we build a BST, and we store representatives of the group using an x-fast tree :
    \begin{itemize}
        \item If there are fewer than $\frac{\log u}{2}$ elements, we store them in a single BST.
        \item otherwise, suppose we add/delete an element. If a group becomes too large, we split it in two. If a group becomes too small, we merge it with its neighbour, then split if needed. 
    \end{itemize}
\end{definition}

\begin{proposition}
    Predecessor queries are in $\O(\log \log u)$ time, updates in $\O(\log \log u)$ expected amortised time, since insertion into the x-fast trie happens only once per $\Theta(\log u)$ new elements.
\end{proposition}
\begin{proof}
    This comes directly from the definition and the definition of the x-fast trees. 
\end{proof}

\part[Algorithms on strings]{Lecture 5 : 9/11}
\section{General Definitions}
\begin{definition}
    An alphabet $\Sigma$ is a finite set. Elements of $\Sigma$ are letters, characters or symbols, denoted by small letters. A string (or word), is a finite sequence of letters, denoted by a capital letter. \\
    The length of a string $S = s_{1}s_{2}\ldots s_{n}$ where $s_{i} \in \Sigma$ is denoted by $\abs{S}$ and defined to be equal to $n$.\\
    For $1 \leq i \leq j \leq S$, we say that $S[i, j] = s_{i}\ldots s_{j}$ is a substring of $S$. If $i = 1$, $S[i, j]$ is a prefix of $s$. If $j = \abs{S}$, $S[i, j]$ is a suffix of $S$. 
    $S[i,j]$ is an occurence of a string $X$ in $S$ if $S[i, j] = X$ 
\end{definition}

\section{Pattern Matching}
\begin{definition}
    We define the pattern matching problem as such : Given a string $P$ of length $m$ (pattern), a string $T$ of lenth $n \geq m$ (text), return all occurences of $P$ in $T$ specified by their starting positions.
\end{definition}

\subsection{Naïve Algorithm}
The naïve algorithm consists of looking, for each possible starting position in the text, check if the $m$ following characters form a string equal to the pattern. This algorithm has time complexity $\O(nm)$ and space complexity $\O(n + m)$.
\begin{remark}
    There are more than 80 algorithms for this problem. See S.Faro, T.Lecroq \href{The Exact String Matching Problem}{https://arxiv.org/pdf/1012.2547.pdf}
\end{remark}

\subsection{Knuth-Morris-Pratt Algorithms}
\begin{definition}
    The border of a string $P$ is a proper prefix of $P$ that is equal to a suffix of $P$.\\
    The border array of length $\abs{P}$ is such that $B[i]$ is the maximal length of a border of $P[1, i]$, $0$ if undefined.
\end{definition}

\begin{proposition}\label{lemma1KMP}
    $B$ can be computed in $\O(\abs{P})$ time.
\end{proposition}
\begin{lemma}
    First, note that : 
    \begin{enumerate}
        \item $B[1] = 0$
        \item If $X$ is a border of $P[1, k]$ of length $x$, then $X^{'} = X[1, x - 1]$ is a border of $P[1, k - 1]$.
        \item $B[k] - 1 = B[k - 1]$ if and only if $P[k] = P[B[k - 1] + 1]$.
        \item If $X$ is a border of $X$ and $Y$ is a border of $X$, $Y$ is a border of $S$. 
    \end{enumerate}
    Suppose that we have computed $B[1], \ldots, B[k - 1]$. We will now compute $B[k]$.\\
    By property 3, if $P[k] = P[B[k - 1] + 1]$, then $B[k] = B[k - 1] + 1$.\\
    Else, if $P[k] \neq P[B[k - 1] + 1]$, consider $B^{2}[k - 1] = B[B[k - 1]]$. If $P[k] = P[B^{2}[k - 1] + 1]$, set $B[k] = B^{2}[k - 1] + 1$, else consider $B^{3}[k - 1]$, and so on.\\

    This algorithm is correct and in $\O(m)$.
\end{lemma}
\begin{proof}
    \begin{itemize}
        \item Correctness : In fact, $B[1, k] \in \left\{B[k - 1] + 1, B^{2}[k - 1] + 1, \ldots, 0\right\}$.\\
        From property 2, any border of $P[1, k]$ can be obtained by appending $P[k]$ to some border of $P[1, k - 1]$.\\
        From property 4, $B^{j}[k - 1]$ is the $j$-th longest border of $P[1, k - 1]$.
        \item Time : $\textmd{Time}(B[k]) \leq \abs{B[k] - B[k - 1]}$, indeed, if $B[k] = B^{j}[k - 1] + 1$, we use $j$ steps.
        Observe that : 
        \[
            -1 \leq \sum_{k}\left(B[k] - B[k - 1]\right) = \sum^{-}\left(B[k] - B[k - 1]\right) + \sum^{+}\left(B[k] - B[k - 1]\right) \leq m
        \]
        However, by property $2$, the sum on the positive differences if $\O(m)$. Hence, the total time is in $\O\left(\sum \abs{B[k] - B[k - 1]}\right) = \O(m)$
    \end{itemize}
\end{proof}

% TODO : add match function 
\begin{algorithm}
    \caption{Knuth-Morris-Pratt Algorithm : KMP}
    \begin{algorithmic}
        \Input $\ \ T, P, n, m$
        \EndInput
        \State $B \gets \textmd{BorderArray}(P)$
        \State $L \gets []$
        \State $i \gets 1, \ j\gets 1$
        \While {$i \leq n - m + j$}
            \State $(i, j) \gets \textmd{match}(i, j, m)$
            \If {$j = m + 1$} 
                \State $L \gets (i - m)::L$
            \Else 
                \If {$j = 1$}
                    \State $i \gets i + 1$
                \Else 
                    \State $j \gets B[j - 1] + 1$
                \EndIf
            \EndIf
        \EndWhile
        \Return \ $L$
    \end{algorithmic}
\end{algorithm}

\begin{proposition}
    This algorithm is Correct
\end{proposition}
\begin{lemma}\label{lemma2KMP}
    No occurence of $P$ can start in the shift interval.
\end{lemma}
\begin{proof}
    If $T[x, x + m - 1] = P$, $P[1, j - 1]$ has a border of length $> B[j - 1]$, contradiction.
\end{proof}

\begin{lemma}\label{obs1KMP}
    We have $ P[1, B[j-1]] = T[i + (j - 1 - B[j-1]), i + j - 2]$. \\
    Hence, we do not need to compare the first $B[j - 1]$ letters of the patter with the letters of the text after the shift.
\end{lemma}
\begin{proof}[Of the Correctness]
    This stands from \ref{lemma2KMP} and \ref{obs1KMP}.
\end{proof}

\begin{proposition}
    This is done using $\O(m)$ extra space, in $\O(n + m)$ time.
\end{proposition}
\begin{proof}
    \begin{itemize}
        \item Space : $\O(m)$ to store $B$ + constant
        \item Time : From \ref{lemma1KSP}, $\O(m)$ to build $B$. To process $T$ we need $\O(n + m)$ time, since $1 \leq i \leq n$ and $1 \leq j \leq n$ and their difference increases after any step.
    \end{itemize}
\end{proof}

\begin{definition}
    An algorithm is called online if can process its input item-by-item in a serial fashion, without having the entire input available from the start.\\
    An online algorithm is called real-time if it processes each data item in constant time.\\
    KMP is an online algorithm, but it is not real-time.
\end{definition}

\begin{proposition}
    We can construct a real-time version of KMP, by using $B^{'}[j, T[i]]$ to decide how to shift the pattern, where : 
    \[
        B^{'}[j, a] = \max{\left\{k \mid k < j,\ P[1, k] = P[j - k + 1, j], \ P[k + 1] = a\right\}}
    \]
    $B^{'}$ occupies $\O(m)$ space and is computed in $\O\left(m \cdot \abs{\Sigma}\right)$ time. 
    The modified KMP is real-time and correct.
\end{proposition}

\section{Dictionaries, Multiple Pattern Matching}
\subsection{Tries}
\begin{definition}
    A dictionary $D$ is a set of string. A trie of $D$ is a tree with every node being labeled by a letter such that : 
    \begin{itemize}
        \item For every node, outgoing edges are labeled with different letters
        \item For every string $S \in D$, there is a root-to-node path that spells out $S$. The end of the path is labeled with the id of $S$.
        \item Every root-to-leaf path spells out a string in $D$.
    \end{itemize}
\end{definition}
\begin{proposition}
    A trie for $D$ uses $\O(\sum_{S \in D}\abs{S})$ space.
\end{proposition}
\begin{proof}
    By laying out each root-to-leaf path.
\end{proof}

We will consider two applications : 
\begin{enumerate}
    \item Dictionary Look-Up : Given a string $P$, decide if it belongs to the dictionary.
    \item Multiple Pattern Matching : Given a dictionary of patterns $D$, find all their occurences in a text $T$.
\end{enumerate}

\subsection{Dictionary Look-Up}
\begin{proposition}
    For $P$ a pattern, $P \in D \Leftrightarrow $  there is a root-to-node path that spells out $P$ labeled by an id of a string from $D$. By starting at the root, if there is an from the root to its child $u$ labeled by $P[1]$, the algorithm moves to $u$ and continues recursively. This takes time in $\O(\abs{P})$.
\end{proposition}

\subsection{Aho-Corasick Algorithm}
We will first assume that no pattern in $D$ is a substring of another. 
\begin{definition}
    The failure link from a node $u$ labeled by a string $S$ points to the deepest node $v$ labeled by a proper suffix of $S$.
\end{definition}

\begin{lemma}
    The trie occupies $\O(m)$ space and can be constructed in $\O(m)$ time.
\end{lemma}
\begin{proof}
    \begin{itemize}
        \item Space : There are $\O(m)$ nodes, each node has exactly one failure link.
        \item Time : Failure links are built top-to-down. Links from nodes of depth $1$ point to the root.
        Suppose that we have built links for all nodes to depth $\leq d - 1$. The failure link from a node labeled with $Sa$ must point to a node labeled with $S^{'}a$, where $S^{'}$ is a suffix of $S$. In other words, the parent of the end of the failure link from the node must belong to the failure link path from the node $p(u)$ , and it must be the deepest node that has an outgoing edge labeled with $a$.
        The algorithm works as follows: we follow the failure link path from $p(u)$. The first node with an outgoing edge labeled with $a$ is the end of the failure link for $u$.
        Consider the time needed to build the failure links for one root-to-leaf path. Let $root, u_{1}, \ldots, u_{k}$ be the nodes in this path, and denote by $f(u_{i})$ the depth of the end of the failure link for $u_{i}$. The time to find the link for $u_{i}$ is $\leq c \times \left(2 + f(u_{i-1} - f(u_{i}))\right)$. Thus the total time for the trie is $\O(m)$.
    \end{itemize}
\end{proof}

% TODO : add failure_link function
\begin{algorithm}
    \caption{Aho-Corasick Algorithm}
    \begin{algorithmic}
        \Input \ $T, D, n$
        \EndInput
        \State $curr_node \gets root, i \gets 1, L \gets []$
        \While {$i \leq n$}
            \If {$\exists e = (curr_node, u)$ labeled with $T[i]$}
                \State $curr_node \gets u$
                \If {$curr_node$ corresponds to a pattern}  
                    \State $L \gets i :: L$
                \EndIf
                \State $i \gets i + 1$
            \Else
                \If {$curr_node = root$}
                    \State $i \gets i + 1$
                \Else
                    \State $curr_node = \textmd{failure_link}(curr_node)$
                \EndIf
            \EndIf
        \EndWhile
    \end{algorithmic}
\end{algorithm}

\begin{proposition}
    This algorithm has space complexity $\O(m)$ and time complexity $\O(m + n)$.
\end{proposition}
\begin{proof}
    \begin{itemize}
        \item Space : $m$ is the total length of the patterns
        \item Time : We need $\O(m)$ time to build the trie. Consider how the depth of $curr_node$ changes during the algorithm.
        \begin{itemize}
            \item Every time we do down an edge (happens $\leq n$ times), it increases by $1$. Every time we follow a failure link, it decreases by $\geq 1$.
            \item Therefore, as the depth is always positive, we follow a failure link at most $n$ times.
        \end{itemize}
    \end{itemize}
\end{proof}

\begin{theorem}
    If no pattern is a substring of another, the multiple pattern matching problem can be solved in $\O(m)$ space and $\O(n + m)$ time.
\end{theorem}

\begin{definition}
    When patterns are substrings of others, we add output links : an output link from a node $u$ goes to the nearest node on the failure link path outgoing from $u$ that corresponds to a pattern of the dictionary.
\end{definition}

\begin{proposition}
    Output links can be built in $\O(m)$ time by one top-down traversal of the failure link tree.
\end{proposition}

We modify the algorithm : at each step it follows the path from $curr_node$ and outputs the patterns in the output link path.

\begin{theorem} 
    The multiple pattern matching problem can ve solved in $\O(m)$ space and $\O(n + m + occ)$ where $occ$ is the total number of occurences of the patterns in the text.
\end{theorem}

\section{Suffix Trees and Applications}
\begin{definition}
    A text index is a data structure that represents a text and supports pattern matching queries. The suffix tree is one of them. 

    We define the compact trie for $D$ as the trie for $D$ where nodes with only one child have been replaced by an edge. The suffix tree of a string is the compact trie for the set of the suffixes of that string with \$ appended (\$ must not be in $\Sigma$).
\end{definition}

\subsection{Suffix Trees}
\begin{proposition}
    A suffix tree for a string of length $n$ has $n$ leaves, and at most $2n - 1$ nodes and $2n - 2$ edges. Storing the labels on the edges can take $\Theta\left(\abs{T}^{2}\right)$. To save space we represent each label as two numbers, the left and the right endpoints in $T$.
\end{proposition}
\begin{proof}
    By induction.
\end{proof}

To implement algorithms on the suffix tree efficiently, we need to be able to identify, given a node $u$ and a letter $a$, an edge $u, v$ such that its label starts with $a$. Then, for each node $u$, we store an array $A_{u}$ of size $\abs{\Sigma}$ , with $A_{u}[a]$ being a pointer to the child $v$ of $u$ such that the label of $(u, v)$ starts with $a$.

\begin{proposition}
    This structure takes $\O(\abs{Sigma} \cdot \abs{T})$ space.
\end{proposition}

\subsection{Pattern Matching}
\begin{remark}
    A suffix of $T$ starts with $P$ if and only if it corresponds to a leaf of the suffix tree that belongs to a subtree rooted at the end of the path labeled with $P$.
\end{remark}

To answer pattern matching queries, we start at the root. Then, we follow the path labeled by the letters of $P$, and use arrays $A_{u}$ to find the next edge to follow. If there is no path labeled by $P$, there are no occurences of $P$ in $T$. Otherwise, the starting positions of the occurences of $P$ in $T$ are the starting positions of the suffixes that are in the subtree rooted at the end of the path labeled by $P$. We retrieve the occurences by traversing the subtree depth-first.
\begin{proposition}
    This algorithm has total time $\O(m + occ)$.
\end{proposition}
\begin{proof}
    The arrays allow to decide which edge to follow next in $\O(1)$ time. In total, finding the path requires $\O(m)$ time. If there is a path labeled by $P$, the subtree rooted at its end has $\O(occ)$ leaves, where $occ$ is the number of occurences of $P$ in $T$. As the subtree does not have nodes of degree one (except possibly for the root), its size is $\O(occ)$.
\end{proof}

\subsection{Longest Common Substring}
Given two strings, find the longest substring that occurs both in $T_{1}$ and $T_{2}$. We will denote $n = \abs{T_{1}} + \abs{T_{2}}$.

To find the longest common substring of $T_{1}$ and $T_{2}$, we need the suffix tree containing the suffixes of both string (sometimes called generalised suffix tree). 

\begin{proposition}
    This suffix tree can be built in $\O\left(\abs{T_{1}} + \abs{T_{2}}\right)$.
\end{proposition}

\begin{remark}
    If a string $X$ occurs at position $i$ in $T_{1}$, and at position $j$ in $T_{2}$, then the subtree rooted at the end of the path labeled by $X$ contains the leaf corresponding to $T_{1}[i\ldots]$ and $T_{2}[j \ldots]$.
\end{remark}

By bottom-up traversal, we find the deepest node in the tree such that its subtree contains both leaves corresponding to suffixes of $T_{1}$ and suffixes of $T_{2}$. It is labeled by the longest common substring of $T_{1}$ and $T_{2}$.
\begin{proposition}
    This algorithm has time complexity $\O(\abs{T_{1}} + \abs{T_{2}})$.
\end{proposition}

We can generalise this to any $m$ strings in time $\O(n)$ where $n = \sum_{k = 1}^{m} \abs{T_{k}}$. We will use the following fact : we can preprocess a tree of size $\O(n)$ in time $\O(n)$ to support lowest common ancestor ($LCA$) queries in constant time. (see \href{this}{http://static.aminer.org/pdf/PDF/000/118/469/theoretical_and_practical_improvements_on_the_rmq_problem_with_applications.pdf}) $LCA(u, v)$ must return the lowest node that is an ancestor of both $u$ and $v$.

\begin{openpb}
    We showed that the longest common substring problem for two string can be solved in $\O(n)$ space and $\O(n)$ time. It is also known that the problem can be solved in $\O(s)$ extra space and time 
    \[
        \O\left(\frac{n^{2}\log(n)\log^{*}(n)}{L\cdot s} + n \log n\right)
    \]
    where $L$ is answer.
    What is the optimal trade-off.
\end{openpb}

\part[Disjoint-Set Structure]{Lecture 6 : 23/11}

OUIIIIIIIIIIIIIII UNIOOOOOOOOOON-FIIIIIIIIIIIIIIND

\part[DFS-Matroids-MST]{Lecture 7 : 16/11}  % By Pierre Aboulpierre
\section{Definitions}
All along the course : 
\begin{definition}
    \begin{itemize}
        \item $n$ is the number of vertices
        \item $m$ is the number of edges
        \item An algorithm in $\O(n + m)$ is said to be linear
        \item $K_{n}$ is the complete graph on $n$ vertices
        \item $G = \left(U, V, E\right)$ is a bipartite graph means $U$ and $V$ is the partition.
        \item $K_{a, b}$ denotes the complete bipartite graph with parts of size $a$ and $b$.
        \item $H$ is a subgraph of $G$ if $H$ can be obtained from $G$ by deleting vertices and edges.
        \item $H$ is a induced subgraph of $G$ if $H$ can be obtained from $G$ by deleting vertices.
        \item $H$ is a subdivision of $G$ if $H$ can be obtained  from $G$ by subdividing some edges.
        \item $H$ is a minor of $G$ if $H$ can be obtained from $G$ by deleting vertices, edges and contracting edges.
        \item A sink is a vertex of outdegree 0
    \end{itemize}
\end{definition}
\begin{table*}
    \caption{Some Graph Parameters}
    \begin{tabular*}{cc}
        $\delta(G)$ & minimum degree \\
        $\Delta(G)$ & maximum degree\\
        $\omega(G)$ & clique number\\
        $\alpha(G)$ & size of a maximum independent set\\
        $\chi(G)$ & chromatic number\\
        $\tau(G)$ & vertex cover\\
        $\kappa(G)$ & vertex connectivity\\
        $\lambda(G)$ & edge connectivit\\
        $\mu(G)$ & size of a maximum matching\\
        Girth & length of a shortest cycle \\
    \end{tabular*}
\end{table*}


\begin{definition}[Generic Graph Search]
    The problem is to visit the vertices following the edges of the graph. A search also outputs a search tree, and an ordering on the vertices.
\end{definition}

\section{BFS, DFS and Graph Encoding}
    \subsection{The Algorithms}
        Nan mais flemme à un moment.\\

    \subsection{Properties}
        \begin{definition}
            We will denote by $v.d$ the time at which a vertex is discovered, and $v.f$ the time at which a vertex is finished, i.e. all of its neighbour are discovered.
        \end{definition}
        \begin{theorem}[Parenthesis Theorem]
            In any DFS of a graph, for any two vertices $u$ and $v$ either : 
            \begin{itemize}
                \item $\left[u.d, u.f\right]\cap \left[v.d, v.f\right] = \emptyset$ and $u$ is not a descendant nor an ancestor of $v$.
                \item $\left[u.d, u.f\right] \subset \left[v.d, v.f\right]$ and $u$ is a descendant of $v$
                \item $\left[v.d, v.f\right] \subset \left[u.d, u.f\right]$ and $v$ is a descendant of $u$
            \end{itemize}
        \end{theorem}

        \begin{corollary}
            A vertex $v$ is a descendant of $u$ in the DFS forest if and only if $u.d < v.d < v.f < u.f$.
        \end{corollary}

        \begin{theorem}[White-path Theorem]
            In a DFS forest of a digraph, a vertex $v$ is a descendant of a vertex $u$ if and only if at time $u.d$, there is a $(u, v)$-path made of undiscovered vertices.
        \end{theorem}

        \begin{definition}
            We define arc types in terms of the DFS forest $G_{\pi}$ :
            \begin{itemize}
                \item Tree arcs are arcs of $G_{\pi}$
                \item Forward arcs are arcs $uv$ such that $u$ in an ancestor of $v$
                \item Back arcs are arcs $uv$ such that $u$ is a descendant of $v$
                \item Cross arc are arcs $uv$ such that $u$ is not an ancestor of $v$ and $v$ is not an ancestor of $u$.
            \end{itemize}
        \end{definition}

        \begin{proposition}
            A digraph has a directed cycle if and only (any) DFS produces a back arc.
        \end{proposition}
    
        \subsection{Topological Sort}
        \begin{definition}
            A topological ordering of the vertices of a digraph is a labeling $f$ such that $uv \in E only if f(u) < f(v)$.
        \end{definition}

        \begin{theorem}
            $G$ has a topological ordering if and only if it is a DAG.
        \end{theorem}

        \begin{theorem}
            The problem of finding a topological is solved in linear time using a DFS.
        \end{theorem}

        \begin{lemma}
            A digraph is acyclic if and only if a DFS of the graph yields no back edges.
        \end{lemma}

        \subsection{Strongly Connected Components}
        \begin{definition}
            A strong connected component (scc) of a directed graph $G$ is a maximum set of vertices such that every pair of vertices as a directed path from the first to the second and from the second to the first. \\ We define $G^{SCC}$ the quotient graph for $u \mathcal{R} v \Leftrightarrow u \leadsto v \wedge v \leadsto u$.
        \end{definition}
        
        \begin{proposition}
            $G^{SCC}$ is a DAG.
        \end{proposition}

        \begin{lemma}
            Let $C$, $C^{'}$ be two scc of a digraph. Let $u, v \in C$ and $u^{'}, v^{'} \in C^{'}$. If $u \leads to u^{'}$ then there is no path from $v^{'}$ to $v$.
        \end{lemma}

        \begin{definition}
            We extend the discovery and finishing times to sets of vertices : 
            \begin{itemize}
                \item $d(S) = \min \left\{u.d \mid u \in S\right\}$
                \item $f(S) = \max \left\{u.f \mid u \in S \right\}$
            \end{itemize}
        \end{definition}

        \begin{lemma}
            Let $C$ and $C^{'}$ be two scc and let $u \in C$, $v \in C^{'}$ such that $uv \in E$. Then $f(C) > f(C^{'})$.
        \end{lemma}

        \begin{definition}
            We define $G^{R} = \left(V, E^{R}\right)$ where $E^{R} = \left\{uv \mid vu \in E\right\}$.
        \end{definition}

        \begin{proposition}
            $G^{R}$ has the same scc as $G$
        \end{proposition}

        \begin{algorithm}
            \Caption{Kosaraju's Two Pass Algorithm}
            \begin{algorithmic}
                \State Call $\texttt{DFS}(G$) 
                \State Compute $G^{R}$
                \State Call $\texttt{DFS}(G^{R})$ considering vertices in order of decreasing $u.f$
                \Return Output the vertices of each tree in the DFS forest computed by $\texttt{DFS}(G^{R})$
            \end{algorithmic}
        \end{algorithm}

        \begin{theorem}
            Kosaraju's two pass algorithm computes the scc in linear-time.   
        \end{theorem}
        

        \section{Minimum Weighted Spanning Tree - MWST}
        \subsection{Trees}
        \begin{definition}
            A tree is a connected acyclic graph. A forest is an acyclic graph. A leaf of a tree is a vertex of degree $1$.
        \end{definition}

        \begin{proposition}
            \begin{itemize}
                \item Every tree contains at least two leaves
                \item Every tree $T$ satifies : $\abs{E(T)} = \abs{V(T)} - 1$.
                \item Every subgraph of a tree is a forest.
            \end{itemize}
        \end{proposition}

        \begin{proposition}
            Let $G = (V, E)$ be a graph. The following are equivalent : 
            \begin{itemize}
                \item $G$ is a tree
                \item Any two vertices in $G$ are linked by a unique path
                \item $G$ is connected and $\abs{E} = \abs{V} - 1$
                \item $G$ is acyclic and $\abs{E} = \abs{V} - 1$
                \item $G$ is connected but for every edge $uv$, $G - \{uv\}$ has exactly two connected components, one containg $u$ and the other $v$.
                \item $G$ is acyclic but if any edge is added to $G$, the resulting graph contains a unique cycle going through this added edge.
            \end{itemize}
        \end{proposition}

        \begin{definition}
            $\omega : E \to \R$ is called a weight function. A spanning tree is a subgraph of $G$ that is a tree and contains all vertices of $G$. The weight of a subgraph is the sum of the weights of its edges.
        \end{definition}


        \begin{proposition}[Cut and Paste Technic]
            Let $G = (V, E)$ and let $T$ be a spanning tree of $G$. Let $uv \in E(G) - T$ and let $T_{uv}$ be the unique path linking $u$ and $v$ in $T$. Then for every edge $xy$ of $T_{uv}$ $T\setminus \{xy\} \cup \{uv\}$ is a spanning tree of $T$. 
        \end{proposition}
        \begin{proof}
            Since $T_{uv}$ is in the unique path of $T$ linking $u$ and $v$, removing any edge $xy \in T_{uv}$ from $T$ breaks $T$ into two connected components, one containing $u$ and the other $v$. Adding $uv$ reconnects the two parts and form a new spanning tree $T \setminus \{xy\} \cup \{uv\}$.
        \end{proof}

        \begin{theorem}
            Let $G = \left(V, E, \omega\right)$. A spanning tree $T$ of $G$ is a minimum spanning tree if and only if for every edge $e \in E \setminus T$ :
            \[
                \omega(e) \geq \omega(f) \forall f \in \text{ the unique cycle of } T \cup \{e\}
            \]
        \end{theorem}

        \begin{definition}
            A set of edges $A$ is said to be promising if it can be completed into a minimum spanning tree. At each step, we determine an edge $uv$ that we can add to $A$ without violating this invariant, that is $A \cup \{uv\}$ is still a promising set.
            Such an edge $uv$ is said to be safe with respect to $A$.
        \end{definition}

        The algorithm works as follows : We start with an empty set, and while our set is not a spanning tree, we find a safe edge and add it to the set.
        How to find safe edges ? 

        \begin{definition}
            A cut $\left(S, V - S\right)$ is a partition of $V$. An edge of the cut or crossing edge is an edge with one end in $S$ and the other in $V - S$. A cut respects a set of edges if no edge of the set is a crossing edge of the cut.
        \end{definition}

        \begin{proposition}
            Let $A$ be a promising set of edges. Let $\left(S, V - S\right)$ be a cut respecting $A$ and $uv$ be a lightest crossing edge. Then $uv$ is safe.
        \end{proposition}
        \begin{proof}
            Let $T$ be a MST containing $A$. If $T$ contains $uv$ we are done, so assume it does not. Let $T_{uv}$ be the unique path linking $u$ and $v$ in $T$. Since $uv$ is a crossing edge of $(S, V - S)$, $T_{uv}$ contains an edge $xy$ of $(S, V - S)$. By the cut and paste technique, $T^{'} = (T \setminus \{xy\} ) \cup \{uv\}$ is a spanning tree of $G$. By hypothesis, $uv$ is a lightest edge of $(S, V - S)$, so $\omega(uv) \leq \omega(xy)$ and thus: \[\omega(T^{'}) = \omega(T) + \omega(uv) - \omega(xy) \leq \omega(T)\]
            So $T^{'}$ is a MST containing $uv$ 
        \end{proof}

        \subsection{KRUSKAAAAAAAAL}
        Kruskal algorithm grows a promising forest $A$ : 
        \begin{itemize}
            \item Sort the edges by non-decreasing order of weight
            \item Start with the empty set
            \item Add a minimum weighted edge that does not create a cycle which is equivalent to add a minimum weighted edge that connects two connected components of the growing forest.
        \end{itemize}

        \begin{algorithm}
            \caption{Kruskal}
            \begin{algorithm}
                \Input \ \ $G = \left(V, E, \omega\right)$
                \EndInput
                \State $A \gets \emptyset$
                \For {$u \in V$} 
                    \State $\texttt{Make-Set}(u)$
                \EndFor
                \State $\texttt{Sort}(E)$ \Comment{Sort by non-decreasing order of weight}
                \For {$uv \in E$}
                    \If {$\texttt{Find}(u) \neq \texttt{Find}(v)$} 
                        \State $A \gets A \cup \{uv\}$
                        \State $\texttt{Union}(u, v)$
                    \EndIf
                \EndFor
                \Return \ \ A
            \end{algorithm}
        \end{algorithm}

        \begin{theorem}
            The Kruskal algorithm answers the problem in $\O(m \log(n))$ time
        \end{theorem}

        \begin{proof}
            \begin{itemize}
                \item Complexity : from union-find, we do $\O(m)$ \texttt{Find-Set} and $\O(n)$ \texttt{Union} and \texttt{Make-Set} in $\O(m + n)\alpha(n)^{4}$. As we do one sort, the total complexity is $\O(m\log(n))$.
                \item Correctness : Comes from the safe property, by considering the loop invariant : Prior to each iteration, $A$ is promising. 
            \end{itemize}
        \end{proof}

        \subsection{Prim}
        This algorithm works like Kruskal's, but instead of growing a forest, we grow a tree $A$. At each iteration, we add the lightest edge with exactly one extremity in $V(A)$.
        \begin{definition}
            A Priority Queue is a data structure that maintains a set $S$ of elements, each with an associated value called a key. TIt supports the following : 
            \begin{itemize}
                \item $\texttt{Insert}(S, x)$
                \item $\texttt{Minimum}(S)$
                \item $\texttt{Extract-Min}(S)$
                \item $\texttt{Decrease-Key}(S, x, k)$ does $x.key \gets k$.
            \end{itemize}
        \end{definition}
        \begin{remark}
            This is for a min-queue, we can also do a max-queue.
        \end{remark}

        We implement priority queues using a heap.
        \begin{definition}
            A max-heap is a complete binary tree where the key of a node is larger than the keys of its children. It can be implemented in an array : $\texttt{Parent}(i) = \lfloor \frac{i}{2}\rfloor, \texttt{Left}(i) = 2 i + 1, \texttt{Right}(i) = 2i + 2$. It supports the same operations as a max-queue.
        \end{definition}

        \begin{theorem}
            Using a binary heap, insertion is done in $\log N$, max-retrieval is done in constant time and max-deletion is done in $\log N$.
        \end{theorem}
        \begin{proof}
            See implementation.
        \end{proof}

        \begin{algorithm}
            \Caption{PRIM}
            \begin{algorithmic}
                \Input \ $G = \left(V, E, \omega\right), r$
                \EndInput
                \For {$v \in V$} 
                    \State $v.\pi \gets \texttt{NIL}$ and $v.key \gets + \infty$
                \EndFor
                \State $r.key \gets 0$
                \State $Q \gets V$
                \While {$Q \neq \emtpyset$}
                    \State $u \gets \texttt{Extract-Min}(Q)$
                    \For $v \in \texttt{Adj}[u]$ 
                        \If $v \in Q \wedge v.key > \omega(uv)$
                            \State $v.key \gets \omega(uv)$
                            \State $v.\pi \gets u$
                        \EndIf
                    \EndFor
                \EndWhile
            \end{algorithmic}
        \end{algorithm}

        \begin{theorem}
            This algorithm solves the problem in $\O(m + n\log n)$.
        \end{theorem}

        \begin{proof}
            \begin{itemize}
                \item Correctness comes from the safe property
                \item Complexity is in $\O(m\log n)$ with a min-heap and $\O(m + n\log n)$ with a Fibonacci Heap
            \end{itemize}
        \end{proof}

        \section{Matroids and Greedy Algorithms}
        \subsection{Definitions}
        \begin{definition}
            Let $E$ be a set of elements and $\mathcal{I} \subseteq 2^{E}$. $\left(E, \mathcal{I}\right)$ is a hereditary set system if it satisfies
            \begin{itemize}
                \item M1 : $\emptyset \in \mathcal{I}$
                \item M2 : If $X \subeteq Y \in \mathcal{I}$, $X \in \mathcal{I}$.
            \end{itemize}
            \begin{itemize}
                \item $E$ is called the ground set.
                \item Sets in $\mathcal{I}$ are called independent sets.
                \item Maximal independent sets are called bases.
                \item For $X \subseteq E$, the rank of $X$ denoted by $rk(X)$ is the size of a maximum independent set included in $X$.
                \item Sets not in $\mathcal{I}$ are called dependent sets.
                \item Minimal dependent sets are called circuit.
            \end{itemize}
        \end{definition}

        Given a positive weight function, we want to find a maximum independent set such that its weight is maximum.

        \begin{example}
            \begin{itemize}
                \item MST : $\mathcal{I} = \left\{I \subseteq E \mid I \text{ is a forest } \right\}$
                \item TSP, SPP, Maximum Matching Problem, Maximum Stable Set, Knapsack are also of the sort.
            \end{itemize}
        \end{example}

        \begin{theorem}[Rado - Edmonds]
            The greedy algorithm for this problem is optimal for any weight function if and only if $\left(E, \mathcal{I}\right)$ is a matroid.
        \end{theorem}

        \begin{definition}
            A matroid is a hereditary set system $E, \mathcal{I}$ such that : if $X, Y \in \mathcal{I}$ and $\abs{X} > \abs{Y}$, there is $x \in X \setminus Y$ such that $Y \cup \{x\} \in \mathcal{I}$
        \end{definition}







\end{document}

