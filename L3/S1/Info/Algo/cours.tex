\documentclass{cours}

\newtheorem{openpb}{Open Problem}

\title{Algorithmique}
\author{Pierre Aboulker, Paul Jeanmaire, Tatiana Starikovskaya}
\date{\today}

\begin{document}
\part{Lecture 1 - 28/09}
\localtableofcontents
\section{Organisation}
Mail Tatiana : \url{starikovskaya@di.ens.fr}
Homeworks are 30\% of the final grade, final (theory from lecture)
Textbooks : 
\begin{itemize}
    \item \textsl{Introduction to Algorithms} - Cormen, Leiserson, Riverst, Stein
    \item \textsl{Algorithms on strings, trees, and sequences} - Gusfield
    \item \textsl{Approximation Algorithms} - Vazirani
    \item \textsl{Parametrized Algorithms} - Cygan, Fomin, Kowalik, Lokshtanov, Marx, Pilipczuk, Pilipczuk, Saurabh
\end{itemize}

\section{Introduction}
Algorithm take Inputs and give an output.
\begin{openpb}[Mersenne Prime]
    Find a new prime of form $2^{n} - 1$
\end{openpb}
Algorithms do not depend on the language. Algorithms should be simple, fast to write and efficient.
Word RAM model : Two Parts : one with a constant number of registers of $w$ bits with direct access, and one with any number of registers, only with indirect access (pointers). 
Allows for elementary operations: basic arithmetic and bitwise operations on registers, conditionals, goto, copying registers, halt and malloc. 
To index the memory storing input of size $n$ with $n$ words, we need register length to verify $w \geq \log n$
Algorithms can always be rewritten using only elementary operations. 
Complexity : 
\begin{itemize}
    \item $Space(n)$ is the maximum number of memory words used for input of size $n$
    \item $Time(n)$ is the maximum number of \textsl{elementary} operations used for input of size $n$
\end{itemize}
Complexity Notations : 
\begin{itemize}
    \item $f \in \mathcal{O}(g)$ if $\exists n_0 \in \mathbb{N}, c \in \mathbb{R}_{+},\ f(n) \leq c \cdot g(n), \ \forall n \geq n_0$
    \item $f \in \Omega(g)$ if $\exists n_0 \in \mathbb{N}, c \in \mathbb{R}_{+},\ f(n) \geq c \cdot g(n), \ \forall n \geq n_0$
    \item $f \in \Theta(g)$ if $\exists n_0 \in \mathbb{N}, c_{1}, c_{2} \in \mathbb{R}_{+},\ c_{1} \cdot g(n) \leq f(n) \leq c_{2} \cdot g(n), \ \forall n \geq n_0$
\end{itemize}

\section{Data Structures}
\subsection{Introduction}
Way to store elements of a data base that is created to answer frequently asked queries using pre-processing.
We care about space used, construction, query and update time.
Can be viewed as an algorithm, which analysed on basics.
Containers are basic Data Structures, maintaining the following operations : 
\begin{enumerate}
    \item Random Access : given $i$, access $e_{i}$
    \item Access first/last element
    \item Insert an element anywher
    \item Delete any element
\end{enumerate} 

\subsection{Array}
An array is a pre-allocated contiguous memory area of a \emph{fixed} size. It has random access in $\mathcal{O}(1)$, but doesn't allow insertion nor deletion.

Linear Search : given an integer $x$ return $1$ if $e_{i} = x$ else 0.
\begin{algorithm}
    \caption{Linear Search in an Array. \\ Complexity : Time = $\mathcal{O}(n)$ | Space = $\mathcal{O}(n)$}
    \begin{algorithmic}
        \Input
        $x$
        \EndInput
    \end{algorithmic}
\end{algorithm}

\subsection{Doubly Linked List}
Memory area that does not have to be contiguous and consists of registers containing a value and two pointers to the previous and next elements.
It has random access in $\mathcal{O}(n)$, access/insertion/deletion at head/tail in $\mathcal{O}(1)$.
\begin{algorithm}
    \caption{Insertion in a Doubly Linked List \\ Complexity : $\mathcal{O}(1)$}
    \begin{algorithmic}
        \Input
        $ L, x$
        \EndInput
        \State $x.next \gets L.head$ 
        \If{$L.head \neq NIL$}
            \State $L.head.prev \gets x$
        \EndIf
        \State $L.head \gets x$
        \State $x.prev = Nil$
    \end{algorithmic}
\end{algorithm}


\subsection{Stack and Queue}
Stack : Last-In-First-Out data structure, abstract data structure. Access/insertion/deletion to top in $\mathcal{O}(1)$.

\begin{openpb}[Optimum Stack Generation]
    Given a finite alphabet $\Sigma$ and $X \in \Sigma^{n}$. Find a shortest sequence of stack operations push, pop, emit that prints out $X$. You must start and finish with an empty stack.
    Current best solution is in $\tilde{\mathcal{O}}(n^{2.8603})$.
\end{openpb}

Queue : First-In-First-Out abstract data structure. Access to front, back in $\mathcal{O}(1)$, deletion and insertion at front and back in $\mathcal{O}(1)$.

\section{Approaches to algorithm design}
Solve smalle sub-problems to solve a large one.

\subsection{Dynamic Programming}
Break the problem into many closely related sub-problems, memorize the result of the sub-problems to avoid repeated computation

Examples : 
\begin{algorithm}
    \caption{Recursive Fibonacci Numbers \\ Complexity: Exponential}
    \begin{algorithmic}
        \State RFibo($n$) :
        \Input
        $ n$
        \EndInput
        \If {$n \leq 1$}

            \Return $n$
        \EndIf\\
        \Return RFibo($n-1$) + RFibo($n-2$)
    \end{algorithmic}        
\end{algorithm}

\begin{algorithm}
    \caption{Dynamic Programming Fibonacci Numbers \\ Time = $\mathcal{O}(n)$ | Space = $\mathcal{O}(n)$}
    \begin{algorithmic}
        \Input
        $ n$
        \EndInput
        \State $Tab \gets zeros(n)$ \Comment $zeros(n)$ returns a $n$-array of zeros.
        \State $Tab[0] \gets 0$
        \State $Tab[1] \gets 1$
        \For{$i \gets 2$ to $n$} 
            \State $Tab[i] = Tab[i-1] + Tab[i-2]$
        \EndFor\\
        \Return Tab[n]
    \end{algorithmic}        
\end{algorithm}

Levenshtein Distance between two strings can be computed in $\mathcal{O}(mn)$ instead of exponential time. Based on \url{https://arxiv.org/pdf/1412.0348.pdf}, this is the best one can do. 
RNA folding : retrieving the 3D shape of RNA based on their representation as strings. Currently, we know it is possible to find $\mathcal{O}(n^3)$, in $\tilde{\mathcal{O}}(n^{2.8606})$ and if \emph{SETH} is true, there is no $\mathcal{O}(n^{\omega - \epsilon})$. We know $\omega \in \left[2, 2.3703\right)$

\begin{openpb}
    Is there a better Complexity for RNA folding ? What is the true value of $\omega$ ?
\end{openpb}

Knapsack problem : An optimization problem with bruteforce complexity $\mathcal{O}(2^{n})$.
\begin{algorithm}
    \caption{Knapsack : Dynamic Programming \\
    Time = $\mathcal{O}(nW)$ | Space = $\mathcal{O}(nW)$}
    \begin{algorithmic}
        \Input $\ W, w, v$ \Comment Capacity, weight and values vectors.
        \EndInput

        \State $KP = zeros(n, W)$

        \For {$i \gets 0$ to $n$}  
            \State $KP[i, 0] = 0$
        \EndFor

        \For {$w \gets 0$ to $W$}  
            \State $KP[0, w] = 0$
        \EndFor
    
        \For {$i \gets 0$ to $n$}  
            \For {$w \gets 0$ to $W$}  
                \If {$w < w_{i}$}
                    \State $KP[i, w] \gets KP[i-1, w]$
                \Else
                    \State $KP[i, w] = \max\begin{cases}
                        &KP[i-1, w] \\
                        &KP[i-1, w - w_{i}] + v_{i}
                    \end{cases}$.
                \EndIf
            \EndFor
        \EndFor\\
        \Return $KP[n, W]$
    \end{algorithmic}
\end{algorithm}

\subsection{Greedy Techniques}
Problems solvable with the greedy technique form a subset of those solvable with DP. Problems must have the optimal substrcture property.
Principle : choosing the best at the moment.


Example : The Fractional Knapsack Problem\\
Algorithm : Iteratively select the greatest value-per-weight ratio. \\
\begin{theorem}
    This algorithm returns the best solution, in time $\mathcal{O}(n \log n)$
\end{theorem}
\begin{proof}[By contradiction]
    Suppose we have $\frac{v_{1}}{w_{1}} \geq \ldots \geq \frac{v_{n}}{w_{n}}$. Let $ALG = p = (p_{1}, \ldots, p_{n})$ be the output by the algorithm and $OPT = q = (q_{1}, \ldots, q_{n})$ be optimal.\\
    Assume that $OPT \neq ALG$, let $i$ be the smallesst index such $p_{i} \neq q_{i}$. There is $p_{i} > q_{i}$ by construct. \\
    Thus, there exists $j > i$ such that $p_{j} < q_{j}$. We set $q' = (q_{1}^{'}, \ldots, q_{n}^{'}) = (q_{1}, \ldots, q_{i-1}, q_{i} + \epsilon, q_{i+1}, \ldots, q_{j} - \epsilon\frac{w_{i}}{w_{j}}, \ldots, q_{n})$\\
    $q^{'}$ is a feasible solution : $\sum\limits_{i = 1}^{n} q_{i}^{'} \cdot w_{i} = \sum\limits_{i = 1}^{n} q_{i} w_{i} \leq W$\\
    Yet, $\sum\limits_{i = 1}^{n} q_{i}^{'} \cdot v_{i} > \sum\limits_{i = 1}^{n} q_{i} \cdot v_{i}$, ce qui contredit la 
\end{proof}

\part{TD 1 - 29/09}
\localtableofcontents
\section{Mathematical Complexity}
\subsection{Exercice 1}
\subsubsection{Question 1}
Non : prendre $f = 1$ et $g = exp$.
\subsection{Question 2}
Non, si g = h = f.
\subsubsection{Question 3}
Non : Si on a $f = n, g = n^{2} \in \Omega(f(n)), h = f \in \Theta(f(n))$ alors $g + h \neq \O(f(n))$

\subsection{Exercice 2}
On rappelle la formule de Stirling : \[n! \sim \left(\frac{n^{n}}{e^{n}}\right)\sqrt{2\pi n}\]
Immédiatement, on en déduit la première relation.

On a par ailleurs la seconde égalité en passant au logarithme, fonction continue en $+\infty$

\subsection{Exercice 3}
On rappelle les formules suivantes: 
\[\begin{cases}    
    (n+a)^{b} &= n^{b}(1 + \frac{a}{n})^{b}\\
    (1+\frac{a}{n})^{b} &= 1 + b\frac{a}{n} + o(\frac{a}{n}) \in \left[1; 1+ba\right]
\end{cases}\]
Immédiatement, on a la relation souhaité.

\section{Data Structures}
\subsection{Exercice 4}
Il suffit de diviser l'array en deux sous arrays de taille $n/2$, une array commençant en $i = 0$, une commençant en $j = -1$ et on stocke les deux indices de fin de la pile courant.

\subsection{Exercice 5}
\subsubsection{Question 1}
On définit un algorithme de $reverse$ de list en temps linéaire en ajoutant tous les éléments dans une pile puis en dépilant dans une liste. On effectue bien $2n = \O(n)$ opérations.\\
Il suffit alors de comparer les deux listes en temps linéaire.

\subsubsection{Question 2}
Pour une liste vide, ou d'un seul élément on renvoie True. 
On reverse en place la première moitié de la liste et on la compare à la seconde et normalement ça marche. 

\subsection{Exercice 6}
On utilise deux piles : On push dans la première, et on pop de la seconde. Lorsque la seconde pile est vide, on pop de la première et on push dans la seconde, ce qui permet bien de former une pile. 

\subsection{Exercice 7}
\subsubsection{Question 1}
On utilise les arrays standards et lorsqu'on dépasse la capacité, on double le nombre de cases, qu'on initialise à -1, en stockant l'indice du dernier élément de la liste. On a alors toujours une complexité en espace en $\O(n)$ puisqu'on a toujours au plus $2n$ cases. \\

\subsubsection{Question 2}
On effectue la suite suivante d'instructions, pour $n \in \N$ : \begin{enumerate}
    \item On ajoute $2n$ éléments
    \item On retire $n+1$ éléments
    \item On ajoute $1$ élément
    \item On recommence en modifiant $n$
\end{enumerate}

\subsubsection{Question 3}
Il suffit alors d'attendre de passer en dessous de la barre de 25\% du tableau rempli. On a bien tout de même une complexité en $\O(n)$.

 \section{Greedy Algorithms}
\subsection{Exercice 8}
\subsubsection{Question 1}
\begin{algorithm}
   \caption{Greedy Algorithm for Scheduling Problem} 
   \begin{algorithmic}
        \Input $\ a$ \Comment Vecteur de tuples correspondant aux activités 
        \EndInput
        \State $E \gets ListeVide()$
        \State $Sort(a, (fun : x, y \mapsto x[1] \leq y[1]))$ \Comment On trie les activités par date de fin croissante
        \State $s \gets PileVide()$
        \State $Push(a, s)$  \Comment On ajoute une à une les activités de $a$ dans une pile.
        \While ($s$)
            \State $ac \gets Pop(s)$
            \If ($ac$ est compatible)
                \State $Ajouter(E, ac)$
            \EndIf
        \EndWhile\\
        \Return $E$
   \end{algorithmic}
\end{algorithm}
\begin{proof}[Correction]
    On introduit une solution optimale, la plus proche possible de l'algo.     
\end{proof}

\subsubsection{Question 2}
On prend $T1 = \left[1, 2\right], T2 = \left[3, 4\right], T3 = \left[1.5, 2.5\right]$

\subsubsection{Question 3}
Bon on fait de la Programmation Dynamique. Relation de récurrence $\forall i DP(i)$ est le max des poids sur $\left\{T_{1}, \ldots, T_{i}\right\}$
\[
\begin{cases}
    DP(0) &= 0\\
    DP(i+1) &= \max(DP(i), w_{i+1} + DP(p(i+1))) \text{ où } p(i) \text{ est le dernier indice de la dernière tâche compatible} 
\end{cases}    
\]

\part{Lecture 2 - 5/10}
\localtableofcontents
\section{Divide and Conquer}
Divide a problem into smaller ones to solve those, then combine the solutions to get a solution of the bigger problem.\\
Example : \textsl{Merge Sort} : Its complexity verifies $T(n) = T(\lceil n/2\rceil) + T(\lfloor n/2 \rfloor) + \O(n)$. From that we will derive that $T(n) = \O(n\log n)$

\section{Analysis of Recursive Algorithms}
We have recurrences we want to solve. We have three methods : 


\subsection{Substitution Method}
The method :
\begin{enumerate}
    \item Guess the asymptotic of $T(n)$
    \item Show the answer via induction
\end{enumerate}
For \textsl{Merge Sort}: we guess $T(n) \leq c \cdot n\log_{2}n, \forall n \geq 2$. We choose $c$ that verifies that property until $n = 6$. \\
Substituting in the recurrence equation : \[T(n) \leq cn\log_{2}\frac{n}{2} + c\log_{2}\frac{n}{2} + c\frac{n+2}{2} + a\cdot n = c \dot n\log_{2}n + a \cdot n + c \cdot \log_{2}n - c\frac{n}{2}\]
If we then choose $c$ so that it is bigger than $20a$ we get : 
\[T(n) \leq cn\log_{2}n + a\cdot n - c\cdot n/20 \leq cn\log_{2}n\]

\subsection{Recursion-tree Method}
\begin{enumerate}
    \item Simplify the equation : 
    \begin{itemize}
        \item Delete floors and ceils
        \item Suppose $n$ is of a good form
    \end{itemize}
    \item Draw a tree, rooted with the added term and the recursive calls
    \item Start again, and recursively fill the tree
\end{enumerate}
We get a tree of depth $\log_{k}n$ if $n$ is divided by $k$ in recursive calls. We can now sum the values of the nodes, to get an approximation, and start verifying.

\section{Master Theorem}
\subsection{The Theorem}
\begin{theorem}[Master Theorem]\label{thm:MT}
    If we have recurrence equation $T(n) = aT(n/b) + f(n)$ where $a \geq 1, b > 1$ are integers, $f(n)$ is asymptotically positive. Let $r = \log_{b}a$, we have : 
    \begin{enumerate}
        \item If $f(n) = \O(n^{r-\epsilon})$ for some $\epsilon > 0$, then $T(n) = \Theta(n^r)$
        \item If $f(n) = \Theta(n^{r})$ then $T(n) = \Theta(n^{r}\log n)$
        \item If $f(n) = \Omega(n^{r + \epsilon})$ for some $\epsilon > 0$, and $af(n/b) \leq cf(n)$ for some constant $c < 1$ and all sufficiently large $n$ (regularity condition) then $T(n) = \Theta(f(n))$.
    \end{enumerate} 
\end{theorem}
\begin{remark}
    The Master Theorem \ref{thm:MT} does not cover all possible cases for $f(n)$. Example : $f(h) = h^{r}/\log h$
\end{remark}
\begin{remark}
    The Master Theorem \ref{thm:MT} is sometimes called \textsc{Théorème sur les Récurrences de Partition}
\end{remark}

\section{The Proof}
    Plan : \begin{itemize}
        \item Analyse the recurrence as if $T$ is defined over reals (continuous version)
        \item Prove the discrete version
    \end{itemize}

\subsection{Continuous Master Theorem}
\begin{proof}
    \begin{lemma}
        Define $T(n) = \begin{cases}
            \Theta(1) &\text{if } n \leq \hat{n}\\
            aT(n/b) + f(n) &\text{if } n > \hat{n}
        \end{cases}$
        Then \[T(n) = \Theta\left(n^{r}\right) + \sum_{k=0}^{\lceil \log_{b}(n/\hat{n})\rceil - 1}a^{k}f(n/b^{k})\]
    \end{lemma}
    \begin{proof}
        In the Recursion-Tree, stopped when the argument of $T$ is smaller than $\hat{n}$ which is when depth is $\lceil \log_{b}(n/\hat{n})\rceil - 1$, we get : 
        \[
            \begin{aligned}
            T(n) &\leq \sum_{k=0}^{\lceil \log_{b}(n/\hat{n})\rceil - 1}a^{k}f(n/b^{k}) + \Theta(a^{log_{b}(n/\hat{n})})\\ &= \sum_{k=0}^{\lceil \log_{b}(n/\hat{n})\rceil - 1}a^{k}f(n/b^{k}) + \Theta(a^{\log_{b}(n)})\\ &= \sum_{k=0}^{\lceil \log_{b}(n/\hat{n})\rceil - 1}a^{k}f(n/b^{k}) + \Theta(n^{\log_{b}(a)})
            \end{aligned}
        \]
    \end{proof}
    Back to the proof : 
    \begin{lemma}
        Define $g(n) = \Theta(n^{r}) + \sum_{k= 0}^{q} a^{k}f(n/b^{k})$ Then : 
        \begin{enumerate}
            \item If $f(n) = \O(n^{r - \epsilon})$ then $g(n) = \Theta(n^{r})$
            \item If $f(n) = \Theta(n^{r})$ then $g(n) = \Theta(n^{r}\log{n})$
            \item If $f(n) = \Omega(n^{r + \epsilon})$ and we have the regularity condition then $g = \Theta(f)$
        \end{enumerate}
    \end{lemma}
    \begin{proof}
        \begin{enumerate}
            \item Case 1 : \[
            g(n)  
            \begin{aligned}
                &= \Theta(n^{r}) + \sum_{k= 0}^{q} a^{k}f(n/b^{k}) \\ 
                &= \Theta(n^{r}) + \O\left(\sum_{k= 0}^{q} a^{k}(n/b^{k})^{r - \epsilon}\right)\\
            \end{aligned}
            \]
            However : 
            \[
                \begin{aligned}
                    \sum_{k= 0}^{q} a^{k}(n/b^{k})^{r - \epsilon} &= n^{r-\epsilon}\sum_{k = 0}^{q} (ab^{\epsilon}/b^{r})^{k}\\
                    &= n^{r-\epsilon}\sum_{k=0}^{\lceil \log_{b}(n/\hat{n})\rceil - 1}(b^{\epsilon})^{k} = \O(n^{r - \epsilon}(n/\hat{n})^{epsilon})
                \end{aligned}    
            \]
            Thus : $g(n) = \Theta(n^{r})$
            % --------------------------------------------------------------------------
            \item Case 2 : We have : 
            \[
            \begin{aligned}
                g(n) &= \Theta(n^{r}) + \sum_{k=0}^{q} a^{k}f(n/b^{k})\\
                &= \Theta(n^{r}) + \Theta\left(\sum_{k=0}^{q}a^{k}(n/b^{k})^{r}\right)\\
            \end{aligned}    
            \]
            However : 
            \[
              \begin{aligned}
                \sum_{k=0}^{q}a^{k}(n/b^{k})^{r} &= n^{r}\sum_{k = 0}^{q} (a/b^{r})^{k}\\
                &= n^{r}\sum_{k = 0}^{\lceil \log_{b}(n/\hat{n})\rceil - 1}1 = n^{r}\Theta(\log_{b}n/\hat{n})
              \end{aligned}  
            \]
            % ----------------------------------------------------------------------------
            \item Case 3 : By induction on $k$ : $a^{k}f(n/b^{k}) \leq c^{k}f(n)$. Thus : 
            \[
                \sum_{k = 0}^{q}a^{k}f(n/b^{k}) \leq \sum_{k = 0}^{q}c^{k}f(n) = f(n) \sum_{k= 0}^{q}c^{k} = \Theta(f(n))
            \]
        \end{enumerate}
    \end{proof}
    We thus have proved the continuous Master Theorem.
\end{proof}

    \subsection{Discrete Master Theorem}
    We have now showed the continuous Master Theorem, following \textsc{William Kuszmaul, Charles E. Leiserson}, \href{https://epubs.siam.org/doi/pdf/10.1137/1.9781611976496.15}{\textit{Floors and Ceilings in Divide-and-Conquer Recurrences}}, Symposium on Simplicity in Algorithms 2021.\\
        \begin{proof}
            See slides below
            \includepdf[nup = 1x2, pages = 32-45]{Lecture_2.pdf}    
        \end{proof}
        \begin{remark}[Remarks on the Proof]
            \begin{itemize}
                \item Lemmas 1 to 3 serve to show that the argument does not go too far when it is rounded up or down.
                \item Slide 36 Last Line : $\frac{2}{\beta^{i} - 1} < \frac{4}{\beta^{i}}$ for $i \geq i_{0}$. Thus : $\sum_{i = 1}^{\infty} \frac{2}{\beta^{i} - 1} < \sum_{i = 1}^{\infty}\frac{4}{\beta^{i}} + \sum_{i = 0}^{i_0}\frac{2}{\beta^{i} - 1}$ and that last sum is $\O(1)$
                \item Slide 37 Line 3 : The first inequalities comes from the Recursion-Tree, so that we can ensure the argument does not deviate to much, by the second inequalities. 
            \end{itemize}
        \end{remark}
    \subsection{Use Cases}
    Using the Master Theorem we can show the complexity of many algorithms :
    \begin{enumerate}
        \item Merge Sort Complexity : $T(n) = T(\lceil n/2 \rceil) + T(\lfloor n/2 \rfloor) + \O(n) = \Theta(n\log n)$
        \item Strassen's Algorithm for Matrix Multiplication : $T(n) = 7 T(n/2) + \Theta(n^{2}) \Rightarrow T(n) = \O(n^{\log_{2}7}) = \O(n^{2.8074})$
    \end{enumerate}

\section{Fast Multiplication of Polynomials}
The sum of two degree $n$ polynomials can be done in $\O(n)$, Horner's rule for evaluation produces $\O(n)$ complexity. The naïve product can be done in $\O(n^2)$\\
Remembering Lagrange's Theorem on Polynomials (or Vandermonde's Determinant, or anything really), degree $n$ polynomials are entirely represented by their point-value reprensentation over $n$ distinct points $(x_{i}, y_{i})$.
Then, by converting the coefficient reprensentation to point-value representation, then by point-wise multiplicating the polynomials, then by going back to the coefficient representation, we can have a better algorithm.

\subsection{Point-Value Multiplication}
It is easily done in $\O(n)$ if both polynomials are represented over the same axis.

\subsection{Coefficient to Point-Value Conversion - Fast Fourier Transform}
For $P = \sum_{i = 0}^{n-1} a_{i}x^{i}$, we define : \[
  \begin{aligned}
    &P_{odd}(x) &= a_{n-1}x^{n/2-1} + a_{n-3}x^{n/2-3} + \ldots + a_{1}x\\
    &P_{even}(x) &= a_{n-2}x^{n/2-2} + a_{n-4}x^{n/2-4} + \ldots+ a_{2}x^{2/2} + a_{0}
  \end{aligned}  
\]
\begin{enumerate}
    \item We have : $P = xP_{odd}(x^{2}) + P_{even}(x^{2})$
    \item We evaluate $P_{odd}, P_{even}$ at $(\omega_{n}^{i})^{2}$ recursively by the halving property.
    \item We combine the result.
\end{enumerate}

\subsection{Point-Value to Coefficient Conversion - Inverse Fast Fourier Transform}
\begin{theorem}
    $V_{n}^{-1}[i, j] = \omega_{n}^{-ij}/n$
\end{theorem}

\part[Lecture 3 - 12/10]{Hashing}
\localtableofcontents

Answering queries of appartenance on a set, maintaining a dictionary. Python dictionaries do not have upper bound guarantees, we shouldn't use them. \\
Suppose we have a set $S$ over a universe $U$. We dnote by $n$ the number of keys, $m$ the size of the hash table

\section{Naïve Array-based implementation}
Here we assume that no two objects have equal keys. We store $1$ in a bitvector of length $|U|$ at each position $i$ such that $i$ is in $S$.\\
This takes $\O(|U|)$ space, for $\O(1)$ search time, and $\O(1)$ modification time. 

\section{Chained Hash Tables}
We give ourselves a function $h : U \rightarrow \left[1, m\right]$. We send the keys to their image by $h$ in a table. However, since we have no guarantee that $\left|U\right| \leq m$, there might be collisions.\\
To deal with that, instead of storing in the table a boolean, we store a list of all the keys corresponding to $h(k)$. \\
Then we insert a key in constant time $\O(1)$, search a key in time $\O(\left|h[key]\right|)$ and delete a key in time $\O(\left|h[key]\right|)$ since in the worst case the key is at the end of the list.\\

Further analysis leads to the following theorem. 
\begin{theorem}[Simple Uniform Hashing Assumption]
    Assuming SUHA : \og \textit{$h$ equally distributes the keys into the table slots}\fg, and assuming $h(x)$ can be computed in $\O(1)$, $E\left[T_{search}(n)\right] = \O\left(1 + \frac{n}{m}\right)$, and same for deletion time. \\
    Formally, SUHA is : 
    \[
        \begin{aligned}
            \forall y \in \left[1, \abs{T}\right] & \mathbb{P}\left(h(x) = y\right) = \frac{1}{\abs{T}}\\
            \forall y_{1}, y_{2} \in \left[1, \abs{T}\right]^{2} & \mathbb{P}\left(h(x_{1}) = y_{1},\ h(x_{2}) = y_{2}\right) = \frac{1}{\abs{t}^{2}}
        \end{aligned}    
    \]
\end{theorem}
\begin{proof}
    \begin{enumerate}
        \item Unsuccessful Search : Suppose that $k_{0}, \ldots, k_{n-1}$ are keys in the dictionary and we perform an unsuccessful search for a key $k$.
        The number of comparisons is : $\sum_{i = 0}^{n-1} \mathds{1}_{h(k) = h(k_{i})}$. Then, the expected time is : $\mathbb{E}[T_{search}] = \frac{n}{\abs{T}}$ by SUHA.
        \item Successful Search : Suppose keys were introduced in order $k_{0}, \ldots, k_{n-1}$. $k_{i}$ appears before any of $k_{0}, \ldots, k_{i-1}$ and after any of $k_{i+1}, \ldots, k_{n-1}$ that are in the same linked list. Then, to search for $k_{i}$, we need $\sum_{j = i + 1}^{n- 1} \mathds{1}_{h(k_{j}) = h(k_{i})}$. Under SUHA, the expectation of each of these variables is $\frac{1}{\abs{T}}$. Then, the average expected search time is : $\frac{1}{n}\sum_{i = 0}^{n-1} = 1 + \frac{1}{n}\sum_{i = 0}^{n-1}\frac{(n-1-i)}{\abs{T}} = \O(1 + \frac{n}{\abs{T}})$
    \end{enumerate}
    This concludes the proof of the theorem. 
\end{proof}

Good hash functions are functions that distribute the keys evenly. Yet, we do not know what the keys are, and thus will need various heuristics to answer this question. At least, without loss of generality, we can assume that keys are integers.

\subsection{Heuristic Hash Functions}
    \begin{itemize}
        \item Division Method : $h(k) = k \mod m$. It is better to choose $m$ to be a prime number, and avoid $m = 2^{p}$.
        \item Multiplication Method : $h(k) = \left\lfloor m \left\{k\cdot A\right\}\right\rfloor$. $A \in (0, 1)$ and $m = 2^{p}$.
    \end{itemize}
Yet, fixing the function can allow anyone to construct a probability distribution for which the function will be "bad".

\subsection{Universal Family of Hash Functions}
$H = \left\{h : U \rightarrow \left[0, \abs{T}-1\right]\right\}$ is Universal if :
\[
    \forall k_{1} \neq k_{2} \in U, \ \abs{\left\{h \in H \mid h(k_{1}) = h(k_{2})\right\}} \leq \frac{\abs{H}}{m}
\]
\begin{theorem}
    If $h$ is a hash function chosen uniformly at random from a universal family of hash functions. Suppose that $h(k)$ can be computed in constant time and there are at most $n$ keys. Then the expected search time is $\O(1 + \frac{n}{\abs{T}})$
\end{theorem}
\begin{proof}
    Same as the case when $h$ satisfies SUHA. Observe that the probability comes this time from choosing the function. 
\end{proof}

\begin{theorem}
    Let $p \in \mathcal{P}$ such that $U \subseteq \left[0, p-1\right]$. Then $H = \left\{h_{a, b}(k) = \left(\left(ak + b\right) \mod p\right) \mod \abs{T} \mid a \in \Z_{p}^{*}, \ b \in \Z_{p} \right\}$ is a universal family.
\end{theorem}
\begin{proof}
    Let $k_{1} \neq k_{2} \in U$. Let $l_{i} = (ak_{i} + b) \mod p$. We have $l_{1} \neq l_{2}$ and $ a = ((l_{1} - l_{2})((k_{1}-k_{2})^{-1} \mod p) \mod p) \text{ and } b = (l_{1} - ak_{1}) \mod p$. There is then one-to-one mapping between $(a, b)$ and $(l_{1}, l_{2})$. The number of $h \in H$ such that $h(k_{1}) = h(k_{2})$ is :
    \[
        \abs{\left\{(l_{1}, l_{2})\mid l_{1} \neq l_{2} \in \Z_{p}, \ l_{1} = l_{2} \mod m \right\}} \leq \frac{p(p-1)}{\abs{T}} \leq \frac{\abs{H}}{\abs{T}}
    \]
\end{proof}


\section{Open Addressing}
Elements are stored in the table. To insert, we probe the hash table until we find $x$ or an empty slot. If we find an empty slot, insert $x$ here. To define which slots to probe, we use a hash function that depends on the key and the probe number. To search, we probe the hash table until we either find $x$ (return YES) or an empty slot (return NO). In the analysis, we will assume $h$ to be uniform. 

\begin{theorem}[Analysis]
    Given an open-address hash-table with load factor $\alpha = \frac{n}{\abs{T}} < 1$, the expected number of probes in an unsuccessfulsearch is at most $\frac{1}{1-\alpha}$, assuming uniform hashing. 
\end{theorem}
\begin{proof}
    An unsuccessful search on $x$ means that every probed slot except the last one is occupied and does not contain $x$, and the last one is empty. 
    We define $A_{i}$ the event \og The $i$-th probe occurs and is occupied.\fg. By Bayes's Theorem, we must estimate : 
    \[
        \mathbb{P}[\text{\# of probes} \geq i] = \prod_{k = 1}^{i - 1} \mathbb{P}[A_{k} \mid \bigcap\limits_{j = 1}^{k - 1} A_{j}]
    \]
    But we have : $\mathbb{P}[A_{j}\mid A_{1} \cap \ldots \cap A_{j - 1}] = \frac{n - j + 2}{\abs{T} - j + 2}$
    So we have : $\mathbb{P}[\text{\# of probes} \geq i] \leq \frac{n}{\abs{T}}^{i - 1} = \alpha^{i - 1}$
    Then, the expected number of probes is : 
    \[
        \sum_{i = 1}^{+\infty} \mathbb{P}[\text{\# of probes} \geq i] \leq \sum_{i = 1}^{+\infty} \alpha^{i - 1} = \frac{1}{1 - \alpha}
    \]
\end{proof}
\begin{corollary}
    The expected number of probes during insertion is at most $\frac{1}{1 - \alpha}$.
\end{corollary}
\begin{proof}
    If we insert $x$ we first ran an unsuccessful search for it
\end{proof}

\begin{theorem}
    The expected number of probes during a Successful search is at most $\frac{1}{\alpha}\ln\frac{1}{1- \alpha}$.
\end{theorem}
\begin{proof}
    A successful search for $x$ probes the same sequence of slots as insertion.\\
    If $x$ is the $i$-th element inserted into the table, insertion probes less than $\frac{1}{1 - \frac{i}{\abs{T}}}$ slots in expectation.
    Therefore, the expected time of a successful search is at most : 
    \[
        \frac{1}{n}\sum_{i = 0}^{n - 1}\frac{\abs{T}}{\abs{T} - i} = \frac{\abs{T}}{n}\sum_{i = 0}^{n - 1}\frac{1}{\abs{T} - i} = \sum_{i = 0}^{\abs{T}}\frac{1}{i} - \sum_{i = 0}^{\abs{T}-n} \frac{1}{i} \leq \frac{\abs{T}}{n}\ln\frac{\abs{T}}{\abs{T} - n} = \frac{1}{\alpha}\ln\frac{1}{1 - \alpha}
    \]
\end{proof}

This is hard to implement, so we will use heuristics.

\subsection{Heuristic hash functions}
Let $h^{'}, h^{''}$ be two auxiliary hash functions. 
\begin{itemize}
    \item Linear Probing : $h(k, i) = (h^{'}(k) + i) \mod \abs{T}$
    This is easy to implement but it suffers from clustering. 
    \item Quadratic Probing : $h(k, i) = (h^{'}(k) + c_{1}i + c_{2}i^{2}) \mod \abs{T}$
    We must choose the constants $c_{1}$ and $c_{2}$ carefully, and this still suffers from clustering. 
    \item Double Hashing : $h(k, i) = (h^{'}(k) + i h^{''}(k)) \mod \abs{T}$
    To use the whole table, $h^{''}(k)$ must be relatively prime to $m$, e.g. $h^{''}(k)$ is always odd, $m = 2^{i}$.
\end{itemize}

\section{Cuckoo Hashing}
Hashing scheme with search time constant in the worst case, as it maintains a hash function without collisions to achieve perfect hashing. This is possible if the set of keys is static.
Assume that we have two hash functions $h_{1}, h_{2}$ that satisfy SUHA. We store $x$ in either $T[h_{1}(x)]$ or $T[h_{2}(x)]$. Search for $x$ is done in constant time.

\begin{algorithmic}
    \Function{Insert}x
    \If {$ x = T[h_{1}(x)] \text{ or } x = T[h_{2}(x)]$}
        \Return
    \EndIf

    \State $pos \gets h_{1}(x)$
    \For {$i \gets 0 \text{ to } n$}
        \If $T[pos] = Null$
            \State $T[pos] = x$; \Return
        \EndIf

        \State $x \longleftrightarrow T[pos]$
        \If {$pos = h_{1}(x)$} 
            \State $pos \gets h_{2}(x)$
        \Else 
            \State $pos \gets h_{1}(x)$
        \EndIf
    \EndFor
    \State \textsc{Rehash}
    \State \textsc{Insert}(x)
    \EndFunction
\end{algorithmic}
\begin{theorem}
    This insertion is done in constant time.
\end{theorem}
\begin{lemma}
    Suppose that $\abs{T} \geq c \cdot n$ for some $c > 1$. For any $i, j$, the probability that there exists a path from $i$ to $j$ of length $l \geq 1$ which is a shortest path from $i$ to $j$ is at most $\frac{1}{c^{l}\cdot \abs{T}}$
\end{lemma}
\begin{proof}
    By induction on $l$ :
    \begin{itemize}
        \item Initialization : By SUHA, $\mathbb{P}[h_{1, 2}(x) = y] = \frac{1}{\abs{T}}$. Thus $\mathbb{P}[\text{there is an edge from }i \text{ to } j] = \frac{n}{\abs{T}^{2}} \leq \frac{1}{c\abs{T}}$
        \item Heredity : For $l \geq 1$ there must exist $k$ such that there is a path of length $l - 1$ from $i$ to $k$ and an edge from $k$ to $j$.
    \end{itemize}
\end{proof}
\begin{proof}
    We define the bucket of $x$ as all the cells that can be reached either from $h_{1}(x)$ or $h_{2}(x)$. $x, y$ are in the same bucket if and only if there is a path from $\left\{h_{1}(x), h_{2}(x)\right\}$ to $\left\{h_{1}(y), h_{2}(y)\right\}$. Then :
\[
    \mathbb{P}[x, y \text{ are in the same bucket}] \leq 4 \sum_{l = 1}^{\infty}\frac{1}{c^{l}\abs{T}} = \frac{4}{(c - 1)\abs{T}}
\]
So : 
\[
    \mathbb{E}[\abs{\text{bucket of } x}] = \mathbb{E}\sum X_{x, y} = \sum \mathbb{E}[X_{x, y}] = \sum \mathbb{P}[x, y \text{ are in the same bucket}] \leq \frac{4}{c - 1}
\]    
Hence, in the absence of rehash, expected insertion time in constant.\\

The probability that we need a rehash is at most probability that there is a cycle, i.e. a path from $i$ to $i$ : $\frac{4}{c - 1} \leq \frac{1}{2}$.
The probability that we will need $d$ rehashes is then at most $\frac{1}{2^{d}}$.
Thus, the expected time per insertion is : 
\[
    \frac{1}{n}\cdot\O(n)\sum_{d = 1}^{+\infty}\frac{1}{2^{d}} = \O(1)
\]
\end{proof}

\subsection{Rolling Hash Functions}
\begin{definition}
    The Karp-Rabin fingerprint of a string $S = s_{1}\ldots s_{m}$ is : 
    \[
        \phi(s_{1}\ldots s_{m}) = \sum_{i = 1}^{m}s_{i}r^{m - 1}\mod p    
    \]
    where $p$ is a big enough prime and $r \in \mathbb{F}_{p}$.
\end{definition}
\begin{proposition}
    \begin{itemize}
        \item If S = T, then $\phi(S) = \phi(T)$
        \item Else, $\phi(S) \neq \phi(T)$ with high probability.
    \end{itemize}
\end{proposition}
\begin{proof}
    Let $\sigma$ be the size of the alphabet, $p \geq max \left\{\sigma, n^{c}\right\}$ where $c > 1$ is a constant. We have: 
    \[
        \phi(S) = \phi(T) \Leftrightarrow \sum_{i = 1}^{m}(s_{i} - t_{i}) \cdot r^{m - i} \mod p = 0
    \]
    Hence, $r$ is a root of $P(x) = \sum_{i = 1}^{m}(s_{i} - t_{i}) \cdot x^{m-i}$ a polynomial over $\mathbb{F}_{p}$. The probability of such event is at most $\frac{m}{p} \leq \frac{1}{n^{c-1}}$.
\end{proof}
The Karp-Rabin algorithm is as follows : 
\begin{itemize}
    \item Compute the fingerprint of the pattern
    \item Compare it with the fingerprint of each $m$-length substring of the text. If the fingerprint is equal to the fingerprint of a substring, report it as an occurrence
\end{itemize}
\begin{proposition}
    We have : 
    \[
        \phi(s_{1}\ldots s_{j+1}) = \phi(s_{1}\ldots s_{j})\cdot r + s_{j+1} \mod p    
    \]
\end{proposition}
\begin{proof}
    Observe that : 
    \[
    \begin{aligned}
        \phi(s_{1}\ldots s_{j+1}) &= \sum_{i = 1}^{j+1}s_{i}r^{j+1-i} \mod p \\
        \
    \end{aligned}    
    \]
\end{proof}
\end{document}
