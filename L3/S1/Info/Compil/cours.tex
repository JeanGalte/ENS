\documentclass{cours}
\title{Langage de Programmation et Compilation}
\author{Jean-Cristophe Filliâtre}
\date{\today}

\newcommand*{\rrarrow}{\twoheadrightarrow}
\newcommand*{\llarrow}{\twoheadleftarrow}
\newcommand*{\fun}[2]{\textsf{fun } #1 \rightarrow #2}
\newcommand*{\letval}{\textsf{let }}
\newcommand*{\letin}[1]{\textsf{let } #1 \textsf{ in }}

\begin{document}

\part[Aperçu de la Compilation - Assembleur x86-64]{Cours 1 29/09}

\section*{Introduction}
Maîtriser les mécanismes de la compilation, transformation d'un langage dans un autre. Comprendre les aspects des langages de programmation.\\

\subsection{Un Compilateur}
Un compilateur est un traducteur d'un langage source vers un langage cible. Ici le langage cible sera l'asembleur. \\
Tous les langages ne sont pas compilés à l'avance, certains sont interprétés, transpilés puis interprétés, compilés à la volée, transpilés puis compilés...
Un compilateur prend un programme $P$ et le traduit en un programme $Q$ de sorte que : $\forall P, \exists Q, \forall x, \ P(x) = Q(x)$. Un interpréteur effectue un travail simple mais le refait à chaque entrée, et donc est moins efficace.\\
Exemple : le langage \textsl{lilypond} va compiler un code source en fichier .pdf. \\

\subsection{Le Bon et le Mauvais Compilateur}
On juge un compilateur à : \begin{enumerate}
    \item Sa correction
    \item L'efficacité du code qu'il produit
    \item Son efficacité en tant que programme
\end{enumerate}
\begin{quote}
    \og Optimizing compilers are so difficult to get right that we dare say that no optimizing compiler is completely error-free ! Thus, the most important objective in writing a compiler is that it is correct \fg - \textit{Dragon Book, 2006}
\end{quote}

\subsection{Le Travail d'un Compilateur}
Le travail d'un compilateur se compose : 
\begin{itemize}
    \item d'une phase d'analyse qui : 
    \begin{enumerate}
        \item reconnaît le programme à traduire et sa signification
        \item signale les erreurs et peut donc échouer
    \end{enumerate}
    \item d'une phase de synthèse qui : 
    \begin{enumerate}
        \item produit du langage cible 
        \item utilise de nombreux langages intermédiaires
        \item n'échoue pas
    \end{enumerate}
\end{itemize}
Processus : source $\rightarrow$ analyse lexicale $\rightarrow$ suite de lexèmes (tokens) $\rightarrow$ analyse syntaxique $\rightarrow$ Arbre de syntaxe abstraite $\rightarrow$ analyse sémantique $\rightarrow$ syntaxe abstraite + table des symboles $\rightarrow$ production de code $\rightarrow$ langage assembleur $\rightarrow$ assembleur $\rightarrow$ langage machine $\rightarrow$ éditeur de liens $\rightarrow$ exécutable.

\section{L'assembleur}
\subsection{Arithmétique des ordinateurs}
On représente les entiers sur $n$ bits numérotés de droite à gauche. Typiquement, $n$ vaut 8, 16, 32 ou 64. On peut représenter des entiers non signés jusqu'à $2^{n} - 1$. On peut représenter les entiers en définissant $b_{n-1}$ comme un bit de signe, on peut alors représenter $\left[-2^{n-1}, 2^{n-1}-1\right]$. La valeur d'une suite de bits est alors : $-b_{n-1}2^{n-1} + \sum_{k = 0}^{n-2} b_{k} 2^{k}$. On ne peut pas savoir si un entier est signé sans le contexte. \\
La machine fournit des opérations logiques (bit à bit), de décalage (ajout de bits 0 de poids fort, 0 de poids faible ou réplication du bit de signe pour interpréter une division), d'arithmétique (addition, soustraction, multiplication). \\
\subsection{Architecture}
Un ordinateur contient : 
\begin{itemize}
    \item Une unité de calcul (CPU) qui contient un petit nombre de registres et des capactités de calcul
    \item Une mémoire vive (RAM), composée d'un très grand nombre d'octets (8 bits), et des données et des instructions, indifférenciables sans contexte.
\end{itemize}
L'accès à la mémoire coûte cher : à 1B instructions/s, la lumière ne parcourt que 30cm entre deux instructions.\\
En réalité, il y a plusieurs (co)processeurs, des mémoires cache, une virtualisation de la mémoire\ldots\\
Principe d'exécution : un registre (\%rip) contient l'adresse de l'instruction, on lit (fetch) un ou plusieurs octects dans la mémoire, on interprète ces bits (decode), on exécute l'instruction (exectue), on modifie (\%rip) pour l'instruction suivante. En réalité, on a des pipelines qui branchent plusieurs instructions en parallèle, et on essaie de prédire les sauts conditionnels. \\
Deux grandes familles d'Architectures : CISC (complex instruction set), qui permet beaucoup d'instructions différentes mais avec assez peu de registres, et RISC (Reduced Instruction Set) avec peu d'instruction effectuées très régulièrements et avec beaucoup de registres. Ici, on utilisera l'architecture \textit{x86-64}.

\subsection{L'architecture \texttt{x86-64}}
Extension 64 bits d'une famille d'architectures compatibles Intel par AMD adoptée par Intel. Architecture à 16 registres, avec adressage sur 48 bits au moins et de nombreux modes d'adressage. \\
On ne programme pas en langage machine mais en assembleur, langage symbolique avec allocation de données globales, qui est transformé en langage machine par un assembleur qui est en réalité un compilateur. On utilise l'assembleur GNU avec la syntaxe AT\&T (la syntaxe Intel existe aussi).

\subsection{L'assembleur \texttt{x86-64}}
Pour assembler un programme assembleur, appeler \texttt{as -o file.o} puis appeler l'édition de lien avec \texttt{gcc -no-pie file.s -o exec-name}. 
On peut débuguer en ajoutant l'option \texttt{-g}.
La machine est petite boutiste (little-endian) si elle stocke les valeurs dans la RAM en commençant par le bit de poids faible, gros boutiste (big-endian) pour le poids fort. \\
Commandes : Dans cette liste, \%($r$) désigne l'adresse mémoire stockée dans $r$
\begin{itemize}
    \item \texttt{movq \$a \%b} permet de mettre la valeur $a$ dans le registre $b$
    \item \texttt{movq \%a \%b} permet de copier le registre $a$ dans le registre $b$
    \item \texttt{movq \$label \%b} permet de changer l'adresse de l'étiquette dans le registre $b$
    \item \texttt{addq \%a \%b} permet d'additionner les registres $a$ et $b$. 
    \item \texttt{incq \%r} permet d'incrémenter le registre $r$, de même pour \texttt{decq}.
    \item \texttt{negq \%r} permet de modifier la valeur de $r$ en sa négation
    \item \texttt{notq \%r} permet de modifier la valeur de $r$ en sa négation logique.
    \item \texttt{orq \%r1 \%r2} (resp. \texttt{andq} et \texttt{xorq}) permet d'affecter à $r2$, $or(r1, r2)$ (resp. $and, xor$)
    \item \texttt{salq \$n \%r}/\texttt{salq \%cl \%r} décale la valeur de $r$ de $n$ (ou $\%cl$) zéros à gauche. 
    \item \texttt{sarq} est le décalage à droite arithmétique, \texttt{shrq} le décalage à droite logique. 
    \item Le suffixe \texttt{q} désigne une opération sur 64 bits. \texttt{b} désigne 1 octet, \texttt{w} désigne 2 octets, \texttt{l} désigne 4 octets. Il faut préciser les deux extensions si celles-ci diffèrent. 
    \item \texttt{jmp label} permet de jump à une étiquette.
\end{itemize}

La plupart des opérations fositionnent des drapeaux selon leur résultat.\\
Certaines instructions : \texttt{j(suffixe)} (jump), \texttt{set(suffixe)} et \texttt{cmov(suffixe)}(move) permettent de tester des drapeaux et d'effectuer une opération selon leur valeur.

On ne sait pas combien il y a d'instructions en \texttt{x86-64}.

\subsection{Le Défi de la Compilation}
C'est de traduire un programme d'un langage de haut niveau vers ce jeu d'instruction.\\
Constat : les appels de fonctions peuvent être arbitrairement imbriqués et les registres ne suffisent pas $\Rightarrow$ on crée alors une pile car les fonctions procèdent majoritairement selon un mode LIFO.\\
La pile est stockée tout en haut, et croît dans le sens des adresses décroissantes, \texttt{\%rsp} poînte sur le sommet de la pile. Les données dynamiques sont allouées sur le tas, en bas de la zone de données. Chaque programme a l'illusion d'avoir toute la mémoire pour lui tout seul, illusion créée par l'OS. \\
En assembleur on a des facilités d'utilisation de la pile : 
\begin{itemize}
    \item \texttt{pushq \$a} push $a$ dans la pile
    \item \texttt{popq \%rdi} dépile
\end{itemize}
Lorsque $f$ (caller) appelle une fonction $g$ (callee), on ne peut pas juste \texttt{jmp g}. On utilise \texttt{call g} puis une fois que c'est terminé \texttt{ret}.\\
Mézalor tout registre utilisé par $g$ sera perdu par $f$. On s'accorde alors sur des \textbf{conventions d'appel}. Des arguments sont passés dans certains registres, puis sur la pile, la valeur de retour est passéee dans \texttt{\%rax}. 
Certains registres sont \textit{callee-saved} i.e. l'appelé doit les sauvegarder pour qu'elle survivent aux appels. Les autres registres sont dit \textit{caller-saved} et ne vont pas survivre aux appels. \\
Il faut également qu'en entrée de fonction \texttt{\%rsp + 8} doît être multiple de $16$, sinon des fonctions peuvent planter. \\
Il y a quatres temps dans un appel : 
\begin{enumerate}
    \item Pour l'appelant, juste avant l'appel : 
    \item Pour l'appelé, au début de l'appel :
    \begin{enumerate}
        \item Sauvegarde \texttt{\%rbp} puis le positionne.
        \item Alloue son tableau d'activation.
        \item Sauvegarde les registres \textit{callee-saved}.
    \end{enumerate}
    \item Pour l'appelé, à la fin de l'appel : 
    \begin{enumerate}
        \item Placer le résultat dans \texttt{\%rax}
        \item Restaure les registres sauvegardés
        \item Dépile sont tableau d'activation
        \item Exécute \texttt{ret}
    \end{enumerate}
    \item Pour l'appelant, juste après l'appel : 
    \begin{enumerate}
        \item Dépile les éventuels arguments
        \item Restaure les registres \textit{caller-saved}
    \end{enumerate}
\end{enumerate}

\part[Syntaxe abstraite, sémantique, interprètes]{Cours 2: 6/10}
\section*{Introduction}
La signification des programmes est définie souvent de manière informelle, en langue naturelle, e.g. le langage Java.\\

\section{Sémantique Formelle}
La sémantique formelle caractérise mathématiquement les calculs décrits par un programme. C'est utile pour la réalisation d'outils (interprètes, compilateurs), et nécessaire aux raisonnements sur les programmes.\\

\subsection{Syntaxe Abstraite}
On ne peut pas manipuler un programme en tant qu'object syntaxique, on préfère utiliser la syntaxe abstraite (se déduit lors de la compilation à l'analyse syntaxique et sémantique.). On construit un arbre de syntaxe abstraite pour comprendre.\\
On définit la syntaxe abstraite par une grammaire. En OCamL, on réalise la syntaxe abstraite par des types construits. Il n'y a pas de parenthèses dans la syntaxe abstraite. On appelle sucre syntaxique toute construction de la syntaxe concrète qui n'existe pas dans la syntaxe abstraite.

C'est sur la syntaxe abstraite qu'on va définir la sémantique.

\subsection{Sémantiques Useless}
\subsubsection{Sémantique Axiomatique - Logique de Floyd-Hoare}
Tony Hoare, An axiomatic basis for computer programming, 1969, article le plus cité de l'histoire de l'informatique.\\
On caractérise les programmes par l'intermédiaire des propriétés satsfaites par les variables. On introduit le triplet \{$P$\} $i$ \{$Q$\} qui signifie, si $P$ est vraie avant l'instruction $i$, après, $Q$ sera vraie

\subsubsection{Sémantique Dénotationelle}
A chaque expression $e$, on associe sa définition $\left\lVert e\right\rVert $ qui est un objet représentant le calcul désigné par $e$. On définit cet objet récursivement.\\

\subsubsection{Sémantique Par Traduction}
On définit la sémantique d'un langage en le traduisant vers un langage dont la sémantique est connue. 

\subsection{Sémantique Opérationnelle}
La sémantique opérationnelle décrit l'enchaînement des calculs élémentaires qui mènent de l'expression à son résultat.\\
Il y a deux formes de sémantique opérationelle :
\begin{enumerate}
    \item La sémantique naturelle (big-steps semantics) : $e \twoheadrightarrow v$
    \item La sémantique par réduction (small steps semantics) $e \rightarrow e_{1} \rightarrow e_{2} \rightarrow \ldots \rightarrow v$
\end{enumerate}
On applique ça à Mini-ML (qui est Turing-Complet). 

\subsubsection{Sémantique Opérationnelle à Grand Pas}
On cherche à définir $e \twoheadrightarrow v$. On définit les valeurs parmi les constantes, les primitives non appliquées, les fonctions et les paires. \\
Une relation peut être définie comme la plus petite relation satisfaisant un ensemble d'axiomes notés $\overline{P}$ et des règles d'inférences (implications). On définit \textsf{Pair(n)} par $\overline{Pair(0)}$ et $\frac{Pair(n)}{Pair(n+2)}$. La plus petitre relation qui vérifie ces deux propriétés coïncide avec la propriété \og $n$ est un entier naturel pair \fg.\\
Une dérivation est un arbre dont les noeuds correspondent aux règles et les feuilles aux axiomes (les arbres croissent vers le haut). L'ensemble des dérivations possibles caractérise exactement la plus petite relation satisfaisant les règles d'inférence. \\
\begin{definition}
    On définit les variables libres d'une expression $e$, noté $fv(e)$ par récurrence sur $e$ avec :
    \begin{tabular}{rcl}
        $fv(x)$ &$=$&$\left\{x\right\}$\\
        $fv(c)$ &$=$&$\emptyset$\\
        $fv(op)$ &$=$&$\emptyset$\\
        $fv(\fun{x}{e})$ &$=$&$fv(e) \setminus \left\{x\right\}$\\
        $fv(e_{1} e_{2})$ &$=$&$fv(e_{1}) \cup fv(e_{2})$\\
        $fv((e_{1}, e_{2}))$ &$=$&$fv(e_{1}) \cup fv(e_{2})$\\
        $fv(\letin{x = e_{1}} e_{2})$ &$=$&$fv(e_{1}) \cup \left(fv(e_{2})\setminus \left\{x\right\}\right)$\\
        $fv(x)$ &$=$&$\left\{x\right\}$\\        
    \end{tabular}\\
    On défnit la substitution  de toute occurence libre de $x$ dans $e$ par $v$ définie par : 
    \begin{tabular}{rcl}
        $x[x\leftarrow v]$ &=& $v$\\
        $y[x \leftarrow v]$ &$=$& $y$ si $y\neq x$
    \end{tabular}
\end{definition}
On fait le choix d'une stratégie d'appel par valeur, i.e. l'argument est complètement évalué avant l'appel. \\
On a ici comme axiomes : $\overline{c \twoheadrightarrow c}$, $\overline{op \twoheadrightarrow op}$, $\overline{(\textsf{fun } x \rightarrow e) \twoheadrightarrow (\fun{x}{e})}$
et comme règles d'inférences : 
\[    
    \frac{e_{1} \twoheadrightarrow v_{1} \ \ e_{2} \twoheadrightarrow v_{2}}{(e_{1}, e_{2}) \twoheadrightarrow (v_{1}, v_{2})} \ \ \ \frac{e_{1} \twoheadrightarrow v_{1} \ \ e_{2}[x \leftarrow v_{1}] \twoheadrightarrow v}{let\ x  = e_{1} \textsf{ in } e_{2} \twoheadrightarrow v}
\]
et 
\[
  \frac{e_{1}\rrarrow(\fun{x}{e})\ \ e_{2} \rrarrow v_{2} \ \ e[x \leftarrow v_{2}] \rrarrow v}{e_{1}\ e_{2} \rrarrow v}
\]
On ajoute ensuite des règles pour les primitives, dépendant de la forme de chacune, e.g. : 
\[
  \frac{e_{1} \rrarrow + \ \ e_{2} \rrarrow (n_{1}, n_{2})\ \ n = n_{1} + n_{2}}{e_{1}\ e_{2} \rrarrow n}  
\]

Partant, on peut montrer qu'un programme s'évalue en une valeur en écrivant l'arbre de dérivation de celui-ci. \\
\begin{remark}
    Il existe des expressions sans valeur : $e = 1 2$ par exemple.
\end{remark}

On peut établir une propriété d'une relation définie par un ensemble de règles d'inférence, en raisonnant par induction sur la dérivation. Cela signifie par récurrence structurelle.
\begin{proposition}
    Si $e\twoheadrightarrow v$, alors $v$ est valeur. De plus si $e$ est close alors $v$ l'est également.
\end{proposition}
\begin{proof}
    Par induction : $\frac{e_{1} \twoheadrightarrow (\fun{x}{e}) \ \ e_{2} \twoheadrightarrow v_{2} \ \ e[x \leftarrow v_{2}] \twoheadrightarrow v}{e_{1} e_{2} \twoheadrightarrow v}$.
\end{proof}
\begin{proposition}
    Si $e \twoheadrightarrow v$ et $e \twoheadrightarrow v^{'}$, alors $v = v'$.
\end{proposition}
\begin{proof}
    Par induction. 
\end{proof}
\begin{remark}
    On a donc définit une fonction plus qu'une relation. 
\end{remark}

\subsubsection{Sémantique à Petits Pas}
La sémantique opérationnelle à petits pas remédie aux problèmes de programmes qui ne terminent pas, en introduisant une notion d'étape élémentaire de calcul $e_{1} \rightarrow e_{2}$\\
On commence par définir une relation $\rightarrow^{\epsilon}$ correspondant à une réduction en tête, au sommet de l'expression, par exemple : $(\textsf{fun } x\rightarrow e)\ v \rightarrow^{\epsilon} e[x \leftarrow v]$
On se donne également des règles pour les primitives. On réduit en profondeur en introduisant la règle d'inférence : $\frac{e_{1}\rightarrow^{\epsilon} e_{2}}{E(e_{1})\rightarrow E(e_{2})}$ où $E$ est un contexte défini par la grammaire suivante :
\begin{tabular}{crl}
    $E$ &$::=$& $\square$\\
    &|& $E e$\\
    &|& $v E$\\
    &|& $\letin{x = E} e$\\
    &|& $(E, e)$\\
    &|& $(v, E)$\\
\end{tabular}\\
Un Contexte est un terme à trou où $\square$ représente le trou. $E(e)$ dénote le contexte $E$ dans lequel $\square$ a été remplacé par $e$. La règle d'inférence permet donc d'évaluer une sous-expression. Tels que définis, les contextes impliquent ici une évaluation en appel par valeur et de gauche à droite. \\
On note $\rightarrow^{*}$ la cloture réflexive et transitive de $\rightarrow$.
\begin{definition}
    On appelle forme normale toute expression $e$ telle qu'il n'existe pas $e^{'}$ telle que : $e \rightarrow e^{'}$
\end{definition}

\subsubsection{Equivalence des Sémantiques}
\begin{theorem}
    Les deux sémantiques opérationnelles sont équivalentes pour les expressions dont l'évaluation termine sur une valeur i.e. :
    \[
        e \rrarrow v \Leftrightarrow e \rightarrow^{*} v    
    \]    
\end{theorem}
\begin{proof}
    \begin{lemma}[Passage au contexte des réductions]
        Supposons $e \rightarrow e'$, alors :        
        \begin{enumerate}
            \item $e e_{2} \rightarrow e^{'} e_{2}$
            \item $v e \rightarrow v e^{'}$
            \item $\letin{x = e} e_{2} \rightarrow \letin{x = e'} e_{2}$
        \end{enumerate}
    \end{lemma}
    \begin{itemize}
        \item $(\Rightarrow)$ On procède par induction sur la dérivation. 
        \item $(\Leftarrow)$ :
        \begin{lemma}[Evaluation des Valeurs]
           On a $v \rrarrow v$.
        \end{lemma}
        \begin{lemma}
            Si $e \rightarrow e'$ et $e' \rrarrow v$ alors $e \rrarrow v$.
        \end{lemma}
        \begin{proof}
            On commence par les réductions de tête, puis on procède par induction aux applications de contexte. 
        \end{proof}
        On a alors, par récurrence sur le nombre de pas, l'implication souhaitée. 
    \end{itemize}
\end{proof}

\subsubsection{Langages Impératifs}
Pour un langage impératif les sémantiques ci-dessus sont insuffisantes. On associe alors typiquement un état $S$ à l'expression évaluée. L'état peut être décomposé en plusieurs éléments pour modéliser par exemple une pile (des variables locales), des tas... 

\section{Interprète}
    On peut programmer un interprète en suivant les règles de la sémantique naturelle. On se donne un type pour la syntaxe abstraite des expressions et on définit une fonction correspondant à la relation $\twoheadrightarrow$\\
    Un interprète renvoie la (ou les) valeur(s) d'une expression, souvent récursivement.\\
    On peut éviter l'opération de substitution, en interprétant l'expression $e$ à l'aide d'un environnement donnant la valeur courante de chaque variable (un dictionnaire). Ceci pose problème car le résultat de $\textsf{let } x = 1 \textsf{ in fun } y \rightarrow +(x, y)$ est une fonction qui doit \og mémoriser \fg que $x = 1$.\\
    On utilise alors le module \textsf{Map} pour les environnements (c'est une \textit{fermeture}). On représente alors la valeur d'une fonction avec son environnement. \\
    Pour un interprète de la sémantique à petits pas, il vaut mieux utiliser un \textit{zipper} que de recalculer le contexte tout le temps. 



\end{document}