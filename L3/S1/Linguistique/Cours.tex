\documentclass{cours}
\usepackage{enumitem}
\usepackage{qtree}
\usepackage{tipa}



\title{LING 101\! : Introduction to Linguistics}
\author{Salvador Mascarenhas \\ \small Michael Goodale (TA)}
\date{21 Septembre 2023}

\begin{document}
\section[Cours 1\!: 21/09]{Language\! : Psychological and Social Entity}
\subsection*{Introduction}
DEC is called like that because is would have bothered litteraries
Prof\! : Master at NYU, Junior Research at Oxford, here since 2016.
Using language as a window into human mind\\
Linguistics is a broad term for serious principled study of language.
Many perspectives, from the cognitive point or study language by an external perspective (structuralism\! : Ferdinand de Saussure, or Leonard Bloemfield) looking out on structures.
Also quite general here.
Language looked at as a social entity.
History of languages, typology of languages.
Not only about cognitive studies, pretty broad look out.\\
Teaching assistant\! : Michael Goodale PhD student, LRSCP, computational models of Language Acquisitions. Statistical inference and model language after formal tools.
Practical Skills really.\\
Assessments\! : Homework, graded on a qualitative schedule, due on lecture days.
Final\! : Last Lecture, 30\% of grade
10\% of grade in TA participation.
Website\! : Moodle hosted by \href{https://moodle.u-paris.fr/course/view.php?id=7374}{Université de Paris}, \href{https://moodle.u-paris.fr/pluginfile.php/1002151/mod_resource/content/5/intro-ling-syllabus-2023.pdf}{Syllabus}
Syllabus\! :

\subsection*{Schedule}
\begin{enumerate} 
    \item Language as a psychological and social entity
    \item Language (non-)variation\! : Universals, variation within parameters
    \item \textbf{Morphology}, language typology
    \item \textbf{Syntax I} constituent structure; selection and subcategorization
    \item \textbf{Syntax II} subcategorization; X-overline theory
    \item \textbf{Semantics I} first look at meaning \textsl{Studied by Salvador}
    \item \textbf{Semantics II} philosophy of language and the case for methodological solipism.
    \item \textbf{Phonology} phonetic macro classes;
    \item Language and Reasoning
    \item Language and Reasoning II
    \item Language in the brain; deficit-lesion method; functional brain imagining; psycholinguistics; parsing, reading, lexical access.
    \item Language and thoughts in minds vs. machines
\end{enumerate}

\subsection{Remarks and Observations about the Nature of Language}
R.Descartes \textit{"Discourse on the Method"}\! : Humans, everyone of them, can speak. Animals, though they have what is needed, can't express their thoughts.
Insights\! :
\begin{enumerate}
    \item It doesn't matter on general intelligence, social intelligence or any measure of your intellectual abilities. Happens despite any other difficulties.
    \item \emph{To Our Knowledge} Not any other animal can do what we can do. Article from the \textit{NYT}, saying animals \textbf{can} speak, though it's emoverlineassing. A Chasm \textit{appears} between humans and animals. Yet, it is continuous of what happens in the animal kingdom.
    \item Animals are not incapable of language because they can reproduce human language, or use signs to communicate. Studies on Non-Human primate vocalizations, 3-4 words, all alarm calls $\rightarrow$ Language is independent on organs and communication systems. Yet is it a panic reaction or a real communication. Cannot conclude on A.Is then...
\end{enumerate}

\subsection{The Goal of Linguistics}
A complete understanding of how sound (/sign/etc) relates to meaning\! :
\begin{itemize}
    \item in terms of the speaker's knowledge\! : the state their mind is in by virtue of having acquired a natural language (competence). Distinguished from mastery of language/way it is produced. Describing skill, not usage.
    \item in terms of using that knowledge in linguistic tasks, like uncovering meaning from sound in real-time comprehension; executing motor commands necessary to externalize meaning in language production (performance)
\end{itemize}
Competence/performance is not really a sort of Chomsky. Chaz Firestone (Yale) published on competence/performance saying machines have been tested on performance and not competence.
Tight connection between thinking and speaking.
Behaviourism = school of thought that tried to figure out a way of studying humans that postulated and said absolutely about our mind. Not only linguistic behaviour\! : we shouldn't describe what happens in people's heads, just study what outputs comes from what inputs, without looking at the stimuli. Not what we will postulate.
Freud sucks.
Cannot look at functions\! : functions = algorithms, studied based on input/output pairs. Cannot do deduction nor induction but only abduction. Yet, we have no less reasons to believe it is right than to believe black holes exist.

\subsection{Different levels of Study}
Example\! : \textsl{Mushrooms are an edible fungus}
\begin{enumerate}
    \item Sound Categories\! : Studying the sound signal based on the phonemes, represented in the mind.
    \item Morphemes\! : first chucks of phonemes that has a meaning\! : Morphology. Here\! : \textsl{Mushroom} and \textsl{z} or \textsl{edi} and \textsl{able}. Sometimes they are not pronounced\! : need for a rigorous description. Sometimes, they're redundant, and appear with the same meaning in different places\! : compare theories. What is the probability of that happening ? And how about irregularities\! : Past = laugh-\textit{ed} or g\textit{a}ve ? Past in a concept that can manifest itself in different places\! : simple theory. FMRI theory can improve this theory. Morphemes don't always come in the same orthograph nor sound.
    \item Words\! : Not much to do here.
    \item Semantics\! : Organizing words into phrases. Here\! : \textsl{edible fungus} is a phrase, but \textsl{edible} is not. Must be done formally.
\end{enumerate}
Three way of looking\! : Us, looking from a native's judgement - introspection \textbf{will} answer some questions.
Exaggeration, yet\! : no written language, only looking at spokn language.

\subsection{Language and Societies}
\subsubsection{Language and Classes}
Language display depends only on human factors, political relationships, genetic factors\! : distinction between the animal and the meat (names coming from French, spoke by the upper class) in english. Happened in other languages. Different from hyperonyms like \textit{poultry} for \textit{chicken}. Context can explain linguistic aberrations.
\subsubsection{Language and Dialect}
Language and Dialect are political constructs and arbitrary decisions\!: \begin{quote}
    A language is a dialect with an army and a navy. (M.Weinreich)
\end{quote}
\subsubsection{Infinity of Language}
Sentences are of arbitrary length, and can always be augmented. Yet infinite-ish sentences are impossible to comprehend because Performance is finite, i.e. cognitive resources are finite. There is a \textit{finite} system generating infinitely many linguistic representations\! : recursions are of the order.
\subsubsection{Description}
Not looking for rules prescripting language (fuck l'académie française), but only for rules describing them. No better way to speak, the way you ought to speak has nothing to do with linguistics and only with politics. Yet, language are principled, even those which are \textit{proscribed}\! : adding \textit{fucking} in the middle of a word\! : \textit{Phila-fucking-delphia}. Rule here\! : \textit{fucking} comes before the stressed syllable and the material right before needs to be heavy. Heavy comes from phonology, rigorous theory of the weight of syllables.
\subsubsection{Phonological Differences}
Languages have different constraints on the syllables composing their words\! : \textit{*pnick} works in French but not in Engl*sh.
\subsubsection{Internal Structure of Sentences}
Sentences are made of constituents that don't act up the same in every language\! : \textit{des} is not used in Engl*sh ($\sim$ \textit{of the}). They cannot be broken\! : \textit{des burgers et des frites}. It is mysterious tho ? Maybe language has something else to do for us than communicate\ldots\\
They cannot be considered alone\! : \textit{Fat cats eat} and \textit{Fat cats eat accumulates}. Supposition that two words next to each other are related in written language. Also, prosody is a big help in understanding.

\subsection{TA 1 27/09}
\subsubsection{Animal Communication}
Language is also communication, not only hearing (trees ?). Many (if not most) Animals Communicate, and almost all react to sound. (cf. \href{https://www.nytimes.com/2023/09/20/magazine/animal-communication.html?unlocked_article_code=_mKij4e1jtSDj61vUQZVjQCPQ678hO69vto7sqwbRaA3kmyw5b8t-mxMcnihQgvfCJuKaQe1pvift5_AInBSESFgm2rBtU7GDoS_gyv_G6GUnUjV5Wb8L_Cb4YjsG-BFKXy_yO3FYnECOJFaCdmGPS7pCbPH8lqQcH5l4mixJE4IfNzBPeACptp-hnBmdQkb0jkD9qa06NCzE12I22V_m94Uh6YT-76HUyTwGvPuYKgrb0-F-xoAdiItiAZoUDJzWBY2GIujcO8Hw7TiORkZXfc8MRihzb4S7i6_eZR1mWD4-yafAQQP4Ya_hFCSV-wmJKxSHpSSrMFpoK9n4sdL&smid=url-share}{NYT Article})
\! : dolphins communicating by signs, bees dancing, monkeys having muscles/organs to 'talk', bird songs, ant pheromones, great apes...

Differences between human and animal communication ? Human language has\! : composition (recursion\! : meaning of sentence can be derived from its constituents), abstraction, no hypothetical/long term/prevention discussion, intentionality, arbitrary length of sentences, systematic neologisms/nonce words (when learning a word, it is usable immediately), non-instrumental.

Many experiments about teaching great apes language suck and were not really concluent. There is a poor, noisy, contradictive and unrepresenting stimulus that child have to make do with. Deaf child make their own language if they need one. For example of the poverty of the stimulus\! : 2 Layer Embedding of possessive happened 67 times in 120k sentences, while kids at 6 can do 4 level possessive embedding.

The words \textsl{stop, mat, tap, butter} all have \textsl{'t'} yet have different sounds\! : there is a sense where this is the same sound, but another one where they have different sounds.

\section[Cours 2\! : 28/09]{(Non-)Variation and Languages}
\subsection{Different yet Similar}
Langugages are made of signs, composed of a form and a meaning. They only look like they have multiple forms/meanings, on another level of analysis they only have one, e.g. past tense in english and the morpheme \textsl{ed}. Variants depend only and purely on properties of the root, and are entirely predictable. With \textsl{bat}, there is an ambiguity phenomenon, but there is also a phenomenon of polysemy, e.g. \textsl{book}, also, similar meanings often derive from a central point.
\begin{quote}
    A sign presents itself to the senses, and something distinct from itself to the mind  - St-Augustine
\end{quote}
There are two types of signs based on the link between form and meaning\!:
\begin{enumerate}
    \item Those with a causal link\! : \textsl{smoke} implies there is a fire
    \item Those with a arbitrary, conventional link\! : \textsl{black attire} implies mourning (\textit{in some cultures but...})
\end{enumerate}
After Ferdinand de Saussure, \begin{quote}
    Language is a systemn of conventional (arbitrary) signs.
\end{quote}
Example\! : The word \textsl{man} in different languages\! : German Mann, Spanish hombre, Français homme, Hungarian ember, Turkish adam. With the sound produced by the rooster, the words differ in languages, but there is still a partial causal link, because you cannot mimic perfectly the sound of the animal, given the differences between vocal organs.
Arbitrary doesn't mean random\! : it just doesn't matter what choice is made. It is no accident there is a resemblance between German and English words for \textsl{man}

\subsection{Similarities between distant unrelated languages}
\subsubsection{Reciprocal pronouns}
Pronouns marking reciprocity always have a mysterious constraint where they must be in the same, finite clause as their antecedents\! : \textsl{They thought I talked about each other} seems weird.\\
Generally, it seems that reciprocal pronouns must refer to a thing that lies in the same proposition, but why ? \\
First question is\! : Do we have a reason to say why they seem weird, just because they are longer ? No, \textsl{I thought that they talked about each other}, is equally long as \textsl{They thought that I talked about each other} and doens't sound half as bad. \\
Then, the sentence \textsl{They thought we talked about each other} shows it's not about third person. \\
Coming up with a precise answer implies looking for phrases where each bit of the sentence has been replaced, one at a time, to isolate the \textit{issue}.\\
In \textsl{They thought that I talked about each of them}, \textsl{each of them} is not necessary reciprocal, as it only include (\textsl{each other})'s meaning so it is compatible.

\subsubsection{Sentivity to negative elements}
Words occur sometimes with negative elements. Every natural language seems to contain at least one lexical items that is sensitive to whether the context in which they occur exhibits negative or positive polarity.\\
Exemple\! : \textsl{Jean a fait le moindre effort} doesn't seem natural, but \textsl{Jean \emph{n'} a \emph{pas} fait le moindre effort} does.\\
Facts are subtle\! : the mere presence of a negatvive item is enough to license a negative polarity, and sometimes negative polarity is infered without the obvious presence of a negative item. A sentence with an empty slot in place of the negative polarity item is the context that needs to be negative in order to license an NPI. \\
Positive polarity also exists\! : \textsl{John didn't see someone} is really weird, and it requires a really particular prosody and/or context to work. You have a meta interpretation of this. The word anyone, more than a negative item is also a free choice item.

\subsubsection{Relations between Sentences}
In all languages, declaratives and interrogatives are linked by \emph{transformations} ; i.e. a reorganisation of the terms of the declarative to build the interrogative. It creates pairs of assignations, not necessarily questions and answers. Also some declaratives link to multiple questions. \\
There is a finite palette of strategies for these transformations\! : no known human language builds questions by mirroring completely the order of the words. Grammars do not count, e.g. there is no language with transformation swap words 1 and 2.\\

\subsubsection{The Puzzle}
Languages are systems of arbitrary signs. Any sign will do for internal calculation, and any convention widely known within a given community will do for communication. This observation does not predict the existence of strong systematic, pervasive similarities whose speech commuinties have had no contact. \\
How can the conventional character of language be reconciled with pervasive cross-linguistic similarities.
\begin{quote}
    We would learn so much if we could do horrible things to babies. We can do horrible things to animals though, not saying we should, but we can.   - Salvador Mascarenhas
\end{quote}

\subsection{Universal Grammar}
\subsubsection{Chomsky's Hypothesis}
Human faculty for language\! : \begin{itemize}
    \item enables humans to acquire and use langugae;
    \item delimits what linguistic structures humans are capable of acquiring and using
    \item delimits the kind of linguistic convetions that a community of humans can develop and successfully hand down to new generations
\end{itemize}
Thus we need to know all about gene reproduction, biological evolution and so on, to understand it fully \\
Universal Grammar in this sens is section of all humans' biological endowment and it is reflected in all natural languages.

\subsubsection{Universal Grammar}
This is not a language in itself and it doesn't imply the idea all languages come from a common source. This does not imply there is no other factors of relevance shaping actual natural languages than \textsl{a priori} constraints, there being no rhyme or reason to those constraints. Chomsky added it might be to optimize computation. This is \emph{not} a collections of generalizations about trends in linguistic diversity, other projects (Dryer, 2005) have been doing it, especially on word order. \\
It has for purpose to classify the languages the human mind can learn and the \textit{impossible} ones. It is a two steps projects\! : Finding facts, photographing the human languages, then deriving generalizations.

\subsubsection{Language Acquisition}
\paragraph{Critical Period}\label{par:critical-period}
There is a critical period for human (and animal aswell) acquisition of language\! : post-puberty, acquisition is severely impaired, e.g. Jeanie, kid discovered in L.A. in 1970 at 13.5y. Any child can learn any language, it depends on features of the environment.\\
Acquisition doesn't seem to be full related to mathematical/intellectual/reasoning skills and so on... Learning a second/third language is totally different tho, it is correlated to musical and computational skills it seems, there even seems to be a purely linguistic talent. Almost nothing about adult phenological is due to purely biology and genes.

\paragraph{Problem of Induction}
Grammars make predictions for infinitely many word sequences, yet the input if finite. Therefore, there is something not in the input is playing a key role in learning.

\paragraph{Absence of Negative Evidence}
Experiments, can give negative information\!: they can show that under certain conditions, an outcome is \emph{not} observed, but a child has extremely limited access to such negative data.

\paragraph{Conclusion}
A child is far better at figuring their native language than a linguist. Linguistics is an empirical science not a fundamental one. \\
Linguistic data consist of judgements on utterances, grammatically judging strings of characters, truth-value judging sentences/scenarios. It can come from introspection, but introspection is limited\!: there is a limit to what one can infer, e.g. \textsl{I have more pictures of my kid on my phone than my dad ever saw me} isn't grammatical, but it is understandable. The unboundness of human mind might mean we can find sense in any sentence.\\
The better way is to speak with other linguists or conduct experiments on a large amount of naive participants. There is a risk of falling into delusion from thinking to much about the same sentence.

\paragraph{Chomsky's Argument}
There is such a gap between what a child is exposed to and the sophistication of the acquired grammar the child must have expectations as to what a language can be, analog to the concept of triangle\!: You've never seen a triangle but you know what it is. Mathis Hademeyer\\
Universal Grammar is a set of principles with a number of free parameters. \\
Principles and Parameters is a method now limited to small studies on variations, mostly in Italy (just you and me). By this we mean the fact that language, unrelated, share a structure. In fact most languages only occupy a minuscule section of the mathematical space of possibilities for a language. Principles and Parameters is a way to cash out on this idea\! : the problem is figuring what values has a child gotten for its parameters, and how he can deduce from some principles, the structure of language. \\
The null-subject parameter\! : In Catalan, \textsl{he speaks} is said $\emptyset$\textsl{parla}, there is no need for a pronoun, contrary to French or English. How do children hande this situation ? It seems that Italian children go through a phase where they omit subject when learning French.


\subsection{TD 2\!: 04/10}
\subsubsection{Homework 1}
\paragraph{Question 1}
The word \textsl{nous} is used in formal context, so it is expected that people use formal grammar in the whole sentence, hence the \textsl{ne} of negation is expected.

\paragraph{Question 2}
You can drop the copula in AAVE when you can contract the verb.

\subsubsection{Exercise}
\paragraph{Swedish Extrapolation}
Separate the sentences in groups of morphemes, then extrapolate the meanings based on the other sentences.

\paragraph{Rule Extrapolation}
Language is Chikisaw or something.

\section[Cours 3\!: 05/10]{Morphology\! : The Structure of Words}
\subsection{Use of Morphology}
Do you spell kick-ass or kickass ? Is it one or two words ? This is not very well understood.

\subsection{The Study of Morphemes}
Morphology is the study of words, and morphemes. Morphemes are the smallest linguistic unit that makes sense, e.g. \textsl{a} or \textsl{I}.
The sound has no meaning, but the morpheme has. It is the smallest entity with both a form and a meaning.\\
Words are formed by processes, so they have a constituent structure, they can be constituted from multiple morphemes.\\
They can be derived from rules of morphology. Languages have a huge range of variations in what they can do.\\

\subsubsection{Words}
A Word is an indentifyable unit of phonology with a prosody (cf. phonology class).

Words have a structure\! : The \emph{noun} (section of speech) \textsl{reuseables} is made of four morphemes\! : \textsl{re, use, able, s}. Here, the morphemes are, in order, \textsl{a prefix, a root, a suffix, a suffix}. \\
Words can be derived from other words also\! : \textsl{to invite - an invite}. They can then have an ambiguity in sense\! : is \textsl{reusable} a noun or an adjective.\\

\subsubsection{Morpheme Types}
Prefixes, suffixes, infixes (e.g. \textsl{abso-\emph{fucking}-lutely}), circumfixes (e.g. \textsl{\emph{em}-bold-\emph{en}} - not a good example as historically it might have never been a circumfix) together are called affixes. Thos morphemes are called bound morphemes, meaning they cannot appear by themselves (e.g. \textsl{mang} root of \textit{manger} in French)\\
When looking at the derivation, the thing that is not an affix is called the stem. The root is the smallest stem. \\
Things called clitics can also come into words, e.g. \textsl{l'}, it has more information than another morpheme\! : \textsl{l'aime} has two concepts in it.\\
Roots and Stems are called open class morphemes, new instances can easily emerge or be invented, e.g. \textsl{blick-ing} would be easy to understand. Affixes are closed class morphemes, meaning new instances develop slowly.\\

Inflectional morphemes only add grammatical information. They can have the same sound shape as some derivational morpheme. Derivational morphemes on the other hand, create new concepts out of existing ones.\\

There is a universal attested word-structure\! : $\left[\left[\left[\textsf{root}\right]\left[\textsf{derivational affixes}\right]\right]\left[\textsf{inflectional affixes}\right]\right]$

\subsection{Forming Words}
Particular grammars make certain derivational processes available, which we can describe by means of rules.

\subsubsection{Derivational Processes}
A rule specifies the category of the input and the category of the output, see below. Affixes typically add further restrictions on stems.

\begin{table}
    \centering
    \caption{(Simplified) Derivational Rules in English}
    \begin{tabular}{lll}
        \toprule
        Affix     & Rule            & Output      \\
        \midrule
        -able     & Verb + -able    & = Adjective \\
        re-       & re- + Verb      & = Verb      \\
        un$_{1}$- & un- + Adjective & = Adjective \\
        un$_{2}$- & un- + Verb      & = Verb      \\
    \end{tabular}
\end{table}

\subsubsection{Constituent Structure}
From a derivational analysis of a word in terms of the processes that generated it, we can extract its constituent structure. We can represent it in a sort of phylollogical tree. \\
For \textsl{un|enjoy|able}\! : \textsl{unenjoy} is not a word, as you cannot reverse enjoyment. Even though \textsl{unsee} exists, it is more of a creative product of language as you might not understand its meaning at first glance; it is understood because it is frequent. So we get a structure like\! : Node(un, Node(enjoy,able)). We might go deeper in analysis, but there is no real rule behind \textsl{en-joy}.\\
For \textsl{re|read|able}\! : We cannot base an argument on a stem that doesn't exist, because both hypothetical stems do. Yet, as \textsl{re-} does not work with an adjective, we can derive the structure to be\! : Node(Node(re,read), able)\\
For \textsl{un|wrap|able}\! : Here, both Node(Node(un, wrap), able) and Node(un, Node(wrap, able)) are valid processes of derivation involving different rules. The second is the negation of the adjective \textsl{wrapable}, which means something that cannot be wrapped. The first on the other hand, is something that can be \textsl{unwrap(ped)}, since we add \textsl{able} to a verb. This is an example of structural ambiguity, that is often disambiguated by prosody e.g. \textsl{fat cats eat...} Yet, this not only allows to express ambiguity, but also to explain it.

\subsubsection{Compounding}
This is a quite mysterious area of morphology.

Compounding is a process to build words from two or more stems. It constitutes in combining those stems with a novel meaning, with lower predictability than derivational processes, e.g. \textsl{bitter|sweet}. There is compounding between, adjectives and adjectives, adjectives and nouns, nouns and nouns, nouns and verbs, verbs and verbs, verbs and nouns... There is a very blurry line between what people think and historical reasons, are people thinking of \textsl{sleep} and \textsl{walk} in \textsl{sleepwalk} ?\\
It can be an ambiguous process\! : If a \textsl{houseboat} is a boat that is a house, what is a \textsl{housecat} ? English compounds are typically headed by their rightmost element, meaning the precedents are qualifying it, but sometimes compounds are headless\! : \textsl{kick-ass}, \textsl{ceasefire}...\\
Are there more rules ? Is an N-N compound necessarily a \textit{for} relation, a \textit{from} relation ? Is there simply just a hidden preposition in it ?


\subsection{TA 3\! : 11/10}
\subsubsection{Homework 2}
Nothing to add, see file.

\subsubsection[Cross-Linguistic Variation]{Cross-Linguistic Variation\footnote{This is section of Lecture 3, see the handout}}
Languages can be put on a spectrum from analytic to polysynthetic, based on how much they use morphemes and/or syntax.
Analytic use the most syntax, polysynthetic don't care about it and only add morphemes.
\begin{itemize}
    \item Analytic (Isolating) languages where each morpheme is a word on its own (e.g. Chinese)
    \item Agglutinating languages where each grammatical bit of meaning is an affix to a stem (e.g. Japanese, Korean, Hungarian)
    \item Fusional languages where an affix or change in the stem can lead to multiple grammatical meaning variations (e.g. Latin, German, Arabic)
    \item Polysynthetic Languages where affixes have extremely rich content (e.g. many native North American languages)
\end{itemize}
Both agglutinating and polysynthetic languages pack a lot of information into a single word, the difference being that agglutinating use affixes for grammatical meaning only.

\subsubsection{Morphological Derivation}
\paragraph{Deriving \textsl{Antidesestablishmentarianism}}
We can see this word from a sequence of trees\! :
\begin{center}
    \begin{tabular}{ccc}
        \toprule
                   & Tree\! :                                               & Explanation                                                  \\
        \midrule
        Step 1\! : & \Tree [dis establish ]                                 & $dis : verb \mapsto verb$                                    \\
        Step 2\! : & \Tree [[dis establish ] ment ]                         & $ment : verb \mapsto noun$                                   \\
        Step 3\! : & \Tree [anti [[dis establish ] ment ] ]                 & People are against \textsl{disestablishment}, not the people \\
        Step 4\! : & \Tree [[anti [[dis establish ] ment ] ] arian ]        & $arians : action \mapsto people in it$                       \\
        Step 5\! : & \Tree [[[anti [[dis establish ] ment ] ] arian ] ism ] & $ism : people/concept/action \mapsto idea$                   \\
        \bottomrule
    \end{tabular}
\end{center}

\paragraph{Deriving \textsl{unfriendliness}}
We only give the tree\! :
\Tree [[un [friend li ] ] ness ]

\paragraph{Deriving \textsl{reinstatement}}
You cannot attach \textsl{in} to a noun, it needs to be attached to a verb, but as it is a rare example of borrowing from latin, we cannot separate \textsl{instate}
\Tree [[re instate ] ment ]

\section[Cours 4\!: 12/10]{Syntax 1}
\subsection{Syntactic Competence}
Syntactic competence is a wide concept, it includes \! :
\begin{itemize}
    \item The knowledge of what sentence is grammatical and what isn't. This comes from the fact there is no syntax without semantics in linguistics, unlike in arithmetics. The idea of an autonomous syntax has disappeared since the 70s. It is however useful to talk about syntax in a somewhat disconnected way from semantics.
          For example, you cannot contract auxiliaries when they carry meaning\! : \textsl{I shoulda bought it} and \textsl{*I shoulda more money}.
    \item An abitity to recognize a sentence as well-formed even if it does not make sense or consists of highly sequential transitions, meaning, even though some transitions have really low probability, e.g. \textsl{Colorless green ideas sleep furiously} and \textsl{Colorless Green} or \textsl{sleep furiously}...
    \item An ability to determine what the sentence means, whether it is ambiguous or not either structurally or lexically\! :
          \begin{center}
              \begin{tabular}{cc}
                  \toprule
                  lexical e.g. \textsl{I went to the \emph{bank}}
                                                                                            & $\text{bank } = \begin{cases}
                                                                                                                       & \text{ financial institution} \\
                                                                                                                       & \text{ river bank}
                                                                                                                  \end{cases}$ \\
                  \midrule
                  \multirow[b]{2}{*}{structural, e.g. \textsl{former producers and extras}} & \Tree [{former producers} and extras ]           \\ &\Tree [former [producers and extras ] ]\\
                  \bottomrule
              \end{tabular}
          \end{center}
    \item Every language has a countably infinite set of sentences (proven by showing there is no longest sentence in a language). Native speakers can handle this set despite having finite minds. We can construct arbitrary long sentences based on the center embedding idea, i.e. \textsl{le jeu du Johnny Depp}.
\end{itemize}

\subsection{Syntactic Analysis Building Blocks}
Before writing rules for syntactic analysis, we need blocks to build those rules on.

\subsubsection{Lexical Categories}
The lexical categories (or parts of speech) such as \textsl{N(oun)}, \textsl{V(erb)}\dots
This does not mean you can only tell what section of speech a word is if you know its meaning.\\
\begin{quotation}
    \textit{The thing about thing is that everything and anything is a thing} - Salvador
\end{quotation}

\begin{definition}
    We define lexical categories given the sentential environments in can occur in and the affixes it can take, i.e. the grammatical distribution of the word.
\end{definition}
\begin{quotation}
    \textit{You shall know a word by the company it keeps} - Ferguses
\end{quotation}
For example, a verb in English satisfies all of the following\! :
\begin{enumerate}
    \item It can occur right after the auxiliary \textsl{will}
    \item It can take the endings \textsl{-s} and \textsl{-ing}
\end{enumerate}

\subsubsection{Constituency}
\begin{definition}[A Heuristic]
    A sequence of words forms a constituent in a sentence if\! :
    \begin{enumerate}
        \item It can be replaced with a minimal unit, ideally one word, presevervin grammaticality and conversely
        \item Occurences of that minimal unit can be replaced by the sequence of words, preserving grammaticality
    \end{enumerate}
    If those conditions hold, then the sequence has the same category as the minimal unit.
\end{definition}

A constituent can also be found by Fronting and Pronominalization, Clefting, Eliding or Coordination

\subsection{Rewriting Grammar}
\textsl{Marie verra Jacques} leads to three possible structures in principle\! :
\begin{itemize}
    \item \Tree [.S Marie verra Jacques ]
    \item \Tree [.S [Marie verra ] Jacques ]
    \item \Tree [.S Marie [verra Jacques ] ] (\textsl{verra Jacques} is a Verb Phrase = VP)
\end{itemize}
We might build a rewriting grammar for a fragment of French\! :
\begin{enumerate}
    \item \textsl{Marie verra/entendra/\dots Jacques}
    \item \textsl{Un type verra Jacques}
    \item \textsl{Marie verra un type}
    \item \textsl{Le type connu a vu Marie}
    \item \textsl{Marie dormira}
    \item \textsl{Marie pense que Paul verra Jacques}
    \item \dots
\end{enumerate}


We then can get an idea on French Grammar\! :
\begin{center}
    \begin{tabular}{cc@{$\ \longrightarrow\ $}c}
        \toprule
        Id                                      & Node                            & Rephrased as                            \\
        \midrule
        0                                       & S(entence)                      & N(oun) P(hrase) + VP                    \\
        1                                       & NP                              & Proper Name                             \\
        2                                       & NP                              & Det + N + (Adj)                         \\
        3                                       & VP                              & V$_{intr}$                              \\
        4                                       & VP                              & V$_{tr}$ + NP                           \\
        \midrule
        \multicolumn{3}{c}{Up to this point, we cannot generate an infinite number of sentences}                            \\
        \midrule
        5                                       & C(omplementizer) P(hrase)       & \textsl{que} + S                        \\
        6                                       & VP                              & V$_{cl(ausal)}$ + CP                    \\
        7                                       & V$_{tr}$                        & voir, entendre,\dots                    \\
        8                                       & V$_{intr}$                      & dormir, briller,\dots                   \\
        9                                       & V$_{cl}$                        & penser, croire,\dots                    \\
        \midrule
        \multicolumn{3}{c}{There is now a loop in rules, so there is recursion and we can generate an infinity of sentence} \\
        \midrule
        10                                      & C$_{temp}$                      & avant que, quand                        \\
        11                                      & CP$_{temp}$                     & C$_{temp}$ + S                          \\
        12                                      & S                               & NP + VP + (CP$_{temp}$)                 \\
        \multicolumn{2}{c}{\multirow{2}{*}{VP}} & a) V$_{intr}$ + (CP$_{temp}$)                                             \\
        \multicolumn{2}{c}{}                    & b)V$_{tr}$ + NP + (CP$_{temp}$)                                           \\
        14                                      & S                               & S + CP$_{temp}$                         \\
        15                                      & VP                              & VP + CP$_{temp}$                        \\
        \bottomrule
    \end{tabular}
\end{center}
We can then infer a tree from a sentence with the lexical categories\! :
\begin{center}
    \Tree [.S \qroof{Marie}.NP [.VP [.VP [.V verra ] \qroof{Jacques}.NP ]  [.CP [.C quand ] \qroof{Paul dormira}.S ] ] ]
\end{center}

\subsection{TA 4\! : 18/10}
\subsubsection{Homework 3}
Nothing to say.

\subsubsection{Study of an abstract language}
\begin{center}
    \begin{tabular}{c@{$\rightarrow$}c}
        \toprule
        A & aB \\
        B & c  \\
        B & z  \\
        B & C  \\
        C & nC \\
        C & d  \\
        \bottomrule
    \end{tabular}
    \Tree [.A a [.B [.C n [.C n [.C n C ] ] ] ] ]
\end{center}

This gives us many strings possible\! : \textsl{ac}, \textsl{az}, \textsl{an$^{*}$d}...

We can draw trees from those strings, e.g. \textsl{annnd}, see above.

\subsubsection{Structure of sentences}
\paragraph{John sees Mary}
\Tree [.S \qroof{John}.NP [.VP [.V sees ] \qroof{Mary}.NP ] ]

\paragraph{John Sleeps}
\Tree [.S \qroof{John}.NP [.VP [.V sleeps ] ] ]

\subsubsection{Heads, Complements, Adjuncts}
\begin{definition}
    A \textit{head} is a word. A \emph{head} is connected to a phrase. Anything that is the sister of a \emph{head} is a \emph{complement}. When a \emph{complement} is unnecessary, it is called an \emph{adjunct}.
\end{definition}

\begin{center}
    \Tree [.S \qroof{John}.NP [.VP [.V {eats =  \textsl{head}} ] {apples = \textsl{complement}} ] ]
    \Tree [.S \qroof{John}.NP [.VP [.{VP or $\overline{\text{ V }}$} [.V eats ] \qroof{apples}.NP ] [.PP [.P on ] \qroof{the train}.NP ] ] ]
\end{center}
\begin{center}
    \Tree [.XP [.XP {X = \textit{head}} {YP = \textit{complement}} ] {ZP = \textit{adjunct}} ]
\end{center}

\begin{center}
    \Tree [.S \qroof{Galileo}.NP [.VP [.V saw ] [.NP \qroof{a man}.NP \qroof{with a telescope}.PP ] ] ]
    \Tree [.S \qroof{Galileo}.NP [.VP [.VP [.V saw ] \qroof{a man}.NP ] \qroof{with a telescope}.PP ] ]
\end{center}

\subsubsection{Structure of Noun Phrases}
To analyze Noun Phrases with adjectives, we can base on the model from the first example below, introducting Determiner Phrases. We can do the same with verb phrases, seeing pronouns as determiners (this is called the DP hypothesis)\! :
\begin{center}
    \Tree [.S [.DP [.Det {the/a/every/each/\dots} ] [.NP [.AP [.A red ] ] [.N cat ] ] ] [.VP [.V sleeps ] ]]
    \Tree [.S [.DP [.Det he ] ] [.VP [.V sleeps ] ] ]
\end{center}

We may guess that in the sentence \textsl{Mary sleeps}, \textsl{Mary} is a determiner, yet we could say \textsl{every Mary sleeps}. There are two ways of seeing the sentence \textsl{Mary sleeps}\! : either \textsl{mary} is a noun phrase or \textsl{Mary} is a determiner phrase with an empty determiner.
\begin{center}
    \Tree [.S [.NP [.N Mary ] ] [.VP [.V sleeps ] ] ]
    \Tree [.S [.DP [.D {$\emptyset$} ] [.NP [.N Mary ] ] ] [.VP [.V sleeps ] ] ]
\end{center}
The latter is better from the study of other languages such as Portuguese where the determiner is not empty\! : \textsl{a Maria dorm}.

\subsubsection{Teaser for next class}
Consider \textsl{He will go to the school}\! :
\begin{center}
    \Tree [.S [.DP [.D He ] ] [.TP [.{T(ense)} will ] [.VP [.V go ] [.PP [.P to [.DP [.D the ] [.NP [.N school ] ] ] ] ] ] ] ]
\end{center}
We here only did binary branching, but there might be ternary branching sometimes.

\subsection{TA 5\! : 25/10}
\subsubsection{Kinda like Homework}
\begin{center}
    \Tree [.S \qroof{The cat}.NP [.VP [.V ate ] \qroof{the fish}.NP ] ]
    \Tree [.S \qroof{John}.NP [.VP [.V is ] \qroof{tall and fast}.AP ] ]
    \Tree [.S \qroof{John}.NP [.VP [.V$_{\text{ditransitive}}$ found ] \qroof{the book}.NP \qroof{useful}.AP ] ]
\end{center}
We could leave the VP in the last example with only two children by leaving the topological order. We would need to implement inside the heads a rule to tell the order of pronunciation. Another way to achieve this is by saying \textsl{the book useful} has secretely a more complex structure such as \textsl{the book to be useful}.

\subsubsection{Grammar Generating a Language}
Language\! : $L = \left\{a^{n}b^{n}\mid n \in \N \right\}$\\
Grammar\! :
\begin{center}
    \begin{tabular}{c@{ $\rightarrow$ }c}
        S & aSb + $\epsilon$ \\
    \end{tabular}
\end{center}
We get, for \textsl{aabb}\!:
\Tree [.S a [.S a [.S $\epsilon$ ] b ] b ]

Language\! : $L = (ab)^{\star}$. \\
Grammar\! :
\begin{center}
    \begin{tabular}{c@{ $\rightarrow$ }c}
        S & abS + $\epsilon$ \\
    \end{tabular}
\end{center}
For \textsl{ababab}\!:
\Tree [.S ab [.S ab [.S ab [.S $\epsilon$ ] ] ] ]

Language\! : Well-Parenthesized Expressions - Dyck's First Language\\
Grammar\! :
\begin{center}
    \begin{tabular}{c@{ $\rightarrow$ }c}
        S & $\epsilon$ \\
        S & ()S        \\
        S & (S)
    \end{tabular}
\end{center}
For \textsl{()()}\! :
\Tree[.S () [.S () [.S $\epsilon$ ] ] ]

\subsubsection{On Disambiguation}
\textsl{John looks like Mary} might mean \textsl{John looks in the same manner as Mary}. Syntactician do not care about priority rules, they don't want to disambue their grammar. They just add probability to their production rules, to generate parses probabilistically.

\section[Cours 5\! : 26/10]{Syntax 2}
\subsection{About Homework 4}
In the sentence \textsl{The milk perished}, we cannot interpret \textsl{perished} as a verb directly, but as a verb in a verb phrase.\\
The sentence \textsl{Time flies like an arrow} can be read as \textsl{Time flies} (a particular kind of fly) \textsl{like an arrow} would then mean that a special type of fly likes a particular arrow. Same thing\! : \textsl{fruit flies like a banana}.\\
\textsl{The charges against him} may be read by the rule $NP \rightarrow NP \ PP$ or by looking at \textsl{against him} as an argument of the noun \textsl{charges}.
By definition, grammatical operations can only target nodes of the syntax tree. Then, we cannot really have noun phrases with structure $NP \rightarrow Det\ N\ PP$. Yet, as all X-phrases have an X in them, except for S(entences), it seems unreasonable to regroup the noun and the PP inside a block inside a NP. We then might want to introduce intermediate generalizations with new parameters such as $\overline{X}$.\\
In \textsl{The accident deprived him of his mobility}, we cannot regroup \textsl{him of his mobility} under one block, and then we may look at \textsl{deprived} as having two complements, \textsl{him} and \textsl{of his mobility}.\\
Passive sentences cause mayhem in syntax, so transformational grammar was created to derive transformation rules that would modify an hypothetical arrangement of the informations in the sentence into the arrangement that comes out of speaking.

\subsection{Inflection Phrases}
Modern syntactic theories postulate the existence of Inflection Phrases (IP) instead of S phrase. This addresses the emoverlinerassment of having a phrasal constituent without a head. The inflection (tense, mood, person/number), seen on the verb, is the head category of a sentence. Then, using $\overline{\text{I}}$ we can retrieve the information on the tense/modality by separating the verb from the inflection\! :
\begin{center}
    \Tree [.IP \qroof{John}.NP [.$\text{I}^{'}$ [.I \textsc{Past} ] [.VP [.V saw ] \qroof{Mary}.NP ] ] ]
\end{center}
In sentences with modals or auxiliary verbs, we can see phonologically overt material occupying the I node, e.g.\! :
\begin{center}
    \Tree [.IP \qroof{John}.NP [.$\text{I}^{'}$ [.I {will} ] [.VP [.V see ] \qroof{Mary}.NP ] ] ]
\end{center}
Most importantly, certain facts about the placement of adverbs help us figure out where precisely in the tree the main verb is to be found. The position is dependent on the language.\\
Inflection phrases can explain the difference in positions of the word \textsl{often} and  \textsl{souvent} in English and French.


\subsection{Displacement}
From Inflection Phrases, we can derive the structure of sentences from others by using transformational rules.\\
Many pairs of sentences in English are intuitively “variations” on each other. They are related by displacement, in that one appears derived from the other by moving certain constituents around. In the first formal theories structures, the pairs (Affirmative, Interrogative) were taken to have a common D(eep)-structure but different S(urface)-structure. S-structures are the results of applying certain transformations to D-structures.
\begin{remark}
    Contemporary syntactic theories in the Chomskyan tradition instatiate this intuition differently, taking a more derivational approach.
\end{remark}
From this postulate, we can derive from languages transformational rules. For exemple, the I-to-C transformational rule is used to build a yes-no question from a D-structure with phonologically overt material in I by\! :
\begin{enumerate}
    \item Projecting a CP above IP
    \item Moving I to C
\end{enumerate}
For example, we get\! :
\begin{center}
    \Tree [.IP \qroof{John}.NP [.I$^{'}$ [.I will ] \qroof{talk to Mary}.VP ] ] $\xrightarrow{\text{I-to-C}}$ \Tree [.CP [.C Will ] [.IP \qroof{John}.NP [.I$^{'}$ [.I {(trace of will)} ] \qroof{talk to Mary}.VP ] ] ]
\end{center}

\subsection{TA 6\! : 8/11}
\subsubsection{On the words \textsl{Often}, \textsl{Souvent}, and Displacement}
In english, we can do\! :
\begin{center}
    \Tree [.CP [.C {$\emptyset$} ] [.IP [.IP \qroof{John}.DP [.IP {I$_{\text{+past}}$} [.VP often [.VP [.V saw ] \qroof{Mary}.DP ] ] ] ] ] ]
\end{center}
But in French, a similar structure feels unnatural because of a crossing\! : \textsl{Jean voit souvent Marie}. We thus propose to have the verb in the inflection phrase and use a trace in the Verb Phrase\! :
\begin{center}
    \Tree [.CP [.C {$\emptyset$} ] [.IP \qroof{Jean}.DP ] [.{I$^{'}$} [.{I$_{\text{+présent}}$} voit ] [.VP \qroof{souvent}.AdvP [.VP [.V trace ] \qroof{Marie}.DP ] ] ] ]
\end{center}

Then, in future tense, in English\! :
\begin{center}
    \Tree [.CP [.C {$\emptyset$} ] [.IP [.IP \qroof{John}.DP [.IP [.{I$_{\text{+future}}$} will ] [.VP often [.VP [.V see ] \qroof{Mary}.DP ] ] ] ] ] ]
\end{center}
And in French\! :
\begin{center}
    \Tree [.CP [.C {$\emptyset$} ] [.IP \qroof{Jean}.DP [.{I$^{'}$} [.I va ] [.VP \qroof{souvent}.AdvP \qroof{aller parler avec Marie}.VP ] ] ] ]
\end{center}

And for questions ? In English, we move the verb before the subject, but in French, there are multiples solutions\! :
\begin{center}
    \Tree [.CP Va [.IP \qroof{t-il}.DP [.{I$^{'}$} [.I $\emptyset$ ] \qroof{parler avec Marie}.VP ] ] ]
    \Tree [.CP {Est-ce qu'} [.IP \qroof{il}.DP [.{I$^{'}$} [.I $\emptyset$ ] \qroof{parler avec Marie}.VP ] ] ]
\end{center}
And in the present tense\! :
\begin{center}
    \Tree [.CP [.C ] [.IP \qroof{il}.DP [.{I$^{'}$} [.I voit ] [.VP [.V trace ] \qroof{Marie}.DP ] ] ] ]
    \Tree [.CP [.C Voit ] [.IP \qroof{il}.DP [.{I$^{'}$} [.I trace ] [.VP [.V trace ] \qroof{Marie}.DP ] ] ] ]
\end{center}
The verb here moves twice, from the \textsl{V} to the \textsl{I} then to the \textsl{C}. \\

We can do the same in English\! :
\begin{center}
    \Tree [.CP [.C ] [.IP \qroof{John}.DP [.{I$^{'}$} [.I  ] [.VP [.V sees ] \qroof{Mary}.DP ] ] ] ]
    \Tree [.CP [.C ] [.IP \qroof{John}.DP [.{I$^{'}$} [.I does ] [.VP [.V see ] \qroof{Mary}.DP ] ] ] ]
    \Tree [.CP [.C Does ] [.IP \qroof{John}.DP [.{I$^{'}$} [.I trace ] [.VP [.V see ] \qroof{Mary}.DP ] ] ] ]
\end{center}

\section[Class 5\! : 9/11]{Semantics I}
\subsection{Syntax and Grammaticality}
Syntax as we studied it in this course has been about how words are put together to form grammatical sentences, with only limited consideration of meaning. For typical artificial languages, we define semantics over an already defined syntax, and we do not need that every symbol has a sense. Yet, for natural languages, we need to ask for that, and there needs to be a connection between semantics and syntax. In linguistics, there is no such thing as a syntax without a semantics.\\
We need to have things that can be interpreted in particular models. Yet, even tho some words don't have sense or are not words in english (e.g. Jabberwocky) we can figure grammaticality without sense. Same, we can figure out sense without grammaticality, but this is not of interest.

\subsection{Semantics}
\subsubsection{The Study of Meaning}
The meaning of linguistic expressions is what we care about, not really their form. Semantics studies\! :
\begin{enumerate}
    \item The meaning of linguistic expressions
    \item The way in which form and meaning are connected
    \item The different kinds of meanings that natural languages use
\end{enumerate}
Two important uses of language are exchanging information in communication and internal calculation.\\
Propositions are the kinds of meanings that contain information, which can in turn be used in communication and in internal calculation. Indeed, it is very unclear how much (if any) information interrogative sentences contain; they structure, give a goal. \\
The paradigmatic linguistic exponents (syntactic categories) of propositions are declarative sentences.

\subsubsection{Understanding Sentences}
What is "understanding a declarative sentence" ?\\

Consider \textsl{The planet Earth is roughly spherical}, \textsl{France is a country in East Asia}. One property that propositions have is that they can be true or false. We get a first stab at an answer\! : \textit{To understand a sentence is to know whether it is true or false.}\\

But we clearly don't need to know whether a sentence is true or false to understand its meaning\! : \textit{Salvador likes almonds}. (Here we suppose that the speaker is cooperative, thus Salvador is the one we think about, the teacher.)
\begin{quotation}
    I am the most salliant Salvador in your lives - Salvador.
\end{quotation}
Sometimes, there is a way to verify the truth value of a sentence \textsl{The world will come to an end in 2025}, but sometimes there might not be \textsl{Humans will never inhabit Jupiter}, and sometimes it is impossible to be certain \textsl{Socrates never existed}, we only have clues it did. We get a second stab at an answer\! : \textit{To understand a sentence is to know under which conditions it would be true, and under which conditions it would be false.}\\

This is clearly not enough, how to understand swear words in this paradigm. How to understand questions and imperatives ? In the early days, we tried to reduce interrogatives to declaratives, but it doesn't work, since it implies wrapping a declarative aroung an interrogative.

\subsection{Models}
Theories formulated in artificial languages are interpreted with respect to standard models\!: mathematical structures that represent what the artificial language is meant to be about, allowing us to be completely rigorous about the conditions under which statements in the artificial language are true or false.
\subsubsection{Models and Interpretations}
We have something of such\! :
\begin{itemize}
    \item Statement (with sugar)\! : $1 + 2 = 4$
    \item Standard Model\! : $\mathcal{M} = \left\langle\mathbb{N}; s(\cdot), \cdot + \cdot;\leq;0\right\rangle$
    \item Truth Conditions of The Statement\! : To apply the successor function $s(\cdot)$ of $\mathcal{M}$ three times is the same as to apply it four times. (This is \textmd{False} under a successor function that works as intended and as can be defined using a decent axiomatization of arithmetic.)
\end{itemize}
Remember that the interpretation of a formal system (whatever complex), is always given in a natural language. This is only an attemps to reduce the complexity of the interpretation in natural language of a system. The idea is to provide an idea of the number of properties that models need to have to get the truth value.

\subsubsection{First Order Logic}
FoL is a particularly powerful artificial language that constitutes the logical foundation for most of modern science.
\begin{center}
    \begin{tabular}{cp{.5\textwidth}}
        $\exists x$                                                          & There is an individual, call it $x$.                                                                                    \\
        \midrule
        $xKf$                                                                & $x$ stands in the $K$ relation with $f$                                                                                 \\
        \midrule
        $\forall y$                                                          & For any individual, call it $y$                                                                                         \\
        \midrule
        $yKf \rightarrow y = x$                                              & $y$ stands in the $K$ relation with $f$ imposes $y$ to be equal to $x$                                                  \\
        \midrule
        $\wedge$                                                             & conjunction, meaning and                                                                                                \\
        \midrule
        $\exists x.xKf\wedge \left(yKf \rightarrow y = x\right)$             & There is exactly one individual standing in the $K$ relation with something called $f$.                                 \\
        \midrule
        $B(x)$                                                               & $x$ has the property $B$                                                                                                \\
        \midrule
        $\exists x.xKf\wedge \left(yKf \rightarrow y = x\right) \wedge B(x)$ & There is exactly one individual standing in the $K$ relation with something called $f$. Let's call that individual $x$.
    \end{tabular}
\end{center}
This table gives a way to model in FoL the sentence \textsl{The King of France is Bald}.

This is insufficient to understand the sentence, since we don't know what \textsl{King}, \textsl{France} nor \textsl{Bald} means. We have only explained the word \textsl{the}, by asserting the existence and the unicity. $B(x)$ here can be viewed as an interpreation of \textsl{is} and $xKf$ as an interpretation of \textsl{of}. Yet, here, \textsl{of} seems to only be verbal tissue. Here we have understood a section of functional words, but not lingual functions of words, and this is insufficient to end the analysis\! : \textsl{The day was lovely}, is not fully understood, there might have been many loverly days. We then need to use some kind of contextual restriction. \\
Also remember that, following Russel, \textsl{The King of France is not Bald} doesn't negate the whole formula, but only $B(x)$. And, moreover, saying something like \textsl{John knows the Earth is flat}, also implies the speaker thinks the Earth is flat. \\

FoL models are structure which tell us precisely, which individuals there are, what properties they have, what relations they participate in and with whom. \\
A FoL formula will be true in somme FoL models, and false in others, e.g.\! :
\begin{itemize}
    \item In $\mathcal{M} = \scalar{\left\{a, b, f, g\right\}; K = \left\{\scalar{a, f}, \scalar{b, g}\right\}; B = \left\{a, b\right\}}$, the sentence is true.
    \item In $\mathcal{M}^{'} = \scalar{\left\{a, b, f\right\}; K = \left\{\scalar{a, f}, \scalar{b, f}\right\}; B = \left\{a\right\}}$, the sentence is false since there are two $x$ such that $xKf$.
    \item In $\mathcal{M}^{''} = \scalar{\left\{a, b, f\right\}; K = \left\{\scalar{a, f}\right\}; B = \emptyset}$, the sentence is false since there are no elements in $B$.
\end{itemize}
We can take the class of models where the sentence is true to fully specify the truth conditions of the sentence. The task for truth-conditional semantics is to provide an interpretation function that associates each sentence with its class.

\subsection{Compositionality}
\subsubsection{Frege's Principle}
The meaning of a sentence is determined by the meaning of its parts and the way in which those parts are assembled.

This comes from the fact that we interpret sentence we've never heard just as easily as sentences we've already heard. This implies a huge section of compositionality.

\subsubsection{Combining Meanings}
\paragraph{Infering from Structure}
\begin{center}
    \Tree [.S [.NP [.N Kim ] ] [.VP [.V ate ] [.NP [.D a ] [.N pear ] ] ] ]
\end{center}
We first infer $\model{pear}$ then $\model{a pear}$, then $\model{ate a pear}$ and so on...\\

We use the phrase structure rule\! : $S \rightarrow NP \ VP$ where $NP$ is the subject of the predicate $VP$. In many sentences of natural languages, the predicate attributes some property to the individual contributed by the subject.\\
What is the meaning of a name or of a definite noun phrase ? These are all individuals, real or fictional. The noun phrase \textsl{Salvador} refers to Salvador. Salvador is the referent of \textsl{Salvador}.\\
Predicates correspond to properties\! : \textsl{is a carpenter} corresponds to the property of being a carpenter. Properties can apply to (be true of) a number of individuals. At the very least, properties must be able to tell us who has them, and who lacks them. So, at a first approximation, we might identify properties with the sets of individuals that they are true of.\\


Now, since Subject NPs refer to individuals and Predicate VPs denote sets of individuals, we can relate the two so as to capture the truth conditions of the sentence\! : we say $\model{Mary is a carpenter}$ is true if and only if $\model{Mary}$ is a member of the set $\model{is a carpenter}$. These are the truth conditions of this sentence.
\begin{proposition}[Semantic Rule I]
    If S is a sentence...
\end{proposition}

\paragraph{Intersective Adjectives}
Yet, for more complicated predicates (e.g. \textsl{is a smart student}), how do we do ? Intuitively, this VP is composed of two properties (\textsl{begin smart} and \textsl{being a student}). We may want to take the intersection of those two sets then apply Semantic Rule I, this is Semantic Rule II.
\begin{definition}
    Entailment\! : A sentence $X$ entails another sentence $Y$ (noted $X \models Y$) just in case, whenever $X$ is true, $Y$ is true. For example\! :
    \begin{itemize}
        \item \textsl{John is a blond carpenter} $\vDash$ \textsl{John is blond}
        \item \textsl{John loves cats} $\nvDash$ \textsl{John loves dogs}
        \item \textsl{John is a blond carpenter} $\vDash$ \textsl{John is a carpenter}
    \end{itemize}
\end{definition}

\paragraph{Non-Intersective Adjectives}
What about\! : \textsl{Pat is a fake carpenter} ? Does it entail those sentences ?
\begin{itemize}
    \item \textsl{Pat is fake}
    \item \textsl{Pat is a carpenter}
\end{itemize}
This sentence shows our rule for intersection does not work for all adjectives. We restrain rule II to intersective adjectives (such as \textsl{prime} or \textsl{even}), and call non-intersective adjectives those that fail the entailment test. Worse\! : \textsl{That gun is fake} or \textsl{This is a toy gun}; can we then call it a \textsl{gun} ?\\
We might think that fake means not, but Salvador is not a fake carpenter, and is not a carpenter. Then, is a \textsl{fake blah} something that is not a \textsl{blah}, but seems to be a \textsl{blah} ? Fakeness seems to require intentionality. Then, \textsl{fake} is some kind of functional element that applies to nous, and generates a new set of properties, maybe the set of things that intend to look like the argument intersected with the complement.\\
And even then, what can we say about \textsl{tall} ? It cannot simply represent a set. \textsl{Tall} is a subsective adjective since it returns a set beneath a context. Moreover, there is often typicality\! : when shown a picture of a robin, people will more easily say "This is a bird" than when shown a more atypical bird such as a penguin.\\
More over, even for non-intersective adjectives, sometimes the argument is not made of morphemes\! : \textsl{Pat is an excellent carpenter} and \textsl{That carpenter is excellent}. Then, we may say that there is a placeholder of some sort filling the argument, which is resolved by contextuality. Same thing\! : \textsl{Last night there was an excellent politician and a pretty awful carpenter}, what is the carpenter \textsl{awful} at ? Eventually, there is some intersection between excellent and carpenter.


\subsection{TA 7\! : 15/11}
\subsubsection{One Last Big Syntax Tree}
We draw the tree for \textsl{The poor doctor thinks that the black cat walks quietly}. We start by mapping the words to their parts of speech. We assume the DP hypothesis and will use $\overline{\text{X}}$-trees. For example, \textsl{The poor doctor} and \textsl{The black cat} become\! :
\begin{center}
    \Tree [.DP [.{$\overline{\text{D}}$} [.D The ] ] [.NP [.{$\overline{\text{N}}$} [.AP [.{$\overline{\text{A}}$} [.A poor ] ] ] [.{$\overline{\text{N}}$} [.N doctor ] ] ] ] ] and
    \Tree [.DP [.{$\overline{\text{D}}$} [.D The ] ] [.NP [.{$\overline{\text{N}}$} [.AP [.{$\overline{\text{A}}$} [.A black ] ] ] [.{$\overline{\text{N}}$} [.N cat ] ] ] ] ]
\end{center}

Then, we get for \textsl{the black cat walks quietly}\! :
\begin{center}
    \Tree [.IP \qroof{the black cat}.DP [.{$\overline{\text{I}}$} [.I {$\left[\text{present}\right] \emptyset$} ]  [.VP [.{$\overline{\text{V}}$} [.{$\overline{\text{V}}$} [.V walks ] ] [.AdvP [.{$\overline{\text{Adv}}$} [.Adv quietly ] ] ] ] ] ] ]
\end{center}
This shows why we need to use the $\overline{\cdot}$ structure, as it allows us to differentiate complements (that attach to the lowest $\overline{X}$, next to the head $X$), adjuncts (that attach to the other $\overline{X}$) and an eventual specifier that attaches directly to the $XP$. Then\! :
\begin{center}
    \Tree [.VP [.{$\overline{\text{V}}$} [.V thinks ] [.CP [.$\overline{\text{C}}$ [.C that ] \qroof{the black cat walks quietly}.IP ] ] ] ]
\end{center}
Then, the global structure becomes\! :
\begin{center}
    \Tree [.CP [.{$\overline{\text{C}}$} [.C {$\left[\text{decl}\right]\ \emptyset$} ] [.IP \qroof{The poor doctor}.DP [.{$\overline{\text{I}}$} [.I {$\left[\text{present}\right]\ \emptyset$} \qroof{thinks that the black cat walks quietly}.VP ] ] ] ] ]
\end{center}

Finally\! :
\begin{center}
    \Tree [.CP [.{$\overline{\text{C}}$} [.C {$\left[\text{decl}\right] \emptyset$} ] [.IP [.DP [.{$\overline{\text{D}}$} [.D The ] ] [.NP [.{$\overline{\text{N}}$} [.AP [.{$\overline{\text{A}}$} [.A poor ] ] ] [.{$\overline{\text{N}}$} [.N doctor ] ] ] ] ] [.{$\overline{\text{I}}$} [.I {$\left[\text{present}\right]\ \emptyset$} [.IP \qroof{the black cat}.DP [.{$\overline{\text{I}}$} [.I {$\left[\text{present}\right] \emptyset$} ]  [.VP [.{$\overline{\text{V}}$} [.{$\overline{\text{V}}$} [.V walks ] ] [.AdvP [.{$\overline{\text{Adv}}$} [.Adv quietly ] ] ] ] ] ] ] ] ] ] ] ]
\end{center}

\subsubsection{Entailment}
For sentences, we describe their entailment\! :
\begin{itemize}
    \item $\textsl{John is a man} \vDash \textsl{John is a human}$\! : This gives an idea on the way we understand words.
    \item $\textsl{John blicketed Mary} \equiv \textsl{Mary was blicketed by John}$\! : This is purely based on syntax.
    \item $\textsl{Everybody left} \vDash \textsl{John left}$\! : This is only true based on the contextual restriction of \textsl{Everybody}\! : it is true if and only if \textsl{John} is a member of the \textsl{Everybody} considered.
    \item $\textsl{John read and Mary sang} \vDash \textsl{John read}$ and $\textsl{John read and Mary sang} \vDash \textsl{Mary sang}$\! : This comes from the definition of the logical and.
    \item $\textsl{John read} \vDash \textsl{John read or Mary sang}$\! : This is always true.
    \item $\textsl{All lemons are yellow and not all lemons are yellow} \vDash \textsl{Charles the Fifth is a horse}$\! : A false statement implies any statement.
    \item $\textsl{John killed Mary} \vDash \textsl{John caused Mary to die}$\! : The reciprocal seems to not be always true. It depends on the way we interpret \textsl{killed}. Yet, it seems hard to find a counter-example\! : maybe thinking about a long enough causal chain\! : \textsl{Franz Ferdinand} caused all the deaths in WW1, but he didn't kill some random soldier. A way to visualise the difference would be to look at the scenari that come to mind when hearing these sentences.
    \item $\textsl{John did some of his homework} \nvDash \textsl{John did not do all of his homework}$\! : \textsl{Some} implies a section of his homework, which can be the whole homework. Thus, the first sentence does not \emph{entail} the second. We then talk about \emph{implicatures}, which are away to think about pragmatic reasoning. This comes from the fact that people mostly want to be as precise as possible. They might disappear under \textsl{if}-\textsl{then} statements.
    \item $\textsl{The King of France is not bald} \nvDash \textsl{The King of France exists}$\! : Again, this is not entailment, but the fact that the first sentence does not make sense if the second if false makes a \emph{presupposition} mandatory. Another example is \textsl{I stopped smoking} which implies that \textsl{I used to smoke}.
\end{itemize}


\section[Class 6\! : 16/11]{Semantics II}
\subsection{Limits of our Definition}
\subsubsection{Problems}
Something is said to be denotational when it directly points out to something from the real world. \\

In our definition, if two sentences are true in the same conditions, then their meaning are identical.
Yet, talking about \textsl{Kim is a bachelor} and \textsl{Kim is an unmarried man}. Then, what about the Pope? What about a 5 year old?
But then again, consider\! :
\begin{itemize}
    \item i) \textsl{Two plus two equals four} and ii)\textsl{Forty-five times six equals two-hundred and seventy}. Those sentences are true under every circumstances, yet they make sense very differently. Then those two sentences mean exactly the same thing ?
    \item iii) \textsl{I want to take the ENS Semantics course in the Spring} and iv) \textsl{I don't not want to take the ENS Semantics course in the Spring}. Fuck excluded middle.
\end{itemize}
Someone whose mind represents i) doesn't nessecarily assent to ii). So they must, in some sense mean different things, if meanings are supposed to exist in our minds. Rather than mapping sentences to stuff and facts out there in the world, rather than talking about truth conditions, perhaps we should map linguistic elements to the mental representations that give rise to them and that they evoke. If you're a brain in a vat, is there anything about the semantics of your language that changes ? Can we have both projects ? Can we combine the mentalistic project of talking about the structure of mental representations and the naturalistic project that involves connecting language to the external world ?

\subsubsection{Solutions ?}
Putnam argues that "wide beliefs", that is beliefs that are about something outside the mind, cannot be given a purely internalistics semantics. Say I display some behvaior that every human with basic common sense can see was directly caused by some belief or other of mine. Any representational analysis of this belief now needs to connect it to things and facts out there in the world, otherwise our theory of representations does absolutely nothing in way of explaining our behavior, since the behaviour is precisely all about our interactions with the outside world.\\

Fodor bites the methodologically solipsistic bullet\! : perhaps you can't have your cake and eat it, perhaps you can't have a formal semantics that is both psychological and connected to the external world. But that's not such a big deal, precisely because for almost all cases, and notably many of the most mysterious cases, common sense will tell what in your beliefs connects to what in the world, we can just accept this big hole in our understanding of how mental representations connect to the world, and just move on, focusing our scientific enterprise on the matters we do know how to build properly general theories of\!: what must representations look like, what structural relations do they have, what are their truth conditions, how are they used in internal calculation and in communication.

The same problems happen in all science, but there are more traces of arguments about that in cogsciences.

\subsubsection[On the trouble with adaptationist explanations or What Fodor Got Right]{On the trouble with adaptationist explanations or What Fodor Got Right\footnote{What Darwin Got Wrong}}
Say we have two phenotypical traits $T_{1}$ and $T_{2}$, whose precise genetic underpinnings we don't yet understand and we can see pretty clearly using basic common sense that $T_{1}$ is the trait responsible for an increase in fitness and that $T_{2}$ is completely orthogonal\! :
\[
    \begin{aligned}
        T_{1} & \xrightarrow{\text{fit for selection}} \text{ selected}         \\
        T_{2} & \xrightarrow{\text{not fit for selection}} \text{ not selected}
    \end{aligned}
\]

But if the traits are coextensional, no amount of observational data can resolve this question\footnote{Bareinboim et al. 2022}. We need a causal theory of how traits connect to fitness. There is no such general theory of intensional causation, and no one knows if one can even in principle be given.

\[
    \begin{aligned}
        T_{1}        & \xrightarrow{P(F\mid T_{1}) > P(F\mid \lnot T_{1})} \text{ selected for}     \\
        \updownarrow &                                                                              \\
        T_{2}        & \xrightarrow{P(F\mid T_{2}) > P(F\mid \lnot T_{2})} \text{ not selected for}
    \end{aligned}
\]

And for the most theoretically opaque cases, your grandparents could tell you easily which trait is causally connected to fitness. Fodor's conclusion\! : there's a big mind shaped hole in the theory of natural selection.
\[
    \begin{aligned}
        T_{1}        & \xrightarrow{T_{1} \leadsto \text{ fitness}} \text{ selected for}         \\
        \updownarrow &                                                                           \\
        T_{2}        & \xrightarrow{T_{2} \not\leadsto \text{ fitness}} \text{ not selected for}
    \end{aligned}
\]

Much like the problem with representations, here we have a hole in the theory for which we don't seem to know how to give a general account. Yet, we can give special accounts\footnote{André et al. 2023, on the design features of moral cognition, with precise models of cooperation and interaction}. We can conclude that the validity of adaptationist accounts is predicated on the validity of the special causal theories presumed, and we can demand that those causal theories be rigorous. Whether this is too tall an order is an important open question.

At least, we can study the structure of meanings qua elements of mental representations determined by linguistic properties and more generally based on facts observed in the environment.

\subsection{Assertion and Presupposition}
\subsubsection{The notion of Assertion}
The assertive content of a sentence is everything in its meaning that is not preserved in the negation or the interrogation

\subsubsection{The notion of Presupposition}
Take sentences \textsl{Kim has stopped smoking} and \textsl{Kim used to smoke}. The first one entails the second, and so do the negated, interrogative and modal version of it, we say that it presupposes the second.
More formally, $S_{1}$ presupposes $S_{2}$ if $S_{1} \vDash S_{2}$ and so do the negative, interrogative and modal versions of $S_{1}$.

As an example, \textsl{Pat got married again} presupposes that \textsl{Pat was married}, \textsl{John knows that Mary is a blonde carpenter} presupposes \textsl{Mary is a blonde carpenter}.

Presupposed content is assumed to be section of the conversational common ground, and is hard or impossible to negate. The assertion is the section of the content that is not actually presupposed. \textsl{The King of France is not bald} presupposes that \textsl{There is a King of France}. It can be challenged in certain special ways\! :
\begin{itemize}
    \item \textsl{No, the King of France is} \textsc{Not} \textsl{bald, there} \textsc{is} \textsl{no King of France}
    \item \textsl{Hey, wait a minute ! There is no King of France !}
    \item But it seems weird to just say\! : \textsl{That's false}.
\end{itemize}
Here, the negation is sort of 'meta', since you can in fact utter the words.

\subsubsection{Literal Meaning and Implicatures}
The asserted and presupposed contents of a sentence form its literal meaning, but naturally occurring conversational is often not literal at all\! :
\begin{itemize}
    \item Phone Call\! : \textsl{Is Bill there ? }
    \item At dinner\! : \textsl{Can you pass the Salt ?}
    \item Sarcasm\! : \textsl{Not at all, I absolutely} \emph{love} \textsl{being stepped on}
    \item Conversation\! : \textsl{What's the temperature outside ? Oh, you won't need a coat.}
\end{itemize}
Besides having literal meaning, all sentences also acquire a non-literal meaning contained in the context. The propositions that sentences may non-literally suggest, as in the previous slide, are called implications.

\begin{itemize}
    \item Mary\! : Is Pierre a good cook ?
    \item John\! : Hum, he's French.
    \item Implicature\! : He is a good cook.
\end{itemize}

John's reply suggests that Pierre is indeed a good cook, given the stereotype. But how are implicatures differents ? Implicatures can be canceled without the speaker inccurring a contradiction. The same is not true of entailments\! : \textsl{He's French, but he's actually not a French citizen}.

\subsubsection{Cooperation}
\begin{proposition}[Cooperative Principle]
    Participants in a conservation assume that speakers are cooperative\! : everyone is doing their best to further the goals of conservation. Conservations follow certain maxims that stem from the assumption of cooperativeness.

    \begin{itemize}
        \item Relevance\! : Every statement made in a conversation must address the issues that conversation is about
        \item Quality\! : Every time a speaker utters a sentence they say something they believe to be true.
        \item Quantity\! : Every statement needs to be as informative as the situation requires it to be.
    \end{itemize}
\end{proposition}

These maxims are explicit rules that all speakers follow because they are cooperative. Sometimes, literal content of a statement seems to go against one or more of these maxims. They can be superficially flouted. Hearers will draw all manner of conclusions about what speaker meant by their utterance (implicatures of the speaker's utterances), while following the assumption that the speaker is cooperative.

\begin{itemize}
    \item In the example, John seems to go against the maxim of relevance, but the hearer assumes that John is being cooperative, and thus that there must be some way to interpret John's statement as adressing the question. When an answer really goes agaisnt the maxim of relevance, we call them an infelicitous answer, e.g. \textsl{What time is it ? FISH !}.
    \item The maxim of quality makes sure that cooperative speakers aren't absolute skeptics, information exchange is impossible unless we curtail skepticism. Sarcasm is an example of flouting this maxim.
    \item The maxim of quantity implies that speakers will typically be assumed to be imparting as much information as they can.
\end{itemize}

\subsubsection{Implicatures and Operations}
Either\! :
\begin{itemize}
    \item Disjunction is conjunctive in NL
    \item Disjunction is ambiguous in NL
    \item Disjunction is semantics $\left\{\text{disjunction, conjunction}\right\}$ and the actual typing comes from another property of speech.
\end{itemize}

For example\! : \textsl{John or Mary came to the party. In fact they both did} and \textsl{John or Mary, but not both, came to the party. In fact, they both did}. Since the second is contradictory but not the first, they can't mean the same thing.
Take \textsl{If John or Mary came to the party, then Bill was pleased} and \textsl{If John or Mary, but not both, came to the party, then Bill was pleased}. Those sentences have different meanings under the assumption \textsl{John and Mary came to the party} so they can't be equivalent.

For the anecdote\! : the US Supreme Court ruled that "A jury should not be permitted to engage in conjecture whether an unresponsive answer, true, and complete on its face, was intended to mislead or divert the examiner", and thus established the literal truth rule.

\subsection{TA 9\! : 22/11}
\subsubsection{Free Variables and Unpronounced Argument}
\paragraph{Free Variables}
\textsl{Talented Lawyer} seems to mean that the man is talented at being a lawyer, but again, in the example about Dancing with the Stars, it might mean the lawyer is talented at dancing. We want to say that we make an assumption about the argument of \textsl{talented}, while it actually is a free variable. \\
In \textsl{John ate to a local restaurant}, \textsl{local} has a free variable, as it might mean 'of the locality' as well as 'serving a cuisine from the region'.\\
We can see a free variable as in the phrase\! : $ate(j, x)$ where $x$ is not pre-defined and $j$ is a constant.
We can often bind free variables with a prepositional phrase\! : \textsl{local (to-x)} or \textsl{beautiful (at-x)}.

\paragraph{Unpronounced Argument}
However, in \textsl{John ate}, while we can have an argument to \textsl{ate}, the sentence is sufficient to itself.
It thus seems that there is only an un pronounced argument, from the logic phrase $\exists x, ate(j, x)$. \\
In French, \textsl{Jean a vu} when answering a affirmation \textsl{Il s'est passé ça}. \\


\section[Class 7\! : 23/11]{Phonology}
\subsection{Preliminaries}
The vast majority of human languages primarily use sound as a mean of externalization. Phonetics asks questions about the properties of the sounds of languages\!: How are they characterized acoustically ? How are they produced ? How are they processed ? Phonology (within cogscience) asks questions about the mental representations of the sounds of language\!: What are they like ? What computations do they participate in ? What sound-related processess do we find in natural language ?

\subsubsection{Phonological Competence}
There are many things speakers know about the phonology of their native language. This knowledge isn't conscious, but some of it is accessible to introspection with the right training.\\
There are individual sounds that are ok, but sequences as a whole that are not\! : e.g. \textsl{bnick} is made of sounds 'b', 'n', 'ik' that are ok while \textsl{bn} is not. \textsl{Abnormal} seems a counter-example but \textsl{bn} is cut between two syllabs. And in russian, \textsl{rtut} is fine, but not in english. This is all about the way sounds are organised. \\
Sometimes, the problem with sounds comes from the internal word position, e.g. the first sound of \textsl{hang} cannot be at the end of a word, and the last one cannot be at the beginning of a word.

\subsection{Phonetics}
\subsection{Definitions}
Phonetics studies the physical aspects of how speech sounds are produced and perceived\! :
\begin{itemize}
    \item What muscles and movements are involved in articulating speech ?  What do speakers do to produce different sounds ?
    \item How does the auditory system perceive different sounds ? Is it just the auditory system ? (McGurk effect)
    \item What are the acoustic properties of speech sound ? What differentiates \textsl{sh} from \textsl{s} in perception ?
\end{itemize}

There is a big chasm between orthography and the actual sound of words\! :
\begin{center}
    \begin{tabular}{ccc}
        \toprule
                      & letters & sounds \\
        \midrule
        phlegm        & 6       & 4      \\
        blame         & 5       & 4      \\
        ophthalmology & 13      & 10     \\
        awe           & 3       & 1      \\
        music         & 5       & 6      \\
        \bottomrule
    \end{tabular}
\end{center}
We need a better way than orthography to talk about the sounds of human speech.

\subsubsection{Speech Sounds}
The sounds of speech can be studied from two complementary perspectives, articulation and spectrography ? % TODO\! : Complete 

Producing speech sounds is done by moving air through the vocal tract. The air vibrates, and airwaves travel from vocal tract to the listener's eardrums. Sound quality changes when the shape of the vocal tract is modified by moving the jaw and the soft tissue organs inside.
Speech sounds can be classified into different groups, depending how they are articulated. The main dimensions in which phoneticians classify sounds are\! :
\begin{itemize}
    \item voicing\! : do the vocal folds vibrate ? The vocal folds can be pulled together and made to vibrate. If the vocal folds are vibrating while the speech sound is being produced, then the sound is voiced.
    \item manner of articulation\! : how is the airflow articulated ? The air only moves through the vocal tract freely when vowels are produced.
    \item place of articulation\! : ? Sounds often pattern differently depending on where in the vocal tract is obstructed. The terms for differnet places of articulation are based on vocal tract anatomy.
\end{itemize}
There are major classes of sounds\! :
\begin{itemize}
    \item Non-continuants (n, m, p, t\dots) vs continuants (f, $\theta$, l, r\dots)\! : Complete vs. partial obstruction of the oral cavity.
    \item Obstruents (p, t, k, f\dots) vs sonorants (m, n, a, e\dots)\! : Severe vs mild obstruction to the airflow
    \item Consonants vs vowels\! : Obstruction vs no obstruction to the air flow.
\end{itemize}

\subsubsection{Phones and Phonemes}
We have been studying speech sounds and their intrinsic properties (phones), from the perspective of articulation and acoustics. What abous speech sounds in the minds of speakers as section of their linguistic competence ? The abstract mental representations of the speech sounds are called phonemes.
A phoneme can be realized as two different phones\! : In english, $/p/$ can be realized as $[p]$ and $[p^{h}]$. We say that $[p]$ and $[p^{h}]$ are two allophones of the phoneme $/p/$.
We don't say that those phones correspond to different phonemes for multiple reasons\! :
\begin{itemize}
    \item Untrained Speakers have a very hard time distinguishing two allophones of one and the same phoneme, but no such difficulty distinguishing allophones of different phonemes.
    \item The two allophones will behave in completely predictable ways across the phonology.
    \item Mental representations can be less redundant and eliminate predictable aspects of phonetics.
\end{itemize}
How can we tell when two sounds in a language correspond to different phonemes or are allophones of one and the same phoneme ? Well, current methods do not allow us to answer this question directly\!: we cannot directly inspec mental representations and phonological competence. Phonologists address it by asking more sophisticated and indirect questions.

We can construct minimal pairs if the differ in just one sound or one feature of a sound. When possible, it's best to use words of the same morphosyntactic category, as well as to find pairs where the segment in question appears in different positions within the words. A feature is distinctive in a certain context if two sounds differing in just that feature in that context can distinguish words.
Then, since $[p^{h}]$ appears only in the beginning of a word and between two sonorants if the following vowel is stressed, and $[p]$ occurs everywhere else, they are two allophones of the same phoneme.

Some allophonic variation is motivated by coarticulation, i.e. the situation wherein the phonetic realization of a phoneme is influenced by a preceding or following speech sound.
It is extremely common in human languages for a nasal to acquire the same place of articulation features. Do all phonological processes involve considerations about what is easier or harder to pronounce ? No, there are examples of arbitrariness in phonological processes as well\! : \textsl{Laden} in both German and Dutch.

\subsubsection{Loan Words}
Loan words are words that were imported into a language from another, such as \textsl{sito} meaning \textsl{store} in Yoruba, but here, the scheme consonant-vowel was kept.

Languages have overarching constraints on phonology, including sequences of sounds and syllabic structure. This is called phonotactics.


\subsection{TA 10\! : 06/12}
\subsubsection{On Previous Homework}
\paragraph{Difficult}
Two main free variables\! : \textit{to X} and \textit{for X}

\paragraph{Legal}
Main free variables\! : \textit{in X} where $X$ is a jurisdiction, same with \textit{according to X}, e.g. Korean law forbids a Korean resident to smoke weed in another country.

\paragraph{Of the utmost importance}
When someone says something is important, there is a sense of \textit{to achieve X}. The level of degree that stands in the word \textit{importance} that is filled in by \textit{utmost}.

\subsubsection{On the distinction between allophone and phoneme}
The phoneme is the abstract phonology that can be represented by an allophone, a sound. The allophones are characterized by the sounds composing them, the latter being sorted as in the following tabular, based on their place and manner of pronunciation\! :
\begin{center}
    \begin{tabular}{ccccccc}
        \toprule
                    & Nasal       & Plosive/Stop & Fricative          & Sibilants & Tap  & Trill \\
        \midrule
        Bilabial    & m           & p | b        &                    &           & bfra         \\
        Labiodental &             &              & f | v              &           &      &       \\
        Dental      &             &              & \textipa{T} | viii &           &      &       \\
        Alveolar    &             &              &                    &           &      &       \\
        Palatal     &             &              &                    &           &      &       \\
        Velar       & \textipa{N} & k            &                    &           &      & r     \\
        Uvular      &             & q | g        & rh                 &           &      &       \\
        Glottal     &             &              &                    &           &      &       \\
        \bottomrule
    \end{tabular}
\end{center}

A minimal pair is a pair of words that separate two allophones of a same phoneme, e.g. \textit{Either} and \textit{Ether} or \textit{Thigh} and \textit{Thy} on the difference between [\textipa{z}] and [\textipa{s}].\\
A complementary distribution is a set of words such that two allophone cannot appear in the same situation.

\subsubsection{Optimality Theory}
This states that all the languages have the same set of rules that encode the way that ways are formed, e.g. Consonant - Vowel alternance, No Consonant at the end, No Cluster of consonants and so on, but the order of priority changes in the set of rules, so that the less valued can be more easily broken. This rule is especially useful in borrow-words. \\
In Japanese, CVC is the most important, then Agree, then Dependency and so on, while in Hawaïan, No Coda, then Dependency then Agree, and so on...\\
In principle, we should consider the infinity of strings to see which represents the real word.


\section[Class 9\! : 07/12]{Language and Reasoning}
\subsection{Reasoning}
Basing on simple problems\! : \textsl{A bat and a ball cost \$1.10 together. The bat costs \$1.00 more than the ball. How much does the ball cost ?}, we want to see how semantics connect to reasoning. The process that connects us to the wrong answer $10c$ might have to be with the fact that we're looking for the answer in the data that we've gotten. It might also have to do with availability heuristics.
\subsubsection{Systematicity}
Human reasoning fails in strikingly systematic ways, we do something when we try to answer questions like these, but it looks like what we do is the wrong thing. Human reasoning works on mental representations that are often the product of interpretive processes, linguistic processes feed into reasoning processes. We will look at number of interactions between interpretive processes and reasoning. We will study theory of the operations of the faculty for reasoning. \\
For example, in the sentence\! : \textsl{John speaks German and Mary speaks French or else Bill speaks Italian}, we get trouble on entailment\! :
\begin{itemize}
    \item Classical Entailment\! : If \textsl{Bill doesn't speak Italian} then \textsl{John speaks German}
    \item Naive "Entailment", 20/20 acceptance\! : If \textsl{John speaks German} then \textsl{Mary speaks French}
\end{itemize}
Those two different notions of \textit{What follows} are linked to two different representations of deduction\! :
\begin{center}
    \begin{tabular}{p{.4\textwidth}p{.4\textwidth}}
        Truth Conditions                                                                                                    & Inquisitive Semantics                                                                 \\
        $(j \wedge m) \lor b$                                                                                               & $\left\{j \land m, b\right\}$                                                         \\
        (worlds where John speaks German intersect worlds were Mary speaks French) union (worlds where Bill speaks Italian) & Two Alternatives\! : 1) J. speaks German and M. speaks French. 2) Bill speaks Italian
    \end{tabular}
\end{center}
The same type of situations appear when talking about the number of days it takes for lilypads to cover a lake or about the kings and queens of Europe that Mary met.\\
When looking at \textsl{every A or every B}, a reasonable reasoning suggests that either it's \textsl{every A and no B} or the opposite. The same thing happens when talking about \textsl{most of the homework}. Yet, those pragmatic reasoning seem to be systematic across every human being, as anyone could think how the hearer would respond to this signal.

\subsubsection{Formalisation}
\begin{itemize}
    \item Compelling Fallacies are invalid inference patterns that we often accept
    \item Repugnant Validities are valid inference patterns that we often reject
    \item \emph{Failures} of reasoning ? It's possible some of the problems we've seen so far are in fact the result of sound reasoning acting on non-obvious but perfectly reasonable and predictable interpretations of the premises. If we don't countenance this possibility in a systematic and sophisticated way, we run the risk of misdiagnosing interesting interpretive processes as failures of reasoning.
\end{itemize}
Indeed, we resist disjunction introduction : $\phi \lor \psi$ suggests strongly one doesn't know which of $\phi$ and $\psi$ is in fact true. This is because if the speaker did know, they would have given us the true one, which would be more informative that the disjunction. Then, take Moore's Paradox :
\textsl{It's raining but I don't believe it.} And then, \textsl{If it's raining but I don't believe it, then there is something I don't know.}\\

\subsection{Theories on Reasoning}
\subsubsection{Mental Logic and Mental Models}
These share the idea that intuitively humans do something that is related to formal logic.
\begin{itemize}
    \item In Mental Logic : Humans do some form of proof theory, they have deductive rules they use to transform sentential representations into other sentential representations.
    \item In Mental Models : Humans do some form of model theory, and build mental models of the informations they are given, update these representations sequentially and inspect the resulting model for constitutive parts that are of interest.
\end{itemize}
\subsubsection{Heuristics and Biases}
We do not reason with logic or mathematics, instead we use heuristics that give quick decisive answers to our questions. Our reasoning is rendered fallible by the imperfection of these heuristics as well as a collection of innate biases :
\begin{itemize}
    \item Judgement by Substitution : Faced with a hard question, we substitute for it an easier question whose answers we can map into answers to the hard question. We work on answering the easier question, and transpose the answer we find.
    \item Base-rate neglect : We often ignore general information about a representative sample and go purely on information about a particular individual.
    \item Anchoring effet : We tend to incorporate information we recognize as altogether irrelevant into our decision making.
\end{itemize}

\subsubsection{Other Theories}
There also are more theories such as a Probabilistic Approach and one called Argumentative Theory. The latter states that reasoning appeared not to help make better decisions but to help justify decisions and persuade others to adopt our views. Argumentation then is an essential tool allowing for coordination of complex social activities, political structures, and so on. Reasoning helps ground argumentation.


Many experiments can be run to see the way that people reason from visible looks.

\subsection{Manipulations on a Problem}
\subsubsection{Contextuality}
When changing the context of a problem, the answers given by participants change. For example, partipants had better answers to an ecologically valid problem such as the age and alcohol version of the Wason Selection Task than in the pure context of letters and numbers. This was used to validate the environmental influence on reasoning. Multiple reasons have been advanced :
\begin{itemize}
    \item J'ai pas eu le temps de lire...
\end{itemize}
And even in a particular environment, just changing the formulation of the problem changes the results : a disjunction case gives perfect results while introducing negations into the antecedent, the consequent of a conditional or both has a negligible effect on the distribution of people's choice. That is, they seemingly ignore negation. In fact, people, having no idea what is going on except for the fact it has something to do with vowels and even numbers, always answer the same thing : people are stumped by conditional, thus they go back to something they know. \\

\subsubsection{Illusory Content}
Sometimes (cf. beginning of lecture sentences), disjunction-like elements induce illusory inferences. Indeed, sentences that are expressed with a lot of conjunctions and disjunctions can often be simplified significantly, yet, giving the difficulty of parsing, there are problems. The erotetic principle says :
\begin{itemize}
    \item Our natural capacity for reasoning proceeds by treating successive premises as questions and maximally strong answers to them. (Problem of Failure)
    \item Systematically asking a certain type of question as we interpret each new premise allows us to reason in a classically valid way. (Problem of Success)
\end{itemize}
There is commitment on interpretation, since disjunctions raise alternatives and put pressure towards choosing an alternative : disjunctions are like questions in this regard.


\subsection{TA 11\! : 06/12}
\subsubsection{On Last Homework}
\begin{itemize}
    \item \textsl{pass} should be pronounced : \textipa{[p\super{h}A:s]} in british english or \textipa{[p\super{h}\ae s]}.
    \item For two words without a minimal pair, you can find with \textipa{[Z]} and \textipa{[t]}
\end{itemize}
\subsubsection{IIFD}
We study this phenomenon : Given the proposition $a \lor \left(b \land c\right)$ and $b$, people conclude $c$.


There are seemingly two systems inside the head of peoples :
\begin{enumerate}
    \item One that is based on heuristics : $b$ looks like $b \land c$ so people talk conclude $c$.
    \item One that can actually do reasoning.
\end{enumerate}

Multiple theories arise :
\begin{itemize}
    \item One where Pragmatic reasoning is encouraged :
          Through studying the alternatives, people strengthen the statement in : $\left(a \land \lnot b \land \lnot c\right) \lor \left(\lnot a\land b \land c\right)$. This second sentence looks like the \texttt{xor} of the \textit{natural language} sentence, and might come from a pragmatic interpretation of the natural language \textsl{or}.  This is sometimes tested by tring to overflow the brains of people by asking them to remember a grid. It has been found that people doing this were more precise.
    \item One where Pattern Matching happens
    \item One where Erothetic judgement comes in
\end{itemize}


\section[Class 10\!: 14/12]{Follow-up : Language and Reasoning}
\subsection{Erotetic Theory}
A standard employment of \textsl{or} is in the specification of possibilities : This comes from the principle that questions are modeled as sets of propositions. \\ Disjunctions are approaches to free choice, counterfactuals, exceptional scope-taking. Inquisitive Semanctics say disjunctions are the building blocks of questions. \\
Many natural languages (Polish, Korean, Japanese, Slavic...) have the same morphemes for the interrogative complementizer and disjunction operator.\\

Then, illusory inferences in the erotetic theory come from incomplete answers.

\subsubsection{Mental Models}
We formalize Erotetic Theory through mental models. % TODO : See slides.

\subsubsection{Success in the Erotetic Theory}
Sound reasoning is possible. So there must be some strategy using our natural faculties guarantees it. This strategy has to be innate or learnable but it shouldn't be costly :
\begin{theorem}[Soundness Theorem]
    The ETR derivation strategy where an update with $\Delta$ is immediately preceded by a sequence of inquire steps for each atoms that occurs somewhere in $\Delta$ is sound for classical propositional models.
\end{theorem}

\subsection{Testing this Theory}
We can induce the representations of a disjunction with visual animations. This has been done to demonstrate that infants can construct complex representations of alternative possibilities such as induced by a disjunction. \\
From other experiments, we get that children under 4 have impoverished symbolic system and cannot represent alternative possibilities. Children acquire more complex representational arsenal thanks to linguistic bootsrapping wia the language of modality which appears around the same age.

\subsection{TA 12 20/12}
\subsubsection{Erotetic Theory Formalisation}
We don't use $P \land R$ to symbolise \textsl{P and R}, but prefer $P\sqcup R$. \\
For disjunctions, we use sets containing all the possible propositions : $\left\{P \sqcup R, J\right\}$. \\
We represent our situation as a tuple, with our propositions on the first dimension, and the set og things we know to be true in the second dimensions : $\scalar{\left\{P \sqcup R, J\right\}, \emptyset}$. \\
We symbolise updates such as this : $\scalar{\left\{P \sqcup R, J\right\}, \emptyset}\left[j\right]^{up}$ which means that we have learned for sure that $j$ is true. \\
Then, we can chain updates : $\scalar{\left\{P \sqcup R, J\right\}, \emptyset}\left[j\right]^{up}\left[\left\{m, r\right\}\right]^{up}$ is equivalent to $\scalar{j, \left\{\left\{j\right\}\right\}}\left[\left\{m, r\right\}\right]^{up}$ which is equivalent to $\scalar{\left\{j\sqcup m, j \sqcup r\right\}, \{j\}}$

\subsubsection{Disjunction Reduction}
People are better at resolving disjunctions when we update with the negative : we talk about $\lnot P$ and $P \lor Q$ which implies $Q$ :
\[
    \begin{aligned}
         & \scalar{0, \emptyset}\left[\lnot P\right]                                           \\
         & \scalar{\lnot P, \left\{\{\lnot P\}\right\}}\left[\left\{P, Q\right\}\right]        \\
         & \scalar{\lnot P \sqcup Q, \left\{\left\{\lnot P\right\}\right\}}\left[Q\right]^{MR}
    \end{aligned}
\]
is easier than
\[
    \begin{aligned}
         & \scalar{0, \emptyset}\left[\left\{P, Q\right\}\right]^{up}    \\
         & \scalar{\left\{P, Q\right\}, \emptyset}[\lnot P]              \\
         & \scalar{\left\{\lnot P \sqcup P, \lnot P \sqcup Q\right\}, Q}
    \end{aligned}
\]
Here, $MR$ means \textit{Moleculary Reduction}.
\subsubsection{Inquiry Operation}

The inquiry operation works as such :
\[
    \begin{aligned}
         & \scalar{\left\{\lnot a, \lnot b, \lnot c\right\}, Q}\left[a\right]^{inq}                                            \\
         & \scalar{\left\{\lnot a, \lnot b \sqcup a, \lnot b \sqcup \lnot a, \lnot c \sqcup a, \lnot c \sqcup \lnot a\right\}}
    \end{aligned}
\]
We can then go and explicit every formula based on any possible atoms. \\
This is of use when resolving $c$ and \textsl{a and b or c and d}
\[
    \begin{aligned}
         & \scalar{\left\{a \sqcup b, c \sqcup d\right\}, \emptyset}\left[c\right]^{up} \\
         & \scalar{\left\{c \sqcup d\right\}, \left\{\{c\}\right\}}                     \\
         & d
    \end{aligned}
\]
Remind this is not supposed to be classically true, but it models what people do. Then using inquiries :
\[
    \begin{aligned}
         & \scalar{\left\{a \sqcup b, c \sqcup d\right\}, \emptyset}\left[c\right]^{inq}                                    \\
         & \scalar{\left\{a \sqcup b \sqcup c, a \sqcup b \sqcup \lnot c, c \sqcup d\right\}, \emptyset}\left[c\right]^{up} \\
         & \scalar{\left\{a \sqcup b \sqcup c, c \sqcup d\right\}, \left\{\left\{c\right\}\right\}}
    \end{aligned}
\]

\section[Class 11\!: 14/12]{Neurolinguistics and Psycholinguistics}
\subsection{Localizing Language}
The brain is composed of two hemispheres. For humans, as well as for all modern vertebrates, sides swapped. Handedness makes a difference: right ear advantage for speech processing in right-handed people (and so for left)\\
Split-brain surgery is a common treatment for seizures. When different images are presented to split vision field, patients can describe and point out to images on the right but cannot describe images presented on the left.\\
There is a possibility to localize everything in the brain by stimulizing electrically the brain.

\begin{itemize}
    \item Tests with right-handed people demonstrate about $90\%$ left-hemisphere language localization and $10\%$ mixed distribution.
    \item With left-handed people, $75\%$ LH language, $15\%$ mixed distribution and $10\%$ RH language.
    \item The correlation between handedness and language localization is robust but very imperfect.
\end{itemize}
Handedness is not trivial to ascertain, varying by task.

How can we localize language in the brain ?

\subsubsection{Broca's Aphasia}
The lesion-deficit paradigm :
\begin{enumerate}
    \item Find Someone with significant lesion in area $X$
    \item Characterize their deficit in linguistic tasks $Y$
    \item Conclude that $X$ plays a big role in $Y$
\end{enumerate}
This is how it went for Pierre \textsc{Broca} with his patient Tan. The autopsy reveale a lesion in what is known as Broca's area (BA44 \& BA45) of left-hemisphere.\\
In similar cases, there are different manifestations depending on the person. Mainly, people often keep their lexicon words, still have vocabulary, but lose in capacity using function words. This resonates with the case of Jeanie (cf. \ref{par:critical-period}). We also find :
\begin{itemize}
    \item Halting, non-fluent production, with verbs left out more than nouns, a reduced mean length of utterance and a frequent omission of function words and functional morphemes.
    \item Comprehension impairment especially when it crucially depends on syntactic distinctions
    \item Non linguistic problems (e.g. asymmetric paralysis most often of face and right hand)
\end{itemize}

In a 1960 documentary, we see a person having issues understanding a passive way sentence. We then see that syntax does have an influence on comprehension : \textsl{The leopard was killed by the lion} is tougher to understand than \textsl{The lion killed the leopard}. This comes from the fact that things that come later in a sentence are often newer. \\
The role of Broca's area in language is not completely clear
\begin{itemize}
    \item Damage must extend to surrounding areas and underlying whit matter.
    \item Broca's aphasics tend to display language impairments not entirely describable in terms of syntax of phonology.
    \item Broca's area is by no means the only area of the brain where damage will reliably produce systematic language impairment.
    \item Yet, there is a strong correlation with syntax, processing and speech production.
\end{itemize}

\subsubsection{Wernicke's Aphasia}
Similarly to Broca's Aphasia, Wernicke's sees patients after a trauma produce linguistics modification :
\begin{itemize}
    \item Speech is fluent and well articulated, though repetition is impaired
    \item Patients produce frequent semantic and phonemic errors, including non-words
    \item Speech appears "empty" or confused
    \item Associated comprehension deficit, even with word meanings.
\end{itemize}

\subsection{Brain Imaging}
\subsubsection{The System}
\begin{itemize}
    \item Functional Neuroimaging consists to measuring blood flow in the brain. Working harder needs more oxygen and thus more blood. This gives a high spatial resolution. However, blood moves slowly compared to electrical brain activity. There is a 2-6 second delay between electrical activity and the result appearing.
    \item EEG measures electry potentials directly from the scalp, but the electric signal is distorted by the cranium. This gives poor spatial resolution.
    \item MEG measures magnetic fields associated with electrical signals. Since fields aren't distorted by the skull, space resolution is better.
\end{itemize}
MEG allows to sea, about 350ms after reading a word, a peak of activity in the left temporal lobe. Changes depending on the linguistic properties of the stimulus appear, and the activity is quicker when the word is more frequent. The hypothesis associated is that activity is associated with lexical access.

\subsubsection{Morphological Decomposition}
How to interpret the difference between \textsl{date + [PAST] = dated} and \textsl{give + [PAST] = gave}. The first one is clearly bimorphemic, but is the second ? \\
The first answer that seems to be is NO. Indeed, this comes from priming, the ability to recognize words faster if one has seen the word recently. Since \textsl{dated} is recognized faster after seeing \textsl{dated}, and \textsl{give} is not recognized faster after seeing \textsl{gave}. Since \textsl{nine} is not recognized faster after seeing \textsl{asinine}, nothing here has to do with phonetics nor orthographics. \\
Yet, phonologically and orthograpically similar words inhibit priming effects. The hypothesis is that \textsl{gave} IS bimorphemic. \\
If we can measure priming before interference has taken place, then \textsl{give/gave} and \textsl{date/dated} should have the same effect. We find that M350 happens in both \textsl{give/gave} and \textsl{date/dated} but that there is no response in both \textsl{give/gave} and \textsl{boil/broil}. Moreover, we see that priming does improve the result in the first two examples but not on \textsl{boil/broil}. Yet, \textsl{teach/taught} is not improved since it is already faster to recognize than the 3 other pairs. \\
Morphological Decomposition is always needed, even when morphology is not to be seen in phonetics (e.g. \textsl{gave}).

\subsection{Psycholinguistics}
This field talks about how we deploy our knowledge of language :
\begin{itemize}
    \item Grammar : This is a formal specification of structures allowed in a language along with their interpretation. A central element of theory of competence.
    \item Parsing : This is the process of determining the syntactic structure of an input string. A parser takes as input a string of words and produces as output a parse tree: a labeled bracketing of the input sentence.
\end{itemize}

\subsubsection{Parsing}
AAAAAAAAAAAAAAAAAAAH

\subsubsection{Ambiguity}
There is ambiguity at different levels of representation :
\begin{itemize}
    \item Lexical - the same phonological string may correspond to more than one word.
    \item Lexical Semantic - a word may have more than one sense
    \item Structural - a string may have more than one legal parse
\end{itemize}
Also, we need to distinguish :
\begin{itemize}
    \item Global Ambiguity - no information in the sentence resolves the ambiguity
    \item Local Ambiguity - at one or more points during parsing, there is insufficient information to determine the correct structure. But the ambiguity disappears once the entire string is parsed.
\end{itemize}
The Garden Path phenomenon tells us that the parsing of a sentence is entirely serial : Frazier and Rayner theorised that
\begin{itemize}
    \item Only one syntactic representation is built at any given time
    \item Everything is based on heuristics : Late clorsure where awe attach new items to the phrase or clause currently being processed and Minimal Attachment where we attach new items to the existing structure in a manner that requires establishing as few new nodes as possible.
\end{itemize}




\end{document}
