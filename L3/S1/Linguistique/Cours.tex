\documentclass{cours}
\usepackage{enumitem}
\usepackage{qtree}

\title{LING 101 : Introduction to Linguistics}
\author{Salvador Mascarenhas \\ \small Michael Goodale (TA)}
\date{21 Septembre 2023}


\begin{document}
\part[Cours 1 21/09]{Language : Psychological and Social Entity}
\section*{Introduction}
    DEC is called like that because is would have bothered litteraries
    Prof : Master at NYU, Junior Research at Oxford, here since 2016. 
    Using language as a window into human mind\\
    Linguistics is a broad term for serious principled study of language. 
    Many perspectives, from the cognitive point or study language by an external perspective (structuralism : Ferdinand de Saussure, or Leonard Bloemfield) looking out on structures.
    Also quite general here.
    Language looked at as a social entity.
    History of languages, typology of languages.
    Not only about cognitive studies, pretty broad look out.\\
    Teaching assistant : Michael Goodale PhD student, LRSCP, computational models of Language Acquisitions. Statistical inference and model language after formal tools.
    Practical Skills really.\\
    Assessments : Homework, graded on a qualitative schedule, due on lecture days.
    Final : Last Lecture, 30\% of grade
    10\% of grade in TA participation. 
    Website : Moodle hosted by \href{https://moodle.u-paris.fr/course/view.php?id=7374}{Université de Paris}, \href{https://moodle.u-paris.fr/pluginfile.php/1002151/mod_resource/content/5/intro-ling-syllabus-2023.pdf}{Syllabus}
    Syllabus : 

\section*{Schedule}
\begin{enumerate}
    \item Language as a psychological and social entity
    \item Language (non-)variation : Universals, variation within parameters
    \item \textbf{Morphology}, language typology
    \item \textbf{Syntax I} constituent structure; selection and subcategorization
    \item \textbf{Syntax II} subcategorization; X-bar theory
    \item \textbf{Semantics I} first look at meaning \textsl{Studied by Salvador}
    \item \textbf{Semantics II} philosophy of language and the case for methodological solipism.
    \item \textbf{Phonology} phonetic macro classes;
    \item Language and Reasoning
    \item Sign languages (guest lecture)
    \item Language in the brain; deficit-lesion method; functional brain imagining; psycholinguistics; parsing, reading, lexical access.
    \item Language and thoughts in minds vs. machines
\end{enumerate}

\section{Remarks and Observations about the Nature of Language}
R.Descartes \textit{"Discourse on the Method"} : Humans, everyone of them, can speak. Animals, though they have what is needed, can't express their thoughts.
Insights :
\begin{enumerate}
    \item It doesn't matter on general intelligence, social intelligence or any measure of your intellectual abilities. Happens despite any other difficulties.
    \item \emph{To Our Knowledge} Not any other animal can do what we can do. Article from the \textit{NYT}, saying animals \textbf{can} speak, though it's embarassing. A Chasm \textit{appears} between humans and animals. Yet, it is continuous of what happens in the animal kingdom.
    \item Animals are not incapable of language because they can reproduce human language, or use signs to communicate. Studies on Non-Human primate vocalizations, 3-4 words, all alarm calls $\rightarrow$ Language is independent on organs and communication systems. Yet is it a panic reaction or a real communication. Cannot conclude on A.Is then...
\end{enumerate}

\section{The Goal of Linguistics}
A complete understanding of how sound (/sign/etc) relates to meaning : 
\begin{itemize}
    \item in terms of the speaker's knowledge : the state their mind is in by virtue of having acquired a natural language (competence). Distinguished from mastery of language/way it is produced. Describing skill, not usage. 
    \item in terms of using that knowledge in linguistic tasks, like uncovering meaning from sound in real-time comprehension; executing motor commands necessary to externalize meaning in language production (performance)
\end{itemize}
Competence/performance is not really a sort of Chomsky. Chaz Firestone (Yale) published on competence/performance saying machines have been tested on performance and not competence. 
Tight connection between thinking and speaking. 
Behaviourism = school of thought that tried to figure out a way of studying humans that postulated and said absolutely about our mind. Not only linguistic behaviour : we shouldn't describe what happens in people's heads, just study what outputs comes from what inputs, without looking at the stimuli. Not what we will postulate. 
Freud sucks.
Cannot look at functions : functions = algorithms, studied based on input/output pairs. Cannot do deduction nor induction but only abduction. Yet, we have no less reasons to believe it is right than to believe black holes exist.

\section{Different levels of Study}
Example : \textsl{Mushrooms are an edible fungus}
\begin{enumerate}
    \item Sound Categories : Studying the sound signal based on the phonemes, represented in the mind.  
    \item Morphemes : first chucks of phonemes that has a meaning : Morphology. Here : \textsl{Mushroom} and \textsl{z} or \textsl{edi} and \textsl{able}. Sometimes they are not pronounced : need for a rigorous description. Sometimes, they're redundant, and appear with the same meaning in different places : compare theories. What is the probability of that happening ? And how about irregularities : Past = laugh-\textit{ed} or g\textit{a}ve ? Past in a concept that can manifest itself in different places : simple theory. FMRI theory can improve this theory. Morphemes don't always come in the same orthograph nor sound.
    \item Words : Not much to do here. 
    \item Semantics : Organizing words into phrases. Here : \textsl{edible fungus} is a phrase, but \textsl{edible} is not. Must be done formally.
\end{enumerate}
Three way of looking : Us, looking from a native's judgement - introspection \textbf{will} answer some questions. 
Exaggeration, yet : no written language, only looking at spokn language. 

\section{Language and Societies}
    \subsection{Language and Classes}
        Language display depends only on human factors, political relationships, genetic factors : distinction between the animal and the meat (names coming from French, spoke by the upper class) in english. Happened in other languages. Different from hyperonyms like \textit{poultry} for \textit{chicken}. Context can explain linguistic aberrations. 
    \subsection{Language and Dialect}
        Language and Dialect are political constructs and arbitrary decisions: \begin{quote}
            A language is a dialect with an army and a navy. (M.Weinreich)
        \end{quote} 
    \subsection{Infinity of Language}
        Sentences are of arbitrary length, and can always be augmented. Yet infinite-ish sentences are impossible to comprehend because Performance is finite, i.e. cognitive resources are finite. There is a \textit{finite} system generating infinitely many linguistic representations : recursions are of the order.
    \subsection{Description}
        Not looking for rules prescripting language (fuck l'académie française), but only for rules describing them. No better way to speak, the way you ought to speak has nothing to do with linguistics and only with politics. Yet, language are principled, even those which are \textit{proscribed} : adding \textit{fucking} in the middle of a word : \textit{Phila-fucking-delphia}. Rule here : \textit{fucking} comes before the stressed syllable and the material right before needs to be heavy. Heavy comes from phonology, rigorous theory of the weight of syllables. 
    \subsection{Phonological Differences}
        Languages have different constraints on the syllables composing their words : \textit{*pnick} works in French but not in Engl*sh.
    \subsection{Internal Structure of Sentences}
        Sentences are made of constituents that don't act up the same in every language : \textit{des} is not used in Engl*sh ($\sim$ \textit{of the}). They cannot be broken : \textit{des burgers et des frites}. It is mysterious tho ? Maybe language has something else to do for us than communicate\ldots\\
        They cannot be considered alone : \textit{Fat cats eat} and \textit{Fat cats eat accumulates}. Supposition that two words next to each other are related in written language. Also, prosody is a big help in understanding. 

\part{TA 1 27/09}
\section{Animal Communication}
Language is also communication, not only hearing (trees ?). Many (if not most) Animals Communicate, and almost all react to sound. (cf. \href{https://www.nytimes.com/2023/09/20/magazine/animal-communication.html?unlocked_article_code=_mKij4e1jtSDj61vUQZVjQCPQ678hO69vto7sqwbRaA3kmyw5b8t-mxMcnihQgvfCJuKaQe1pvift5_AInBSESFgm2rBtU7GDoS_gyv_G6GUnUjV5Wb8L_Cb4YjsG-BFKXy_yO3FYnECOJFaCdmGPS7pCbPH8lqQcH5l4mixJE4IfNzBPeACptp-hnBmdQkb0jkD9qa06NCzE12I22V_m94Uh6YT-76HUyTwGvPuYKgrb0-F-xoAdiItiAZoUDJzWBY2GIujcO8Hw7TiORkZXfc8MRihzb4S7i6_eZR1mWD4-yafAQQP4Ya_hFCSV-wmJKxSHpSSrMFpoK9n4sdL&smid=url-share}{NYT Article})
 : dolphins communicating by signs, bees dancing, monkeys having muscles/organs to 'talk', bird songs, ant pheromones, great apes... 

Differences between human and animal communication ? Human language has : composition (recursion : meaning of sentence can be derived from its constituents), abstraction, no hypothetical/long term/prevention discussion, intentionality, arbitrary length of sentences, systematic neologisms/nonce words (when learning a word, it is usable immediately), non-instrumental.

Many experiments about teaching great apes language suck and were not really concluent. There is a poor, noisy, contradictive and unrepresenting stimulus that child have to make do with. Deaf child make their own language if they need one. For example of the poverty of the stimulus : 2 Layer Embedding of possessive happened 67 times in 120k sentences, while kids at 6 can do 4 level possessive embedding. 

The words \textsl{stop, mat, tap, butter} all have \textsl{'t'} yet have different sounds : there is a sense where this is the same sound, but another one where they have different sounds.

\part[Cours 2 28/09]{(Non-)Variation and Languages}
\section{Different yet Similar}
Langugages are made of signs, composed of a form and a meaning. They only look like they have multiple forms/meanings, on another level of analysis they only have one, e.g. past tense in english and the morpheme \textsl{ed}. Variants depend only and purely on properties of the root, and are entirely predictable. With \textsl{bat}, there is an ambiguity phenomenon, but there is also a phenomenon of polysemy, e.g. \textsl{book}, also, similar meanings often derive from a central point.
\begin{quote}
    \og A sign presents itself to the senses, and something distinct from itself to the mind \fg - St-Augustine
\end{quote}
There are two types of signs based on the link between form and meaning: 
\begin{enumerate}
    \item Those with a causal link : \textsl{smoke} implies there is a fire
    \item Those with a arbitrary, conventional link : \textsl{black attire} implies mourning (\textit{in some cultures but...})
\end{enumerate}
After Ferdinand de Saussure, \begin{quote}
    \og Language is a systemn of conventional (arbitrary) signs.\fg    
\end{quote}
Example : The word \textsl{man} in different languages : German Mann, Spanish hombre, Français homme, Hungarian ember, Turkish adam. With the sound produced by the rooster, the words differ in languages, but there is still a partial causal link, because you cannot mimic perfectly the sound of the animal, given the differences between vocal organs. 
Arbitrary doesn't mean random : it just doesn't matter what choice is made. It is no accident there is a resemblance between German and English words for \textsl{man}

\section{Similarities between distant unrelated languages}
\subsection{Reciprocal pronouns}
Pronouns marking reciprocity always have a mysterious constraint where they must be in the same, finite clause as their antecedents : \textsl{They thought I talked about each other} seems weird.\\
Generally, it seems that reciprocal pronouns must refer to a thing that lies in the same proposition, but why ? \\
First question is : Do we have a reason to say why they seem weird, just because they are longer ? No, \textsl{I thought that they talked about each other}, is equally long as \textsl{They thought that I talked about each other} and doens't sound half as bad. \\
Then, the sentence \textsl{They thought we talked about each other} shows it's not about third person. \\
Coming up with a precise answer implies looking for phrases where each bit of the sentence has been replaced, one at a time, to isolate the \textit{issue}.\\
In \textsl{They thought that I talked about each of them}, \textsl{each of them} is not necessary reciprocal, as it only include (\textsl{each other})'s meaning so it is compatible. 

\subsection{Sentivity to negative elements}
Words occur sometimes with negative elements. Every natural language seems to contain at least one lexical items that is sensitive to whether the context in which they occur exhibits negative or positive polarity.\\
Exemple : \textsl{Jean a fait le moindre effort} doesn't seem natural, but \textsl{Jean \emph{n'} a \emph{pas} fait le moindre effort} does.\\
Facts are subtle : the mere presence of a negatvive item is enough to license a negative polarity, and sometimes negative polarity is infered without the obvious presence of a negative item. A sentence with an empty slot in place of the negative polarity item is the context that needs to be negative in order to license an NPI. \\
Positive polarity also exists : \textsl{John didn't see someone} is really weird, and it requires a really particular prosody and/or context to work. You have a meta interpretation of this. The word anyone, more than a negative item is also a free choice item. 

\subsection{Relations between Sentences}
In all languages, declaratives and interrogatives are linked by \emph{transformations} ; i.e. a reorganisation of the terms of the declarative to build the interrogative. It creates pairs of assignations, not necessarily questions and answers. Also some declaratives link to multiple questions. \\
There is a finite palette of strategies for these transformations : no known human language builds questions by mirroring completely the order of the words. Grammars do not count, e.g. there is no language with transformation swap words 1 and 2.\\

\subsection{The Puzzle}
Languages are systems of arbitrary signs. Any sign will do for internal calculation, and any convention widely known within a given community will do for communication. This observation does not predict the existence of strong systematic, pervasive similarities whose speech commuinties have had no contact. \\
How can the conventional character of language be reconciled with pervasive cross-linguistic similarities. 
\begin{quote}
    \og We would learn so much if we could do horrible things to babies. We can do horrible things to animals though, not saying we should, but we can.  \fg - Salvador Mascarenhas
\end{quote}

\section{Universal Grammar}
\subsection{Chomsky's Hypothesis}
Human faculty for language : \begin{itemize}
    \item enables humans to acquire and use langugae;
    \item delimits what linguistic structures humans are capable of acquiring and using
    \item delimits the kind of linguistic convetions that a community of humans can develop and successfully hand down to new generations
\end{itemize}
Thus we need to know all about gene reproduction, biological evolution and so on, to understand it fully \\
Universal Grammar in this sens is part of all humans' biological endowment and it is reflected in all natural languages.

\subsection{Universal Grammar}
This is not a language in itself and it doesn't imply the idea all languages come from a common source. This does not imply there is no other factors of relevance shaping actual natural languages than \textsl{a priori} constraints, there being no rhyme or reason to those constraints. Chomsky added it might be to optimize computation. This is \emph{not} a collections of generalizations about trends in linguistic diversity, other projects (Dryer, 2005) have been doing it, especially on word order. \\
It has for purpose to classify the languages the human mind can learn and the \textit{impossible} ones. It is a two steps projects : Finding facts, photographing the human languages, then deriving generalizations.

\subsection{Language Acquisition}
\subsubsection{Critical Period}
There is a critical period for human (and animal aswell) acquisition of language : post-puberty, acquisition is severely impaired, e.g. Genie, kid discovered in L.A. in 1970 at 13.5y. Any child can learn any language, it depends on features of the environment.\\
Acquisition doesn't seem to be full related to mathematical/intellectual/reasoning skills and so on... Learning a second/third language is totally different tho, it is correlated to musical and computational skills it seems, there even seems to be a purely linguistic talent. Almost nothing about adult phenological is due to purely biology and genes. 

\subsubsection{Problem of Induction}
Grammars make predictions for infinitely many word sequences, yet the input if finite. Therefore, there is something not in the input is playing a key role in learning. 

\subsubsection{Absence of Negative Evidence}
Experiments, can give negative information: they can show that under certain conditions, an outcome is \emph{not} observed, but a child has extremely limited access to such negative data.

\subsubsection{Conclusion}
A child is far better at figuring their native language than a linguist. Linguistics is an empirical science not a fundamental one. \\
Linguistic data consist of judgements on utterances, grammatically judging strings of characters, truth-value judging sentences/scenarios. It can come from introspection, but introspection is limited: there is a limit to what one can infer, e.g. \textsl{I have more pictures of my kid on my phone than my dad ever saw me} isn't grammatical, but it is understandable. The unboundness of human mind might mean we can find sense in any sentence.\\
The better way is to speak with other linguists or conduct experiments on a large amount of naive participants. There is a risk of falling into delusion from thinking to much about the same sentence. 

\subsubsection{Chomsky's Argument}
There is such a gap between what a child is exposed to and the sophistication of the acquired grammar the child must have expectations as to what a language can be, analog to the concept of triangle: You've never seen a triangle but you know what it is. Mathis Hademeyer\\
Universal Grammar is a set of principles with a number of free parameters. \\
Principles and Parameters is a method now limited to small studies on variations, mostly in Italy (just you and me). By this we mean the fact that language, unrelated, share a structure. In fact most languages only occupy a minuscule part of the mathematical space of possibilities for a language. Principles and Parameters is a way to cash out on this idea : the problem is figuring what values has a child gotten for its parameters, and how he can deduce from some principles, the structure of language. \\
The null-subject parameter : In Catalan, \textsl{he speaks} is said $\emptyset$\!\textsl{parla}, there is no need for a pronoun, contrary to French or English. How do children hande this situation ? It seems that Italian children go through a phase where they omit subject when learning French.


\part{TD 2: 04/10}
\section{Homework 1}
\subsection{Question 1}
The word \textsl{nous} is used in formal context, so it is expected that people use formal grammar in the whole sentence, hence the \textsl{ne} of negation is expected. 

\subsection{Question 2}
You can drop the copula in AAVE when you can contract the verb. 

\section{Exercise}
\subsection{Swedish Extrapolation}
Separate the sentences in groups of morphemes, then extrapolate the meanings based on the other sentences.

\subsection{Rule Extrapolation}
Language is Chikisaw or something.

\part[Cours 3: 05/10]{Morphology : The Structure of Words}
\section{Use of Morphology}
Do you spell kick-ass or kickass ? Is it one or two words ? This is not very well understood.

\section{The Study of Morphemes}
Morphology is the study of words, and morphemes. Morphemes are the smallest linguistic unit that makes sense, e.g. \textsl{a} or \textsl{I}. 
The sound has no meaning, but the morpheme has. It is the smallest entity with both a form and a meaning.\\
Words are formed by processes, so they have a constituent structure, they can be constituted from multiple morphemes.\\
They can be derived from rules of morphology. Languages have a huge range of variations in what they can do.\\

\subsection{Words}
A Word is an indentifyable unit of phonology with a prosody (cf. phonology class).

Words have a structure : The \emph{noun} (part of speech) \textsl{reuseables} is made of four morphemes : \textsl{re, use, able, s}. Here, the morphemes are, in order, \textsl{a prefix, a root, a suffix, a suffix}. \\
Words can be derived from other words also : \textsl{to invite - an invite}. They can then have an ambiguity in sense : is \textsl{reusable} a noun or an adjective.\\

\subsection{Morpheme Types}
Prefixes, suffixes, infixes (e.g. \textsl{abso-\emph{fucking}-lutely}), circumfixes (e.g. \textsl{\emph{em}-bold-\emph{en}} - not a good example as historically it might have never been a circumfix) together are called affixes. Thos morphemes are called bound morphemes, meaning they cannot appear by themselves (e.g. \textsl{mang} root of \textit{manger} in French)\\  
When looking at the derivation, the thing that is not an affix is called the stem. The root is the smallest stem. \\
Things called clitics can also come into words, e.g. \textsl{l'}, it has more information than another morpheme : \textsl{l'aime} has two concepts in it.\\
Roots and Stems are called open class morphemes, new instances can easily emerge or be invented, e.g. \textsl{blick-ing} would be easy to understand. Affixes are closed class morphemes, meaning new instances develop slowly.\\

Inflectional morphemes only add grammatical information. They can have the same sound shape as some derivational morpheme. Derivational morphemes on the other hand, create new concepts out of existing ones.\\

There is a universal attested word-structure : $\left[\left[\left[\textsf{root}\right]\left[\textsf{derivational affixes}\right]\right]\left[\textsf{inflectional affixes}\right]\right]$

\section{Forming Words}
Particular grammars make certain derivational processes available, which we can describe by means of rules.

\subsection{Derivational Processes}
A rule specifies the category of the input and the category of the output (cf \ref{table:Derivational Rules in English}). Affixes typically add further restrictions on stems.

\begin{table}
    \centering
    \caption{(Simplified) Derivational Rules in English}
    \begin{tabular}{ccc}
        \toprule
        Affix &Rule &Output\\
        \midrule
        -able & Verb + -able &= Adjective\\
        re- & re- + Verb &= Verb\\
        un$_{1}$- & un- + Adjective &= Adjective\\
        un$_{2}$- & un- + Verb &= Verb\\
    \end{tabular}
    \label{table:Derivational Rules in English}
\end{table}

\subsection{Constituent Structure}
From a derivational analysis of a word in terms of the processes that generated it, we can extract its constituent structure. We can represent it in a sort of phylollogical tree. \\
For \textsl{un|enjoy|able} : \textsl{unenjoy} is not a word, as you cannot reverse enjoyment. Even though \textsl{unsee} exists, it is more of a creative product of language as you might not understand its meaning at first glance; it is understood because it is frequent. So we get a structure like : Node(un, Node(enjoy,able)). We might go deeper in analysis, but there is no real rule behind \textsl{en-joy}.\\
For \textsl{re|read|able} : We cannot base an argument on a stem that doesn't exist, because both hypothetical stems do. Yet, as \textsl{re-} does not work with an adjective, we can derive the structure to be : Node(Node(re,read), able)\\
For \textsl{un|wrap|able} : Here, both Node(Node(un, wrap), able) and Node(un, Node(wrap, able)) are valid processes of derivation involving different rules. The second is the negation of the adjective \textsl{wrapable}, which means something that cannot be wrapped. The first on the other hand, is something that can be \textsl{unwrap(ped)}, since we add \textsl{able} to a verb. This is an example of structural ambiguity, that is often disambiguated by prosody e.g. \textsl{fat cats eat...} Yet, this not only allows to express ambiguity, but also to explain it. 

\subsection{Compounding}
This is a quite mysterious area of morphology.

Compounding is a process to build words from two or more stems. It constitutes in combining those stems with a novel meaning, with lower predictability than derivational processes, e.g. \textsl{bitter|sweet}. There is compounding between, adjectives and adjectives, adjectives and nouns, nouns and nouns, nouns and verbs, verbs and verbs, verbs and nouns... There is a very blurry line between what people think and historical reasons, are people thinking of \textsl{sleep} and \textsl{walk} in \textsl{sleepwalk} ?\\
It can be an ambiguous process : If a \textsl{houseboat} is a boat that is a house, what is a \textsl{housecat} ? English compounds are typically headed by their rightmost element, meaning the precedents are qualifying it, but sometimes compounds are headless : \textsl{kick-ass}, \textsl{ceasefire}...\\
Are there more rules ? Is an N-N compound necessarily a \textit{for} relation, a \textit{from} relation ? Is there simply just a hidden preposition in it ? 


\part{TA 3 : 11/10}
\section{Homework 2}
Nothing to add, see file. 

\section[Cross-Linguistic Variation]{Cross-Linguistic Variation\footnote{This is part of Lecture 3, see the handout}}
Languages can be put on a spectrum from analytic to polysynthetic, based on how much they use morphemes and/or syntax.
Analytic use the most syntax, polysynthetic don't care about it and only add morphemes.
\begin{itemize}
    \item Analytic (Isolating) languages where each morpheme is a word on its own (e.g. Chinese)
    \item Agglutinating languages where each grammatical bit of meaning is an affix to a stem (e.g. Japanese, Korean, Hungarian)
    \item Fusional languages where an affix or change in the stem can lead to multiple grammatical meaning variations (e.g. Latin, German, Arabic)
    \item Polysynthetic Languages where affixes have extremely rich content (e.g. many native North American languages)
\end{itemize}
Both agglutinating and polysynthetic languages pack a lot of information into a single word, the difference being that agglutinating use affixes for grammatical meaning only.

\section{Morphological Derivation}
\subsection{Deriving \textsl{Antidesestablishmentarianism}}
We can see this word from a sequence of trees : 
\begin{center}
    \begin{tabular}{ccc}
        \toprule
        & Tree : & Explanation\\
        \midrule
        Step 1 : & \Tree [dis establish ] & $dis : verb \mapsto verb$ \\
        Step 2 : & \Tree [[dis establish ] ment ] & $ment : verb \mapsto noun$\\
        Step 3 : & \Tree [anti [[dis establish ] ment ] ] & People are against \textsl{disestablishment}, not the people\\
        Step 4 : & \Tree [[anti [[dis establish ] ment ] ] arian ] & $arians : action \mapsto people in it$ \\
        Step 5 : & \Tree [[[anti [[dis establish ] ment ] ] arian ] ism ] & $ism : people/concept/action \mapsto idea$ \\
        \bottomrule        
    \end{tabular}
\end{center}

\subsection{Deriving \textsl{unfriendliness}}
We only give the tree : 
\Tree [[un [friend li ] ] ness ]

\subsection{Deriving \textsl{reinstatement}}
You cannot attach \textsl{in} to a noun, it needs to be attached to a verb, but as it is a rare example of borrowing from latin, we cannot separate \textsl{instate} 
\Tree [[re instate ] ment ]

\part[Cours 4 12/10]{Syntax 1}
\section{Syntactic Competence}
Syntactic competence is a wide concept, it includes  : 
\begin{itemize}
    \item The knowledge of what sentence is grammatical and what isn't. This comes from the fact there is no syntax without semantics in linguistics, unlike in arithmetics. The idea of an autonomous syntax has disappeared since the 70s. It is however useful to talk about syntax in a somewhat disconnected way from semantics.
    For example, you cannot contract auxiliaries when they carry meaning : \textsl{I shoulda bought it} and \textsl{*I shoulda more money}.
    \item An abitity to recognize a sentence as well-formed even if it does not make sense or consists of highly sequential transitions, meaning, even though some transitions have really low probability, e.g. \textsl{Colorless green ideas sleep furiously} and \textsl{Colorless Green} or \textsl{sleep furiously}...
    \item An ability to determine what the sentence means, whether it is ambiguous or not either structurally or lexically : 
    \begin{center}
        \begin{tabular}{cc}
            \toprule
            lexical e.g. \textsl{I went to the \emph{bank}} 
            &$\text{bank } = \begin{cases}
                &\text{ financial institution}\\
                &\text{ river bank}
            \end{cases}$ \\
            \midrule
            \multirow[b]{2}{*}{structural, e.g. \textsl{former producers and extras}} &\Tree [{former producers} and extras ]\\ &\Tree [former [producers and extras ] ]\\ 
            \bottomrule
        \end{tabular}
    \end{center}
    \item Every language has a countably infinite set of sentences (proven by showing there is no longest sentence in a language). Native speakers can handle this set despite having finite minds. We can construct arbitrary long sentences based on the center embedding idea, i.e. \textsl{le jeu du Johnny Depp}.
\end{itemize}

\subsection{Lexical Categories}
The lexical categories (or parts of speech) such as \textsl{N(oun)}, \textsl{V(erb)}\dots 
This does not mean you can only tell what part of speech a word is if you know its meaning.\\
\begin{quotation}
    \og \textit{The thing about thing is that everything and anything is a thing}\fg - Salvador
\end{quotation}

\begin{definition}
    We define lexical categories given the sentential environments in can occur in and the affixes it can take, i.e. the grammatical distribution of the word.
\end{definition}
\begin{quotation}
    \og \textit{You shall know a word by the company it keeps} \fg - Ferguses
\end{quotation}
For example, a verb in English satisfies all of the following :
\begin{enumerate}
    \item It can occur right after the auxiliary \textsl{will}
    \item It can take the endings \textsl{-s} and \textsl{-ing}
\end{enumerate}

\subsection{Constituency}
\begin{definition}[A Heuristic]
    A sequence of words forms a constituent in a sentence if :
    \begin{enumerate}
        \item It can be replaced with a minimal unit, ideally one word, presevervin grammaticality and conversely
        \item Occurences of that minimal unit can be replaced by the sequence of words, preserving grammaticality
    \end{enumerate}
    If those conditions hold, then the sequence has the same category as the minimal unit. 
\end{definition}

A constituent can also be found by Fronting and Pronominalization, Clefting, Eliding or Coordination

\subsection{Rewriting Grammar}
\textsl{Marie verra Jacques} leads to three possible structures in principle :
\begin{itemize}
    \item \Tree [.S Marie verra Jacques ]
    \item \Tree [.S [Marie verra ] Jacques ]
    \item \Tree [.S Marie [verra Jacques ] ] (\textsl{verra Jacques} is a Verb Phrase = VP)
\end{itemize}
We might build a rewriting grammar for a fragment of French : 
\begin{enumerate}
    \item \textsl{Marie verra/entendra/\dots Jacques}
    \item \textsl{Un type verra Jacques}
    \item \textsl{Marie verra un type}
    \item \textsl{Le type connu a vu Marie}
    \item \textsl{Marie dormira}
    \item \textsl{Marie pense que Paul verra Jacques}
    \item \dots
\end{enumerate}


We then can get an idea on French Grammar : 
\begin{center}
\begin{tabular}{cc@{$\ \longrightarrow\ $}c}
    \toprule
    Id&Node & Rephrased as\\
    \midrule
    0&S(entence) & N(oun) P(hrase) + VP\\
    1& NP & Proper Name\\
    2&NP & Det + N + (Adj)\\
    3&VP & V$_{intr}$\\
    4&VP & V$_{tr}$ + NP\\
    \midrule
    \multicolumn{3}{c}{Up to this point, we cannot generate an infinite number of sentences}\\
    \midrule
    5&C(omplementizer) P(hrase) & \textsl{que} + S\\
    6&VP & V$_{cl(ausal)}$ + CP\\ 
    7&V$_{tr}$ & voir, entendre,\dots\\
    8&V$_{intr}$ & dormir, briller,\dots\\
    9&V$_{cl}$ & penser, croire,\dots\\
    \midrule
    \multicolumn{3}{c}{There is now a loop in rules, so there is recursion and we can generate an infinity of sentence}\\
    \midrule
    10&C$_{temp}$ & avant que, quand\\
    11&CP$_{temp}$ & C$_{temp}$ + S\\
    12&S & NP + VP + (CP$_{temp}$)\\
    \multicolumn{2}{c}{\multirow{2}{*}{VP}} &  a) V$_{intr}$ + (CP$_{temp}$)\\
    \multicolumn{2}{c}{} & b)V$_{tr}$ + NP + (CP$_{temp}$)\\
    14&S & S + CP$_{temp}$\\
    15&VP & VP + CP$_{temp}$\\ 
    \bottomrule
\end{tabular}
\end{center}
We can then infer a tree from a sentence with the lexical categories : 
\begin{center}
    \Tree [.S \qroof{Marie}.NP [.VP [.VP [.V verra ] \qroof{Jacques}.NP ]  [.CP [.C quand ] \qroof{Paul dormira}.S ] ] ]
\end{center}

\subsection{Complements and Adjuncts}
Complements and adjuncts are ways to add things to a constituent. We get new grammar rules : 
\begin{center}
    \begin{tabular}{cc@{$\ \longrightarrow\ $}c}
        
    \end{tabular}
\end{center}

\part{TA 4 : 18/10}
\section{Homework 3}
Nothing to say.

\section{Study of an abstract language}
\begin{center}
    \begin{tabular}{c@{$\rightarrow$}c}
        \toprule
        A&aB\\
        B&c\\
        B&z\\
        B&C\\
        C&nC\\
        C&d\\
        \bottomrule
    \end{tabular}
    \Tree [.A a [.B [.C n [.C n [.C n C ] ] ] ] ]
\end{center}

This gives us many strings possible : \textsl{ac}, \textsl{az}, \textsl{an$^{*}$d}...

We can draw trees from those strings, e.g. \textsl{annnd}, see above.

\section{Structure of sentences}
\subsection{John sees Mary}
\Tree [.S \qroof{John}.NP [.VP [.V sees ] \qroof{Mary}.NP ] ]

\subsection{John Sleeps}
\Tree [.S \qroof{John}.NP [.VP [.V sleeps ] ] ]

\section{Heads, Complements, Adjuncts}
\begin{definition}
    A \textit{head} is a word. A \emph{head} is connected to a phrase. Anything that is the sister of a \emph{head} is a \emph{complement}. When a \emph{complement} is unnecessary, it is called an \emph{adjunct}. 
\end{definition}

\begin{center}
    \Tree [.S \qroof{John}.NP [.VP [.V {eats =  \textsl{head}} ] {apples = \textsl{complement}} ] ]    
    \Tree [.S \qroof{John}.NP [.VP [.{VP or $\overline{\text{ V }}$} [.V eats ] \qroof{apples}.NP ] [.PP [.P on ] \qroof{the train}.NP ] ] ]
\end{center}
\begin{center}
    \Tree [.XP [.XP {X = \textit{head}} {YP = \textit{complement}} ] {ZP = \textit{adjunct}} ]
\end{center}

\begin{center}
    \Tree [.S \qroof{Galileo}.NP [.VP [.V saw ] [.NP \qroof{a man}.NP \qroof{with a telescope}.PP ] ] ]
    \Tree [.S \qroof{Galileo}.NP [.VP [.VP [.V saw ] \qroof{a man}.NP ] \qroof{with a telescope}.PP ] ]
\end{center}

\section{Structure of Noun Phrases}
To analyze Noun Phrases with adjectives, we can base on the model from the first example below, introducting Determiner Phrases. We can do the same with verb phrases, seeing pronouns as determiners (this is called the DP hypothesis) :
\begin{center}
    \Tree [.S [.DP [.Det {the/a/every/each/\dots} ] [.NP [.AP [.A red ] ] [.N cat ] ] ] [.VP [.V sleeps ] ]]
    \Tree [.S [.DP [.Det he ] ] [.VP [.V sleeps ] ] ]
\end{center}

We may guess that in the sentence \textsl{Mary sleeps}, \textsl{Mary} is a determiner, yet we could say \textsl{every Mary sleeps}. There are two ways of seeing the sentence \textsl{Mary sleeps} : either \textsl{mary} is a noun phrase or \textsl{Mary} is a determiner phrase with an empty determiner. 
\begin{center}
    \Tree [.S [.NP [.N Mary ] ] [.VP [.V sleeps ] ] ]
    \Tree [.S [.DP [.D {$\emptyset$} ] [.NP [.N Mary ] ] ] [.VP [.V sleeps ] ] ]
\end{center}
The latter is better from the study of other languages such as Portuguese where the determiner is not empty : \textsl{a Maria dorm}.

\section{Teaser for next class}
Consider \textsl{He will go to the school} : 
\begin{center}
    \Tree [.S [.DP [.D He ] ] [.TP [.{T(ense)} will ] [.VP [.V go ] [.PP [.P to [.DP [.D the ] [.NP [.N school ] ] ] ] ] ] ] ]
\end{center}
We here only did binary branching, but there might be ternary branching sometimes.


\end{document}



