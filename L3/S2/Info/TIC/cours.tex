\documentclass{cours}
\usepackage{qtree}
\title{TD de TIC}
\author{Mohamed Kadhem Karray}

\begin{document}
\section{TD3}
\subsection{Exercice 1}
Soit $c$ un code PF. Soit $x_1, \ldots, x_{k}, y_{1}, \ldots, y_{l}$ pour lesquels il y a égalité. Alors, soit $i = \min \{i \in \N \mid x_{i} \neq y_{i}\}$. Alors en ne comptant que les $i$ premiers caractères du mot de désection, selon la longueur des codes, ou bien $c(x_{1},\ldots x_{i - 1})$ est un préfixe de $c(y_{1}, \ldots, y_{i - 1})$ ou bien l'inverse.

\subsection{Exercice 2}
\subsubsection{Question 1}
$c$ est un code PF. 

\subsubsection{Question 2}
Pour déchiffrer le code, il suffit de regarder le premier $0$ que l'on trouve. On choisit entre $1, 2$ et $3$ pour le déchiffrer selon la valeur modulo $3$ du nombre de $1$ qui le suivent. 

\subsection{Exercice 3}
J'ai une tête de cloche à fromage ouuuuuuuuuuuu ?

\subsection{Exercice 4}
\subsubsection{Question 1}
On considère $\mathcal{U}_{m} = \left\{u \in \mathcal{U} \mid l(u) \leq m\right\}$. On a alors, puisque $\mU_{m}$ est fini, l'existence de $l_{max, m} = \max \left\{l(u) \mid u \in \mU_{m}\right\} \leq m$. \\
On considère ensuite $c_{m}$ le code $c$ tronqué à $\mU_{m}$. On a :  
% \[
%     \begin{aligned}
%         \left(\sum_{u \in \mU}D^{-l(u)}\right)^{n} =& \sum_{x_{1}^{n}\in \mU^{n}}D^{-l(x_{1}) - \ldots - l(x_{n})}\\
%         =& \sum_{k \geq 1} \alpha(k)D^{-k}\
%         \leq & \sum_{k = 1}^{nl_{max, m}}\alpha(k)D^{-k}\\
%         = & nl_{max, m}
%     \end{aligned}
% \]
% Ainsi, 
% \[
%     \sum_{u \in \mU} D^{-l(u)} \leq (nl_{max, m})^{1/n} \xrightarrow[n\to \infty]{} 1
% \]
La suite des $\mU_{m}$ est croissante et on a $\limsup \mU_{m} = \mU$. Donc, par passage à la limite supérieure, on a bien 
\[
    \sum_{u \in \mU}D^{-l(u)} \leq 1
\]

\subsubsection{Question 2}
% On note $l_{1} = \abs{c(u_{1})} \leq \ldots \leq l_{n} = \abs{c(u_{n})} \leq \ldots$ les valeurs de $l(u)$. On prend comme $i$-ème mot code les $l_{i}$ premiers chiffres après la virgule dans le développement en base $D$ de $\sum_{j = 1}^{i - 1} D^{-l_{j}}$.\\
% Puisque cette somme est à termes strictement positifs et toujours inférieure à $1$, on a bien un code $PF$: les $l_{i}$ premiers chiffres de $c(u_{j})$ forment un nombre plus grand que $c(u_{i})$.
On retire, à sectionir de la profondeur $l(1)$ une proportion $\alpha_{1}D^{-l(1)}$ des branches. On définit une mesure $\mu$ sur les fils de l'arbre comme la proportion des feuilles de l'arbre qu'ils recouvrent. \\
On note $A_{i}$ l'union des fils enracinés à profondeur $l(i)$ qu'on a utilisés pour du codage de longueur $l(i)$. On a $\mu(A_{i}) = \alpha_{i}D^{-l(i)}$. Notre codage complet est l'union des $A_{i}$. On a alors : 
\[
    \mu\left(\bigcup_{i \in \N^{*}} A_{i}\right) = \sum_{i = 1}^{n} \mu(A_{i}) = \sum_{i = 1}^{n}\alpha(i)D^{-l(i)} = \sum_{u \in \mU} D^{-l(u)} \leq 1
\]
Donc on n'a pas de recouvrement des arbres. 

za
\subsection{Exercice 5}
\subsubsection{Question 1}
On code chaque nombre en base $2$ sur $k$ bits. On demande pour chaque bit si le nombre a la même valeur. Bah on a trouvé.

\subsubsection{Question 2}
On utilise le code $c_{2}(k)$ sur les $K$ objets. 
Puisque le code est PF, on peut juste regarder pour tout $i$ le $i$-ème symbole, et en fonction de ce qu'on a lu sur les $i-1$ premiers, lorsqu'on arrive à la fin du code, on a trouvé le résultat. 
On a $E[l(c_{2}(U))]$ questions en moyenne et donc on a bien : 
\[
    H_{2}(U) \leq E[l(c_{2})(U)] \leq H_{2}(U) + 1
\]
Pour tout $u \in \{1, \ldots, K\}$, on utilise $\lceil\log_{2}(p_{u})\rceil$ questions.


\subsubsection{Question 3}
Le nombre moyen de questions d'un questionnaire est équivalent à la longueur moyenne du codage UD qu'il représente. Par inégalité de Kraft et inégalité de Gibbs, on a : 
\[
    \begin{aligned}
        E[l(c)(k)] =& \sum_{k = 1}^{K}p_{k}l(c(k))\\
        =& -\sum_{k = 1}^{K}p_{k}\log_{2}(2^{-l(c(k))})\\
        \geq & -\sum p_{k}\log_{2}q_{k}\\
        \geq & -\sum p_{k}\log_{2}p_{k} \\
        = & H_{2}(U)
    \end{aligned}
\]
Donc le questionnaire a en moyenne au moins $H_{2}(U)$ questions. 

\subsubsection{Question 4}
On considère $U_{1},\ldots, U_{m}$ des réalisations i.i.d. de $U$. 
\begin{enumerate}
\item Avec $c_{2}$, en moyenne, on va utiliser : 
\[
    H_{2}(U) \leq \frac{1}{m}E[l(c_{2}(U_{1})) + \ldots + l(c_{2}(U_{m}))] \leq \frac{H_{2}(U) + 1}{m}
\]
\item Avec un code arbitraire, on va utiliser : 
\[
    H_{2}(U) \leq \frac{1}{m}E[l(c(U_{1})) + \ldots l(c(U_{m}))]
\]
\end{enumerate}


\subsubsection{Question 5}
On passe à la limite dans la question précédente. 

\section{TD4}
\subsection{Exercice 1}
On a : 
\[
	\begin{aligned}
		E[X] =& \int_{0}^{\infty}xf(x)\d x\\
		=& \int_{0}^{\infty}[xf_{X}(t)]_{0}^{t}\d t\\
		=& \int_{0}^{\infty}\int_{0}^{t}f_{X}(t)\d x \d t\\
		=& \int_{0}^{\infty}\int_{x}^{\infty}f_{X}(t)\d t\d x \text{\hspace{2cm} Fubini Tonelli}\\
		=& \int_{0}^{\infty}P(X\geq x)\d x
	\end{aligned}
\]

\subsection{Exercice 2}
On a : 
\[
	\begin{aligned}
		P(Y = y, Z = z | X = x) =& P(Y = y | Z = z, X = x)\times P(Z = z | X = x)\\
		=& f(x, y)\times P(Z = z | X = x)\\
	\end{aligned}
\]
Donc on a bien l'indépendance car 
\[
	P(Y = y | X = x) = \sum_{z \in Z(\Omega)} P(Y = y | X = x, Z = z) = f(x, y)
\]
\subsection{Exercice 3}
\begin{enumerate}
	\item On sépare à chaque étape les pièces en 3. Puisqu'une unique pièce est plus lourde, une balance suffit à trouver dans quel groupe la pièce la plus lourde est: Si l'un des ensembles pesés est plus lourd, c'est dans celui là, sinon, s'il y a égalité c'est dans le troisième. 
	\item Chaque pesée correspond à une manière de séparer les pièces en $3$. En sectioniculier, l'arbre des possibilités correspondant à une stratégie arbitraire va être de hauteur au moins $\log_{3}(n)$. 
		La hauteur de l'arbre correspondant bien au nombre moyen de pesées\ldots
	\item On utilise la stratégie de la première question. On n'utilise alors jamais plus de $\lceil\log_{3}(n)\rceil$ pesée. Si $n = 3^{k}$, on revient au premier cas avec $\lfloor \log_{3}(n) \rfloor = k$ pesées.
	\item On calcule $H(U|Y) = \log_{3}(n) + \frac{\phi(p)}{\log(3)}$ où $\phi(p) = 2p\log p + (1 - 2p)\log(1 - p)$	
	\item On peut tester en deux pesées lors de la première étape si la pièce différente est plus lourde ou plus légère que les autres. En effet, si on sépare en trois ensembles $A, B, C$. Ou bien : $A < B$ et $B = C$ et la pièce est plus légère et dans $A$, ou bien $A < B$ et $B > C$ et la pièce est plus lourde et dans $B$. À renommage près, on applique la stratégie de 1) et on a le résultat. 
	L'entropie dit qu'on nécessiterait $k + \log_{3} 2$ pesées en moyenne, ce qui n'est pas entier. On a donc une solution optimale. 
\end{enumerate}

\subsection{Exercice 4}
\begin{enumerate}
	\item Il n'y a pas de feuilles manquantes à profondeur strictement inférieur à $x = max \abs{c(u)}$ puisqu'on minimise la longueur moyenne des codes. On a par ailleurs $B \leq D-2$ et $n + B = 1 + (D - 1)\alpha$. 
	\item Puisque tous les codes PF sont UD, le code de Huffman $D$-aire convient.
	\item L'arbre ternaire qu'on obtient est : \Tree [0 [10 11 12 ] ] \\
		On a alors $\bar{R_{s}} = 1.75$ et donc un taux de compression de $\frac{7/4}{\log_{3}{4}} = 1.39$
		Pour $D = 2$ on a l'arbre \Tree [$0$ [$10$ [$110$ $111$ ] ] ] ce qui donne $\bar{R_{s}} = 9/4$ et donc un taux de compression de $1.125$.
	\item Pour la distribution $p = \left(0.3, 0.2, 0.2, 0.15, 0.08, 0.07\right)$, on trouve 
		\begin{itemize}
			\item[Binaire] $L_{C} = 2.45$ et donc un taux de compression de $1.225$
			\item[Ternaire] $L_{C} = 1.65$ et donc un taux de compression de $1.012$ environ
		\end{itemize}
\end{enumerate}

\section{TD5}
\subsection{Exercice 1}
On conditionne par rapport au système complet d'évènements $\left(T = k\right)$ pour $k \geq 1$. On a : 
\[
	\begin{aligned}
		\sum_{n = 1}^{T}Y_{n} =& \sum_{n = 1}^{+\infty}Y_{n}\mathds{1}_{T \geq n}\\
		\mathbb{E}\left[\sum_{n = 1}^{T}Y_{n}\right] =& \mathbb{E}\left[\sum_{n = 1}^{+\infty}Y_{n}\mathds{1}_{T \geq n}\right]\\
		=& \sum_{n = 1}^{+\infty}\mathbb{E}\left[Y_{n}\mathds{1}_{T \geq n}\right]\\
		=& \sum_{n = 1}^{+\infty}\mathbb{E}[Y_{1}]P(T \geq n)\\
		=& \mathbb{E}[Y_{1}]\sum_{n = 1}^{+\infty}P(T \geq n)\\
		=& \mathbb{E}[Y_{1}]\mathbb{E}[T]\\
	\end{aligned}
\]

\subsection{Exercice 2}
En remarquant que si $U_{1},\ldots, U_{k}$ forment le plus petit préfixe de $U$ trouvé dans $\cont$ (c'est le seul car le dictionnaire est valide), on peut écrire $U = U_{1}, \ldots, U_{k}, \tilde{U}$ où $\tilde{U}$ est une séquence infinie, on obtient immédiatement l'indépendance des $W_{i}$ de l'indépendance des $U_{k}$. De même, du fait que les $U_{i}$ suivent la même loi et du fait que le code est valide, on a bien l'identique distribution des $W_{i}$.

\subsection{Exercice 3}
On calcule la hessienne de $f$:
\[
	\begin{pmatrix}
		\frac{1}{x} & -\frac{1}{y}\\
		-\frac{1}{y} & \frac{x}{y^{2}}
	\end{pmatrix}
\]
Son polynôme caractéristique est: 
\[
	\lambda^{2} - \frac{\lambda}{x} - \frac{\lambda x}{y^{2}}
\]
Ses valeurs propres (réelles) sont donc 
\[
	\lambda_{1} = 0 \text{ et } \lambda_{2} = \frac{x^{2} + y^{2}}{xy^{2}}
\]
Ces deux valeurs propres sont positives donc $f$ est convexe. 

\subsection{Exercice 4}
On considère l'arbre (semi-infini) enraciné $D$-aire dont les noeuds sont étiquetés par la probabilité que la source émette la séquence de lettre qui amène de la racine à cette branche.\\
On observe que :
\begin{enumerate}
	\item Tous les sommets dans le sous arbre fils de n'importe quel noeud sous moins probable que ce noeud
	\item Tous les sommets dans l'arbre $D$-aire d'un dictionnaire valide pour la source sont aussi des noeuds avec la même probabilité dans notre arbre
\end{enumerate}
Par définition d'un dictionnaire de Tunstall, un dictionnaire valide avec $M = D + \alpha(D-1)$ mots est un dictionnaire de Tunstall si et seulement ses $\alpha + 1$ noeuds sont les $\alpha + 1$ plus probables noeuds dans cet arbre. 
Ainsi, la somme des probabilités des noeuds est la même pour tous les dictionnaires de Tunstall à $M$ mots. 
Mais, on sait que la profondeur moyenne des feuilles est égale à la somme des probabilités des noeuds (la probabilité de chaque noeud étant la somme des probabilités dans le sous-arbre).

\section{TD6}
\subsection{Exercice 1}
\begin{enumerate}
	\item Chaque distribution est indexée de manière unique par $n$ compteurs entre $0$ et $m$, donc $\abs{P_{m}} \leq (m + 1)^{n}$.
	\item En regroupant les indexations qui forment la même distribution, c'est à dire en précisant le dénombrement précédent, on obtient
		\begin{equation*}
			\abs{P_{m}} = \binom{m + n - 1}{n - 1}
		\end{equation*}
	\item Le nombre de suites de longueur $n$ avec $k_{a}$ fois la lettre $a$ pour chaque $a \in \mathcal{U}$ est donné par le nombre de manière distinctes de permuter un multiensemble à $n$ éléments: 
		\begin{equation*}
			\abs{T_{n}(p)} = \binom{n}{k_{a_{1}}\cdots k_{a_{n}}}\frac{n!}{\prod_{a \in \mathcal{U}} k_{a}!}
		\end{equation*}
	\item Pour la borne supérieure, on a, en utilisant la co-entropie:
		\begin{equation*}
			1 \geq \sum_{u^{m} \in T_{m}(p)} e^{-m H(p_{u^{m}}\mid p} = \abs{T_{m}(p)}e^{-mH(p)}
		\end{equation*}
		Pour la borne inférieure, on a 
		\begin{equation*}
			\frac{1}{M} \log \abs{T_{m}(p)} = H(q) + o(1)
		\end{equation*}
		Donc $T_{m}(p) \geq e^{m(H(q) + o(1))} \geq (m + 1)^{-n}e^{mH(q)}$
\end{enumerate}

\subsection{Exercice 3}
\begin{enumerate}
	\item C'est l'inégalité de Gibbs : 
		\begin{equation*}
			D(p, q) =  -\sum p_{i}\log q_{i}  - \left(-\sum p_{i}\log p_{i}\right) = H(q, p) - H(p) \geq 0
		\end{equation*}
		Il y a égalité si et seulement si il y a égalité dans Gibbs, i.e. $p = q$ p.s.
	\item Trivial de par la formule ci dessus.
	\item Trivial de par la formule ci dessus.
	\item Trivial par continuité de $H(q, \cdot)$ et de $H$.
	\item La fonction $p, q \mapsto D(p, q)$ est convexe. Ainsi, s'il y a une borne inf, elle est nécessairement atteinte: Si $p, p'$ vérifie $D(p, q) > \gamma$, on prend  
	\item On a: 
		\begin{equation*}
			\begin{aligned}
				I(X, Y) =& H(Y) - H(Y \mid X)\\
				=& -\sum p_{X}(x)H(Y \mid X = x) - \sum p_{Y}(y)\log p_{Y}(y)\\
				=& \sum p_{X}(x) \left( \sum p_{Y \mid X = x}(y) \log p_{Y \mid X = x}(y) \right) - \sum \left(\sum p_{(X, Y)}(x, y)\right)\log p_{Y}(y)\\
				=& \sum p_{X}(x)p_{Y\mid X = x}(y) \log p_{Y\mid X = x}(y) - \sum p_{(X, Y)}(x, y) \log p_{Y}(y)\\
				=& \sum p_{(X, Y)}(x, y)\log \frac{P_{(X, Y)}(x, y)}{p_{X}(x)} - \sum p_{(X, Y)}(x, y)\log p_{Y}(y)\\
				=& \sum p_{(X, Y)}(x, y)\log \frac{p_{(X, Y)}(x, y)}{p_{X}(x)p_{Y}(y)}\\
				=& D(P_{X, Y}\mid P_{X}\times P_{Y})
			\end{aligned}
		\end{equation*}


\end{enumerate}


\section{TD9}
\subsection{Exercice 1}
\begin{enumerate}
	\item On a: 
	\begin{equation*}
		\begin{aligned}
			H(X \mid Y) =& \sum_{x \in X, y \in Y} P_{X, Y}(x, y) \log P_{X \mid Y}(x \mid y)\\
			=& \sum_{y \in Y} \sum_{x \in X} P_{X, Y}(x, y) \log P_{X \mid Y}(x\mid y)\\
			=& \sum_{y \in Y} P_{Y}(y) \sum_{x \in X}P_{X|Y}(x)\log P_{X \mid Y}(x \mid y)\\
			=& \sum_{y \in Y} P_{Y}(y) H(X \mid Y = y)
		\end{aligned}
	\end{equation*}

	\item On a trivialement, pour tous $X, Y, y$, $-\log P_{X \mid Y}(x\mid y) \geq 0$. Par somme, on a le résultat. 
	Par ailleurs, on a $H(X \mid Y) > 0$ si et seulement si on a:
	\begin{equation*}
		P_{X, Y}(x, y) \neq 0 \Leftrightarrow P_{X\mid Y}(x\mid y) = 1
	\end{equation*}
	c'est à dire si et seulement si, presque sûrement, $x = \phi(Y)$. 

	\item On a: 
	\begin{equation*}
		\begin{aligned}
			H(X, Y) =& \sum_{(x, y) \in X \times Y} P_{X, Y}(x, y) \log P_{X, Y}(x, y)\\
			=& \sum_{y \in Y} \sum_{x \in X} P_{X\mid Y}(x\mid y) P_{Y}(y)\log \left(P_{X\mid Y}(x\mid y)\times P_{Y}(y)\right)\\
			=& \sum_{y \in Y} \left(P_{Y}(y)\sum_{x \in X} P_{X \mid Y}(x \mid y) \log P_{X \mid y}(x \mid y)\right) + P_{Y}(y)\log P_{Y}(y) \\
			=& H(X \mid Y) + H(Y)
		\end{aligned}
	\end{equation*}

	\item Récurrence triviale en considérant $(X_{1}, (X_{2}, \ldots, X_{n})$ comme variables dans l'équation précédente.
	
	\item Si $X \perp Y$ on a le résultat. Sinon, on a $-P_{X, Y}(x, y)\log P_{X\mid Y}(x\mid Y) > -P_{X}(x, y)\log P_{X}(x)$ par stricte convexité de $\phi: t\mapsto t\log t$. 

	\item De même que la question précédente.

	\item De même que la question précédente. 

	\item On a, par définition $P_{\phi(X)}(\phi(x)) = \sum_{x\in \phi^{-1}(\phi(x))} P_{X}(x)$ donc par transfert on vérifie bien que $H(\phi(X)) \leq H(X)$ avec égalité si et seulement si $\phi$ est injective. 

	\item Il s'agit simplement de considérer dans les propriétés précédentes les variables $(X_{j})_{j \neq i}$ et $X_{i}$. Pour la deuxième inégalité, par positivité on a trivialement le résultat du point $8$. 

	\item L'aspect suffisant est évident par $H(X, Y) = H(Y) + H(X \mid Y) = H(X) + H(Y\mid X)$. De plus si $H(X) < \infty$ et $H(Y) \infty$, par $8$ on a $H(X, Y) \leq H(Y) + H(X) < \infty$. 

	\item On applique simplement les raisonnements précédents en passant à l'entropie conditionnelle plutôt qu'à l'entropie qui vérifiera ici les mêmes propriétés. 
\end{enumerate}

\subsection{Exercice 2}
Les propriétés de l'entropie conditionnelle point par point se démontrent de même que précédemment. 

\subsection{Exercice 3}
\begin{enumerate}
	\item On a déjà démontré que $I(X, Y) = H(X) - H(X \mid Y) = H(Y) - H(Y \mid X)$. Par conséquent, comme $H(X\mid Y) = 0$ si $X = \phi(Y)$, on a $I(X, X) = H(X) - H(X \mid X) = H(X)$. 
	\item Découle du point $v$ de l'exercice précédent. 
	\item $I(X, Y) = H(X) - H(X \mid Y) = H(Y) - H(Y\mid X) = I(Y, X)$
	\item $I(X, Y) + H(X, Y) = H(X) - H(X \mid Y) + H(Y) + H(X \mid Y) = H(X) + H(Y)$ par le point $iii$ de l'exercice 1. 
	\item Déjà démontré. 
	\item Vient par positivité de $H(X \mid Y)$ et de $H(X \mid Y) = 0 \Leftrightarrow X = \phi(Y)$ presque sûrement. 
	\item Vient du point $vi$ de l'exercice $1$. 
	\item Vient de l'exercice $1$. 
	\item Par récurrence sur $k, l$, on a $I(X_{k}, X_{l + 1} = I(X_{k}, f(X_{l}) = I(f(X_{l}), X_{k}) \leq I(X_{l}, X_{k}) = I(X_{k}, X_{l})$ et de même $I(X_{k}, X_{l}) \leq I(X_{k + 1}, X_{l})$. 
	\item Calcul direct.

\end{enumerate}


\end{document}

